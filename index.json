[{"authors":["admin"],"categories":null,"content":" More than 10 years experience of developing distributed system in Java ，python and go for commerical and open soucre projects 6 years public cloud development and helped multiple clients migration On-Premises business to hybrid cloud Skilled at large scaled container based cluster platform like kubernetes Leading a big data system design for 20PB storage for many year.  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://wubigo.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":" More than 10 years experience of developing distributed system in Java ，python and go for commerical and open soucre projects 6 years public cloud development and helped multiple clients migration On-Premises business to hybrid cloud Skilled at large scaled container based cluster platform like kubernetes Leading a big data system design for 20PB storage for many year.  ","tags":null,"title":"Wu Bigo","type":"author"},{"authors":null,"categories":[],"content":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nCNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","date":1550996323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550996323,"objectID":"4dbacfc0727205a4c013414bf43243c1","permalink":"https://wubigo.com/post/k8s-cni/","publishdate":"2019-02-24T16:18:43+08:00","relpermalink":"/post/k8s-cni/","section":"post","summary":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nCNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","tags":["K8S","CNI","NETWORK"],"title":"K8S CNI","type":"post"},{"authors":null,"categories":[],"content":"magicloud\n","date":1550912318,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550912318,"objectID":"837a2dd44ced1b3ed69e99200a8ea9fe","permalink":"https://wubigo.com/project/magicloud/","publishdate":"2019-02-23T16:58:38+08:00","relpermalink":"/project/magicloud/","section":"project","summary":"magicloud","tags":[],"title":"Magicloud","type":"project"},{"authors":null,"categories":[],"content":"hpcloud\n","date":1550911549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550911549,"objectID":"179e75a04820f1b3c020c375f2923aa9","permalink":"https://wubigo.com/project/hpcloud/","publishdate":"2019-02-23T16:45:49+08:00","relpermalink":"/project/hpcloud/","section":"project","summary":"hpcloud","tags":[],"title":"Hpcloud","type":"project"},{"authors":null,"categories":["IT"],"content":" setup for gitlab tee .gitlab-ci.yml \u0026lt;\u0026lt; EOF image: monachus/hugo variables: GIT_SUBMODULE_STRATEGY: recursive pages: script: - hugo artifacts: paths: - public only: - master EOF git init echo \u0026quot;/public\u0026quot; \u0026gt;\u0026gt; .gitignore  post  hugo new post//index.md\n deploy  hugo publish the public to web server\n Configuration Lookup Order confit/_default/\n ./config.toml ./config.yaml ./config.json  confit/_default/config.toml\n hugo build destination  publishDir   Number of items per page in paginated lists  paginate = 20   taxonomies\n by tag by author   tag = \u0026quot;tags\u0026quot; author = \u0026quot;authors\u0026quot;  confit/_default/menus.toml\n Navigation Links widget enable/disabl under content/home/ folder Navigation Links widget display order  weight = 1  config/_default/languages.toml\n多语言显示\nlanguageCode = \u0026quot;en-us\u0026quot; languageCode = \u0026quot;zh-Hans\u0026quot;  Blog set content/post/_index.md\n post view as Card  view = 3  content/home/posts.md\n Number of recent posts to list.  count = 20  ","date":1550806707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550806707,"objectID":"a81ccaae6503c87d521581d175a11183","permalink":"https://wubigo.com/post/blog_on_hugo/","publishdate":"2019-02-22T11:38:27+08:00","relpermalink":"/post/blog_on_hugo/","section":"post","summary":"Decide to gave hugo a shot after many years of being jekyll","tags":["BLOG"],"title":"Blog on hugo way","type":"post"},{"authors":null,"categories":["IT"],"content":" version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods  kubectl get pods --field-selector=status.phase=Pending   images list  kubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.go:389] Default route transits interface \u0026quot;enp0s3\u0026quot; I0217 07:28:13.308349 14495 interface.go:196] Interface enp0s3 is up I0217 07:28:13.309611 14495 interface.go:244] Interface \u0026quot;enp0s3\u0026quot; has 2 addresses :[192.168.1.9/24 fe80::a00:27ff:fe75:f493/64]. I0217 07:28:13.310328 14495 interface.go:211] Checking addr 192.168.1.9/24. I0217 07:28:13.311219 14495 interface.go:218] IP found 192.168.1.9 I0217 07:28:13.311961 14495 interface.go:250] Found valid IPv4 address 192.168.1.9 for interface \u0026quot;enp0s3\u0026quot;. I0217 07:28:13.312688 14495 interface.go:395] Found active IP 192.168.1.9 I0217 07:28:13.313427 14495 version.go:163] fetching Kubernetes version from URL: https://dl.K8S.io/release/stable-1.txt I0217 07:28:23.320683 14495 version.go:94] could not fetch a Kubernetes version from the internet: unable to get URL \u0026quot;https://dl.K8S.io/release/stable-1.txt\u0026quot;: Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled (Client.Timeout exceeded while awaiting headers) I0217 07:28:23.321520 14495 version.go:95] falling back to the local client version: v1.13.3 I0217 07:28:23.330622 14495 feature_gate.go:206] feature gates: \u0026amp;{map[]} K8S.gcr.io/kube-apiserver:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 K8S.gcr.io/pause:3.1 K8S.gcr.io/etcd:3.2.24 K8S.gcr.io/coredns:1.2.6   pull images beforehand  kubeadm config images pull -v 4  init phase kubeadm config print init-defaults \u0026gt;adm.defaults.yaml git diff adm.defaults.yaml -imageRepository: K8S.gcr.io +imageRepository: mirrorgooglecontainers sudo kubeadm init phase preflight --config=./adm.defaults.yaml -v 4  Self-hosting the Kubernetes control plane As of 1.8, you can experimentally create a self-hosted Kubernetes control plane. This means that key components such as the API server, controller manager, and scheduler run as DaemonSet pods configured via the Kubernetes API instead of static pods configured in the kubelet via static files. To create a self-hosted cluster, pass the flag \u0026ndash;feature-gates=SelfHosting=true to kubeadm init.\n  https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/ https://discuss.kubernetes.io/t/question-about-etcd-cluster-with-kubeadm-in-1-11/1228\nkubectl get configmaps --all-namespaces kubectl describe configmaps kubeadm-config -n kube-system kubectl -n kube-system get deployment coredns -o yaml | \\ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\ kubectl apply -f - kubectl scale --current-replicas=2 --replicas=1 deployments.apps/nginx1-14 kubectl logs calico-node-4mb5z -n kube-system  ","date":1549856307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549856307,"objectID":"3b9aedf05dacf02f125f31d5b45ac452","permalink":"https://wubigo.com/post/kubeamd-cheat-sheet/","publishdate":"2019-02-11T11:38:27+08:00","relpermalink":"/post/kubeamd-cheat-sheet/","section":"post","summary":"version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods  kubectl get pods --field-selector=status.phase=Pending   images list  kubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.","tags":["K8S"],"title":"kubeamd cheat sheet","type":"post"},{"authors":null,"categories":null,"content":" track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently. In contrast, HTTP hosted on Transmission Control Protocol (TCP) can be blocked if any of the multiplexed data streams has an error.\n reduced connection and transport latency, and bandwidth estimation in each direction to avoid congestion. It also moves control of the congestion avoidance algorithms into the application space at both endpoints, rather than the kernel space, which it is claimed will allow these algorithms to improve more rapidly. Additionally, the protocol can be extended with forward error correction (FEC) to further improve performance when errors are expected\n  [1]. revive Gopherspace\n[2]. gopher client\n","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"ef107e3fcd63362281edd05426733f5a","permalink":"https://wubigo.com/post/2019-02-01-http3notes/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/post/2019-02-01-http3notes/","section":"post","summary":"track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently.","tags":["HTTP","WEB"],"title":"HTTP/3","type":"post"},{"authors":null,"categories":[],"content":"","date":1548468540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548468540,"objectID":"e54608f0590b1b256fd92ccf0842db3b","permalink":"https://wubigo.com/post/cni_l2_network_on_bare_metal/","publishdate":"2019-01-26T10:09:00+08:00","relpermalink":"/post/cni_l2_network_on_bare_metal/","section":"post","summary":"","tags":["K8S","CNI","NETWORK"],"title":"K8SCNI之L2 网络实现","type":"post"},{"authors":null,"categories":null,"content":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"80247d4c6c90ebe4d510bb399f0bb85b","permalink":"https://wubigo.com/post/2018-11-24-microk8s/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-24-microk8s/","section":"post","summary":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf","tags":["K8S"],"title":"MicroK8S","type":"post"},{"authors":null,"categories":null,"content":" generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git\nReferences https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"24686d6a0d054aaf9d5cb425834973cf","permalink":"https://wubigo.com/post/2018-11-28-jupyternotebook/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-28-jupyternotebook/","section":"post","summary":"generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git","tags":["PYTHON"],"title":"entry into jupyter notebook","type":"post"},{"authors":null,"categories":null,"content":" The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features. Moreover, kubenet has many limitations. For instance, when running kubenet in AWS Cloud, you are limited to 50 EC2 instances. Route tables are used to configure network traffic between Kubernetes nodes, and are limited to 50 entries per VPC. Moreover, a cluster cannot be set up in a Private VPC, since that network topology uses multiple route tables. Other more advanced features, such as BGP, egress control, and mesh networking, are only available with different CNI providers.\nCNI in kops At last count, kops supports seven different CNI providers besides kubenet. Choosing from seven different network providers is a daunting task.\nHere is our current list of providers that can be installed out of the box, sorted in alphabetical order.\nCalico Canal (Flannel + Calico) flannel kopeio-vxlan kube-router romana Weave Net Any of these CNI providers can be used without kops. All of the CNI providers use a daemonset installation model, where their product deploys a Kubernetes Daemonset. Just use kubectl to install the provider on the master once the K8S API server has started. Please refer to each projects specific documentation\nSupport Matrix a table of different features from each of the CNI providers mentioned:\n| Provider | Network Model| Route Distribution|Network Policies|Mesh | |External Datastore|Encryption|Ingress/Egress Policies| Commercial Support| | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | :\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;: |:\u0026mdash;\u0026mdash;\u0026ndash; | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: |:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; : | | Calico | Layer 3 | Yes |Yes | Etcd | Yes | Yes | | flannel | Layer 2 vxlan| Mo |No | None | No | No | | Weave | Layer 2 vxlan| N/A |Yes | No | Yes | Yes |\n Calico and Canal include a feature to connect directly to Kubernetes, and not use Etcd. Weave Net can operate in AWS-VPC mode without vxlan, but is limited to 50 nodes in EC2. Weave Net does not have egress rules out of the box.  Table Details\nNetwork Model The Network Model with providers is either encapsulated networking such as VXLAN, or unencapsulated layer 2 networking. Encapsulating network traffic requires compute to process, so theoretically is slower. In my opinion, most use cases will not be impacted by the overhead. More about VXLAN on wikipedia.\nRoute Distribution For layer 3 CNI providers, route distribution is necssary. Route distribution is typically via BGP. Route distribution is nice to have a feature with CNI, if you plan to build clusters split across network segments. It is an exterior gateway protocol designed to exchange routing and reachability information on the internet. BGP can assist with pod to pod networking between clusters.\nNetwork Policies A kubernetes.io blog post about network policies in 1.8 here.\nKubernetes now offers functionality to enforce rules about which pods can communicate with each other using network policies. This feature is has become stable Kubernetes 1.7 and is ready to use with supported networking plugins. The Kubernetes 1.8 release has added better capabilities to this feature.  Mesh Networking This feature allows for “Pod to Pod” networking between Kubernetes clusters. This technology is not Kubernetes federation, but is pure networking between Pods.\nEncyption Encrypting the network control plane, so all TCP and UDP traffic is encrypted.\nIngress / Egress Policies The network policies are both Kubernetes and Non-Kubernetes routing control. For instance, many providers will allow an administrator to block a pod communicating with an EC2 instance meta and data service on 169.254.169.254.\nSummary If you do not need the advanced features that a CNI provider delivers, use kubenet. It is stable, and fast. Otherwise, pick one. If you do need run more than 50 nodes on AWS, or need other advanced features, make a decision quickly (don’t spend days deciding), and test with your cluster. File bugs, and develop a relationship with your network provider. At this point in time, networking is not boring in Kubernetes. It is getting more boring every day! Monitor test and monitor more.\nhttps://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/\n","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542844800,"objectID":"a61dafcd727687faf6d4e6ce3be57298","permalink":"https://wubigo.com/post/2018-11-22-cninetworkproviderforkubernetes/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/post/2018-11-22-cninetworkproviderforkubernetes/","section":"post","summary":"The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features.","tags":["K8S","CNI","DOCKER"],"title":"Choosing a CNI Network Provider for Kubernetes","type":"post"},{"authors":null,"categories":null,"content":" Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n  $bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.log 2\u0026gt;\u0026amp;1 $bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg $pip install /tmp/tensorflow_pkg/tensorflow-\u0026lt;blah\u0026gt;.whl $python -c 'import tensorflow as tf; print(tf.__version__)' $pip list | grep tensorflow  network-performance-monitoring https://github.com/tensorflow/tensorflow/issues/23402\n","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"e37caefe6bf0fd93fa4d32a82a6041bf","permalink":"https://wubigo.com/post/2018-11-08-buildtensorflow-1-12/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/post/2018-11-08-buildtensorflow-1-12/","section":"post","summary":"Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n  $bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.","tags":["DEEPLEARNING","TENSORFLOW"],"title":"build tensorflow 1.12","type":"post"},{"authors":null,"categories":null,"content":" ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate:\ton RX:\ton TX:\ton  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user\nsudo systemctl disable/stop irqbalance  network-performance-monitoring https://opensourceforu.com/2016/10/network-performance-monitoring/\nLinux Network (TCP) Performance Tuning with Sysctl https://www.slashroot.in/linux-network-tcp-performance-tuning-sysctl\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"dbee25a2eefe542b5e9c76a69361b6bc","permalink":"https://wubigo.com/post/2018-11-07-linuxperformancetuning/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/2018-11-07-linuxperformancetuning/","section":"post","summary":"ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate:\ton RX:\ton TX:\ton  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user","tags":["LINUX"],"title":"Linux performance","type":"post"},{"authors":null,"categories":null,"content":"The most important conversation you ever have is the one with yourself\n","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"fec0f9fdbade4d5491c25a75127e566b","permalink":"https://wubigo.com/post/2019-01-01-sevenlevelcommunication/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/post/2019-01-01-sevenlevelcommunication/","section":"post","summary":"The most important conversation you ever have is the one with yourself","tags":["communication"],"title":"7 level communication","type":"post"},{"authors":null,"categories":null,"content":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523059200,"objectID":"db686c5b514d2e320e7018c5b058cc03","permalink":"https://wubigo.com/post/2018-04-07-dgraphnotes/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/post/2018-04-07-dgraphnotes/","section":"post","summary":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","tags":["NOSQL","GRAPH"],"title":"Dgraph note","type":"post"},{"authors":null,"categories":["LIFE"],"content":" 你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.\n 会用好上司的长处，才是你高效能的关键\n（文章来源于：维小维生素摘编）\n","date":1522195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522195200,"objectID":"ea851af3bade29bf69c3197f95ac6ee4","permalink":"https://wubigo.com/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","publishdate":"2018-03-28T00:00:00Z","relpermalink":"/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","section":"post","summary":"你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.","tags":null,"title":"最重要的人脉","type":"post"},{"authors":null,"categories":null,"content":" JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:\n Java 6 JCE\n Java 7 JCE\n Java 8 JCE\n  Connecting to SSL Server from eclipse Append the following to use keystore in eclipse tomcat server\n-Djavax.net.ssl.trustStore=\u0026quot;C:\\Program Files\\Java\\jdk1.8.0_121\\jre\\lib\\ security\u0026quot;  check certificate name by alias then remove from keystore files $keytool -list -v -keystore cacerts | grep 'Alias name:' $sudo keytool -delete -alias wubigo -keystore cacerts  ","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"148bab8c32d2df718a63a191be71e3f6","permalink":"https://wubigo.com/post/2018-03-07-connectingtosslservices/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-07-connectingtosslservices/","section":"post","summary":"JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:","tags":null,"title":"Connecting to SSL services","type":"post"},{"authors":null,"categories":["LIFE"],"content":" 导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。\n许多一流的笔杆子都不是好学生——温斯顿·邱吉尔就是一例。\n有关这个问题的解释是，笔头好的人一般不靠听和读来学习，而靠写来学习，这已成了一种规律。学校不让他们以这种方式学习，所以他们的成绩总是很糟糕。\n实际上，学习大概有六七种不同的方式。\n像邱吉尔这样的人靠写来学习，还有些人以详尽的笔记来学习。有些人在实干中学习，另一些人通过听自己讲话学习。\n我属于读者型还是听者型？我如何学习？这是你首先要问自己的问题。\n但光这些问题显然不够。要想做好自我管理，你还需要问这样的问题：我能与别人合作得好吗？还是喜欢单枪匹马？如果你确实有与别人进行合作的能力，你还得问问这个问题：我在怎样的关系下与他人共事？\n有些人最适合当部属。\n 二战时期美国的大英雄乔治·巴顿将军是一个很好的例子。 巴顿是美军的一名高级将领。然而，当有人提议他担任独立指挥官时， 美国陆军参谋长、可能也是美国历史上最成功的伯乐， 乔治·马歇尔将军说： 「巴顿是美国陆军造就的最优秀的部下，但是，他会成为最差劲的司令官。」  一些人作为团队成员工作最出色。另一些人单独工作最出色。一些人当教练和导师特别有天赋，另一些人却没能力做导师。\n另一个关键的问题是，我如何才能取得成果——是作为决策者还是作为顾问？许多人做顾问时的表现会很出色，但是不能够承担决策的负担和压力。与此相反，也有许多人需要顾问来迫使他们思考，随后他们才能做出决定，接着迅速、自信和大胆地执行决定。\n顺便说一下，一个组织的二号人物在提升到一号职位时常常失败，也正是因为这个原因。最高职位需要一个决策者，而一个强势的决策者常常把其信赖的人放在二号位置，当他的顾问。\n其他有助于认识自我的重要问题包括：\n 我是在压力下表现出色，还是适应一种按部就班、可预测的工作环境？ 我是在一个大公司还是在一个小公司中工作表现最佳？  我不止一次地看到有些人在大公司中十分成功，换到小公司中则很不顺利。\n反过来也是如此。\n下面这个结论值得我们反复强调：不要试图改变自我，因为这样你不大可能成功。但是，你应该努力改进你的工作方式。另外，不要从事你干不了或干不好的工作。\n我们的价值观是什么 要能够自我管理，你最后不得不问的问题是：我的价值观是什么？这不是一个有关伦理道德的问题。道德准则对每一个人都一样。要对一个人的道德进行测试，方法很简单，我把它称为「镜子测试」。\n 20 世纪初，德国驻英国大使是当时在伦敦所有大国中最受尊重的一位外交官。 显然，他命中注定会承担重任，即使不当本国的总理，至少也要当外交部长。 然而，在 1906 年，他突然辞职，不愿主持外交使团为英国国王爱德华七世举行的晚宴。 这位国王是一个臭名昭著的色鬼，并且明确表示他想出席什么样的晚宴。 据有关报道，这位德国大使曾说： 「我不想早晨刮脸时在镜子里看到一个皮条客。」  这就是镜子测试。\n我们所尊从的伦理道德要求你问自己：我每天早晨在镜子里想看到一个什么样的人？在一个组织或一种情形下合乎道德的行为，在另一个组织或另一种情形下也是合乎道德的。但是，道德只是价值体系的一部分——尤其对于一个组织的价值体系来说。\n如果一个组织的价值体系不为自己所接受或者与自己的价值观不相容，人们就会备感沮丧，工作效力低下。\n一个人的工作方式和他的长处很少发生冲突，相反，两者能产生互补。但是，一个人的价值观有时会与他的长处发生冲突。\n我们属于何处 少数人很早就知道他们属于何处。\n比如，数学家、音乐家和厨师，通常在四五岁的时候就知道自己会成为数学家、音乐家和厨师了。物理学家通常在十几岁甚至更早的时候就决定了自己的工作生涯。\n但是，大多数人，尤其是很有天赋的人，至少要过了二十五六岁才知道他们将身属何处。\n然而，到这个时候，他们应该知道上面所谈的三个问题的答案：\n- 我的长处是什么？ - 我的工作方式是怎样的？ - 我的价值观是什么？  随后，他们就能够并且应该决定自己该向何处投入精力。或者，他们应该能够决定自己不属于何处。\n已经知道自己在大公司里干不好的人，应该学会拒绝在一个大公司中任职。已经知道自己不适合担任决策者的人，应该学会拒绝做决策工作。\n成功的事业不是预先规划的，而是在人们知道了自己的长处、工作方式和价值观后，准备把握机遇时水到渠成的。知道自己属于何处，可使一个勤奋、有能力但原本表现平平的普通人，变成出类拔萃的工作者。\n我该做什么贡献 综观人类的发展史，绝大多数人永远都不需要提出这样一个问题：我该做出什么贡献？因为他们该做出什么贡献是由别人告知的，他们的任务或是由工作本身决定的（例如农民或工匠的任务），或是由主人决定的（例如佣人的任务）。\n对于知识工作者来说，他们不得不提出一个以前从来没有提出过的问题：我的贡献应该是什么？\n要回答这个问题，他们必须考虑三个不同的因素：\n- 当前形势的要求是什么？ - 鉴于我的长处、我的工作方式以及我的价值观，我怎样才能对需要完成的任务做出最大贡献？ - 最后，必须取得什么结果才能产生重要影响？  一般来说，一项计划的时间跨度如果超过了 18 个月，就很难做到明确和具体。\n因此，在多数情况下我们应该提出的问题是：\n- 我在哪些方面能取得将在今后一年半内见效的结果？ - 如何取得这样的结果？  回答这个问题时必须对几个方面进行权衡。\n首先，这些结果应该是比较难实现的，要有「张力」 （stretching）。但这些结果也应该是能力所及的。\n其次，这些结果应该富有意义，要能够产生一定影响。\n最后，结果应该明显可见，如果可能的话，还应当能够衡量。确定了要实现的结果之后，接着就可以制订行动方针：做什么，从何处着手，如何开始，目标是什么，在多长时间内完成。\n对人际关系负责 除了少数伟大的艺术家、科学家和运动员，很少有人是靠自己单枪匹马而取得成果的。不管是组织成员还是个体职业者，大多数人都要与别人进行合作，并且是有效的合作。要实现自我管理，你需要对自己的人际关系负起责任。这包括两部分内容。\n首先要接受别人是和你一样的个体这个事实。\n他们有自己的长处，自己的做事方式和价值观。因此，要想卓有成效，你就必须知道共事者的长处、工作方式和价值观。\n这个道理听起来让人很容易明白，但是没有几个人真正会去注意。\n一个习惯于写报告的人就是个典型的例子——他在第一份工作时就培养起写报告的习惯，因为他的老板是一个读者型的人，而即使下一个老板是个听者型，此人也会继续写着那肯定没有任何结果的报告。这位老板因此肯定会认为这个员工愚蠢、无能、懒惰，肯定干不好工作。但是，如果这个员工事先研究过新老板的情况，并分析过这位老板的工作方式，这种情况本来可以避免。\n老板既不是组织结构图上的一个头衔，也不是一个「职能」。他们是有个性的人，他们有权以自己最得心应手的方式来工作。与他们共事的人有责任观察他们，了解他们的工作方式，并做出相应的自我调整，去适应老板最有效的工作方式。\n事实上，这就是「管理」上司的秘诀\n这种方法适用于所有与你共事的人。至于工作方式，人各有别。提高效力的第一个秘诀是了解跟你合作和你要依赖的人，以利用他们的长处、工作方式和价值观。工作关系应当既以工作为基础，也以人为基础。\n人际关系责任的第二部分内容是沟通责任。\n在我或是其他人开始给一个组织做咨询时，我们听到的第一件事都与个性冲突有关。其中大部分冲突都是因为：人们不知道别人在做什么，他们又是采取怎样的工作方式，专注于做出什么样的贡献以及期望得到怎样的结果。而这些人不了解情况的原因是，他们没有去问，结果也就不得而知。\n即使一些人懂得负起人际关系责任的重要性，他们和同事的交流也往往不够。他们总是有所顾虑，怕别人把自己看成是一个冒昧、愚蠢、爱打听的人。他们错了。\n因为我们看到，每当有人找到他的同事说「这是我所擅长的工作。这是我的做事方式。这是我的价值观。这是我计划做出的贡献和应当取得的成果」，这个人总会得到如此回答：「这太有帮助了，可你为什么不早点告诉我？」\n如果一个人继续问道：「那么，关于你的长处、你的工作方式、你的价值观以及你计划做出的贡献，我需要知道什么？」他也会得到类似的答复——据我的经验，无一例外。\n事实上，知识工作者应该向与他们共事的每一个人，不管是下属、上司、同事还是团队成员，都发出这样的疑问。\n组织已不再建立在强权的基础上，而是建立在信任的基础上。人与人之间相互信任，不一定意味着他们彼此喜欢对方，而是意味着彼此了解。因此，人们绝对有必要对自己的人际关系负责。\n这是一种义务。不管一个人是公司的一名成员，还是公司的顾问、供应商或经销商，他都需要对他的所有共事者负起这种责任。所谓共事者，是指在工作上他所依赖的同事以及依赖他的同事。\n管理后半生 我们听到了许多有关经理人中年危机的谈论，「厌倦」这个词在其中频频出现。\n45 岁时，多数经理人的职业生涯达到了顶峰。但是他们学不到新东西，也没有什么新贡献，从工作中得不到挑战，因而也谈不上满足感。在他们面前，还有 20 到 25 年的职业道路要走。这就是为什么经理人在进行自我管理后，越来越多地开始发展第二职业的原因。\n发展第二职业有三种方式：\n 第一种是完全投身于新工作。  这常常只需要从一种组织转到另一种组织。\n例如，一家大公司某事业部的会计师成为一家中型医院的财务总监。\n但是也有越来越多的人转入完全不同的职业。还有许多人在第一份职业中取得的成功有限，于是改行从事第二职业。这样的人有很多技能，他们也知道该如何工作。\n为后半生做准备的第二种方式是， - 发展一个平行的职业。\n许多人的第一职业十分成功，他们还会继续从事原有工作。除此之外，他们会开创一项平行的工作，通常是在非营利机构。\n 最后一种方法是社会创业。  社会创业者通常是在第一职业中非常成功的人士。他们都热爱自己的工作，但是这种工作对他们已经不再有挑战性。\n他们虽然继续做着原来的工作，但在这份工作上花的时间越来越少。他们同时开创了另一项事业，通常是非营利性活动。\n管理好自己后半生的人可能总是少数。多数人可能数着年头一年一年过去，直至退休。但正是这些少数人，这些把漫长的工作寿命看做是自己和社会之机会的人，才会成为领袖和模范。\n管理好后半生有一个先决条件：你必须早在你进入后半生之前就开始行动。当 30 年前人们首次认识到工作寿命正在迅速延长时，许多观察家（包括我自己）认为，退休人员会越来越多地成为非营利机构的志愿者。可是，这种情况并没有发生。一个人如果不在 40 岁之前就开始做志愿者，那他 60 岁之后也不会去做志愿者。\n同样，我认识的所有社会创业者，都是早在他们原有的事业达到顶峰之前就开始从事他们的第二事业。\n发展第二兴趣还有一个原因：任何人都不能指望在生活或工作中很长时间都不遭遇严重挫折。在这样的时刻，第二兴趣——不仅仅是业余爱好——可能发挥重要作用。\n在一个崇尚成功的社会里，拥有各种选择变得越来越重要。在知识社会里，我们期望每一个人都能取得成功。这显然是不可能的。\n对许多人来说，能避免失败就行。可是有成功的地方，就会有失败。因此，有一个能够让人们做出贡献、发挥影响力或成为「大人物」的领域，这不仅对个人十分重要，对个人的家庭也同样重要。\n这意味着人们需要找到一个能够有机会成为领袖、受到尊重、取得成功的第二领域——可能是第二份职业。\n自我管理中面临的挑战看上去比较明显。但自我管理需要我们做出以前从未做过的事情。自我管理需要每一个知识工作者在思想和行动上都要成为自己的首席执行官。\n更进一步来看，这样的转变——从一切听从别人吩咐的体力劳动者到不得不自我管理的知识工作者——也使得社会结构发生了深刻变化。\n历史上每一个社会，甚至是个人主义倾向最强的社会，都认为两件事情理所当然（即使只是下意识的）：\n- 第一，组织比员工更长寿； - 第二，大多数人从不挪地方。  如今，情况恰恰相反。知识工作者的寿命超过了组织寿命，而且他们来去自如。\n于是，人们对自我管理的需要在人类事务中掀起了一场革命。\n","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"83ad231316907510eb063abd4a52482f","permalink":"https://wubigo.com/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","section":"post","summary":"导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。","tags":null,"title":"自我管理","type":"post"},{"authors":null,"categories":null,"content":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).\nA file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS to one or more clients on-premises.\n","date":1519948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519948800,"objectID":"916e22deac7af087ed6cf28ed5f5184c","permalink":"https://wubigo.com/post/2018-03-02-filegatewayforhybridarchitectures/","publishdate":"2018-03-02T00:00:00Z","relpermalink":"/post/2018-03-02-filegatewayforhybridarchitectures/","section":"post","summary":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).","tags":null,"title":"File Gateway for Hybrid Architectures; Overview and Best Practices","type":"post"},{"authors":null,"categories":[],"content":"","date":1519614668,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519614668,"objectID":"5eb1c4f6bb98d786e4e2e3fa3b56c449","permalink":"https://wubigo.com/post/k8s_cni_kube-router/","publishdate":"2018-02-26T11:11:08+08:00","relpermalink":"/post/k8s_cni_kube-router/","section":"post","summary":"","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Kube Router实现","type":"post"},{"authors":null,"categories":[],"content":"","date":1519614647,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519614647,"objectID":"8586d462dfbc6d894d5fa6a3818c6442","permalink":"https://wubigo.com/post/k8s_cni_calico/","publishdate":"2018-02-26T11:10:47+08:00","relpermalink":"/post/k8s_cni_calico/","section":"post","summary":"","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Calico实现","type":"post"},{"authors":null,"categories":[],"content":" PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet\nIn local clusters, the default StorageClass uses the hostPath provisioner. hostPath volumes are only suitable for development and testing. With hostPath volumes, the data lives in /tmp on the node the Pod is scheduled onto and does not move between nodes\nProvisioning There are two ways PVs may be provisioned: statically or dynamically.\n Static  A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\n Dynamic  When none of the static PVs the administrator created matches a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class in order for dynamic provisioning to occur. Claims that request the class \u0026ldquo;\u0026rdquo; effectively disable dynamic provisioning for themselves.\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the \u0026ndash;enable-admission-plugins flag of the API server component. For more information on API server command line flags, please check kube-apiserver documentation.\n","date":1519426553,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519426553,"objectID":"ef1314c59426d86855db66979c79537e","permalink":"https://wubigo.com/post/k8s-csi/","publishdate":"2018-02-24T06:55:53+08:00","relpermalink":"/post/k8s-csi/","section":"post","summary":"PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet","tags":["STORAGE","K8S","CSI"],"title":"K8S CSI","type":"post"},{"authors":null,"categories":null,"content":" CSP “Another lineage among Go’s ancestors, and one that makes Go distinctive among recent programming languages, is a sequence of little-known research languages developed at Bell Labs, all inspired by the concept of communicating sequential processes (CSP) from Tony Hoare’s seminal 1978 paper on the foundations of concurrency. In CSP, a program is a parallel composition of processes that have no shared state; the processes communicate and synchronize using channels.”\nhindsight “As a recent high-level language, Go has the benefit of hindsight, and the basics are done well: it has garbage collection, a package system, first-class functions, lexical scope, a system call interface, and immutable strings in which text is generally encoded in UTF-8. But it has comparatively few features and is unlikely to add more. For instance, it has no implicit numeric conversions, no constructors or destructors, no operator overloading, no default parameter values, no inheritance, no generics, no exceptions, no macros, no function annotations, and no thread-local storage”\ngofmt Go does not require semicolons at the ends of statements or declarations, except where two or more appear on the same line. In effect, newlines following certain tokens are converted into semicolons, so where newlines are placed matters to proper parsing of Go code. For instance, the opening brace { of the function must be on the same line as the end of the func declaration, not on a line by itself, and in the expression x + y, a newline is permitted after but not before the + operator.\npoint var  “The variable to which p points is written *p. The expression *p yields the value of that variable, an int, but since *p denotes a variable, it may also appear on the left-hand side of an assignment, in which case the assignment updates the variable.” “Expressions that denote variables are the only expressions to which the address-of operator \u0026amp; may be applied.” “Each time we take the address of a variable or copy a pointer, we create new aliases or ways to identify the same variable. For example, *p is an alias for v.”  Function Values have types declare a var with function value as type * func(int) int -\u0026gt; type\nvar f func(int) int  nil pointer dereference package main import \u0026quot;fmt\u0026quot; var f func(int) int func main() { //fmt.Println(f(2)) fmt.Println(f(2)) f = func(i int) int { if i == 0 { return 1 } return i * f(i-1) } fmt.Println(f(2)) } panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4850f0]goroutine 1 [running]:main.main()\t/home/bigo/go/src/github.com/gopl.io/ch5/t2/main.go:9 +0x30exit status 2Process exiting with code: 1  Capturing Iteration Variables “The problem of iteration variable capture is most often encountered when using the go statement or with defer since both may delay the execution of a function value until after the loop has finished. But the problem is not inherent to go or defer.”\nvar s1 [3]int = [3]int{1, 2, 3} var fs []func() func main() { for _, v := range s1 { //fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) //v is caputed and shared }) } for _, f := range fs { f() } fs = nil for _, v := range s1 { i := v fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, i) }) } for _, f := range fs { f() } }  The reason is a consequence of the scope rules for loop variables. In the program immediately above, the for loop introduces a new lexical block in which the variable dir is declared. All function values created by this loop “capture” and share the same variable—an addressable storage location, not its value at that particular moment. The value of dir is updated in successive iterations, so by the time the cleanup functions are called, the dir variable has been updated several times by the now-completed for loop.\nexpression in go/defer must be function call  express anonymous function as function call()  var s1 [3]string = [3]string{\u0026quot;go\u0026quot;, \u0026quot;func\u0026quot;, \u0026quot;value\u0026quot;} func f(s string) { fmt.Println(s) } func main() { fmt.Println(\u0026quot;s escapsed\u0026quot;) for _, s := range s1 { defer func() { fmt.Println(s) }() } fmt.Println(\u0026quot;work expected\u0026quot;) for _, s := range s1 { defer func(s string) { fmt.Println(s) }(s) } for _, s := range s1 { defer f(s) } for _, s := range s1 { go func(s string) { fmt.Println(\u0026quot;h %s\u0026quot;, s) }(s) } for _, s := range s1 { go f(s) } }  install go-1.10 ***vscode will report go-runtime read-only error which has been installed via snap\nsudo add-apt-repository ppa:gophers/archive sudo apt-get update sudo apt-get install golang-1.10-go export GOROOT=\u0026quot;/usr/lib/go-1.10\u0026quot; export GOPATH=$HOME/go export PATH=\u0026quot;$PATH:$GOPATH/bin:$GOROOT/bin/\u0026quot; go version  ","date":1518307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518307200,"objectID":"86dffc1bfeabf8bb66c97bf766180a48","permalink":"https://wubigo.com/post/2018-02-11-gonotes/","publishdate":"2018-02-11T00:00:00Z","relpermalink":"/post/2018-02-11-gonotes/","section":"post","summary":"CSP “Another lineage among Go’s ancestors, and one that makes Go distinctive among recent programming languages, is a sequence of little-known research languages developed at Bell Labs, all inspired by the concept of communicating sequential processes (CSP) from Tony Hoare’s seminal 1978 paper on the foundations of concurrency. In CSP, a program is a parallel composition of processes that have no shared state; the processes communicate and synchronize using channels.","tags":null,"title":"go notes","type":"post"},{"authors":null,"categories":null,"content":" Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/\n","date":1517529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517529600,"objectID":"b2e1d541771bcdee60a291eb52a62110","permalink":"https://wubigo.com/post/2018-02-02-dynamicroutingbetweencapsules/","publishdate":"2018-02-02T00:00:00Z","relpermalink":"/post/2018-02-02-dynamicroutingbetweencapsules/","section":"post","summary":"Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/","tags":null,"title":"Understanding Dynamic Routing between Capsules","type":"post"},{"authors":null,"categories":null,"content":" enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out bigo.crt -days 500 kubectl config set-credentials bigo --client-certificate=./bigo.crt --client-key=./bigo.key kubectl config set-context bigo-context --cluster=kubernetes --namespace=bigo-NS --user=bigo kubectl config get-contexts ... CURRENT NAME CLUSTER AUTHINFO NAMESPACE bigo-context kubernetes bigo bigo * kubernetes-admin@kubernetes kubernetes kubernetes-admin ...  binding role to user cat rolebinding-bigo-access.yaml kind: RoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: access-manager-binding namespace: bigo-NS subjects: - kind: User name: bigo apiGroup: \u0026quot;\u0026quot; roleRef: kind: Role name: access-role apiGroup: \u0026quot;\u0026quot; kubectl create -f rolebinding-bigo-access.yaml  USER, GROUP, ROLE , ROLEBIND, RBAC  list all users ``` kubectl config view \u0026hellip; users: name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED \u0026hellip; ```  Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or ``` cat tiller-clusterrolebinding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects:\n kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026ldquo;\u0026rdquo;\n  kubectl create -f tiller-clusterrolebinding.yaml\nUpdate the existing tiller-deploy deployment with the Service Account helm init \u0026ndash;service-account tiller \u0026ndash;upgrade\n  helm install \u0026ndash;name prometheus stable/prometheus\nhelm install \u0026ndash;name prometheus1 stable/prometheus \u0026ndash;set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false,\n alertmanager: enabled: false name: p-alertmanager server: name: prometheus880  PVC using local PV  create PVC\ncat storage-class-hdd.yaml apiVersion: storage.K8S.io/v1 kind: StorageClass metadata: name: local-hdd provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer   kubectl apply -f storage-class-hdd.yaml\n create local PV ``` cat local_volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: local-hdd spec: capacity: storage: 8Gi volumeMode: Filesystem accessModes:\n ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-hdd local: path: /mnt/pv/ nodeAffinity: required: nodeSelectorTerms:  matchExpressions: key: kubernetes.io/hostname operator: In values:  bigo-vm4 ``` \u0026gt;kubectl apply -f local_volume.yaml     PersistentVolume nodeAffinity is required when using local volumes. It enables the Kubernetes scheduler to correctly schedule Pods using local volumes to the correct node.\nPersistentVolume volumeMode can now be set to “Block” (instead of the default value “Filesystem”) to expose the local volume as a raw block device. The volumeMode field requires BlockVolume Alpha feature gate to be enabled.\nWhen using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. See the example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity\nPort Forwarding a local port to a port on K8S kubectl port-forward \u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward pods/\u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward deployment/prometheus 9090:9090 or kubectl port-forward svc/prometheus 9090:9090 or kubectl port-forward rs/prometheus 9090:9090  ","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"2032d62dae500d156fb88e01ef96d3a3","permalink":"https://wubigo.com/post/2018-01-11-kubectlcheatsheet/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/post/2018-01-11-kubectlcheatsheet/","section":"post","summary":"enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out bigo.crt -days 500 kubectl config set-credentials bigo --client-certificate=./bigo.crt --client-key=./bigo.key kubectl config set-context bigo-context --cluster=kubernetes --namespace=bigo-NS --user=bigo kubectl config get-contexts ... CURRENT NAME CLUSTER AUTHINFO NAMESPACE bigo-context kubernetes bigo bigo * kubernetes-admin@kubernetes kubernetes kubernetes-admin .","tags":["K8S"],"title":"kubectl cheat sheet","type":"post"},{"authors":null,"categories":null,"content":" Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with. At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all. * Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts. * Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution. * Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.\n","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"10b6c1db1cfd215584023df7ed5469c7","permalink":"https://wubigo.com/post/2018-01-07-webtestingautomation/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/post/2018-01-07-webtestingautomation/","section":"post","summary":"Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with.","tags":null,"title":"web testing automation","type":"post"},{"authors":null,"categories":null,"content":" Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/\n","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"70d65632982e837a935dc764b934fd16","permalink":"https://wubigo.com/post/2018-01-02-deathofmicroservicemadness/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/post/2018-01-02-deathofmicroservicemadness/","section":"post","summary":"Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/","tags":null,"title":"Death of Microservice Madness","type":"post"},{"authors":null,"categories":null,"content":" Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.aerogrammestudio.com/2013/03/07/pixars-22-rules-of-storytelling/\nKeybase is now supported by the Stellar Development Foundation https://keybase.io/blog/keybase-stellar\nMachine Learning Crash Course https://developers.google.com/machine-learning/crash-course/\nThe Makefile I use with JavaScript projects http://www.olioapps.com/blog/the-lost-art-of-the-makefile/\nHow GDPR Will Change The Way You Develop https://www.smashingmagazine.com/2018/02/gdpr-for-web-developers/\nUber and Waymo Reach Settlement https://www.uber.com/newsroom/uber-waymo-settlement/\nPostmortem of Service Outage at 3.4M Concurrent Users https://www.epicgames.com/fortnite/en-US/news/postmortem-of-service-outage-at-3-4m-ccu\nPerspective: Streaming pivot visualization via WebAssembly https://github.com/jpmorganchase/perspective\nTinc VPN: Secure Private Network Between Hosts https://www.tinc-vpn.org/\nA reimplementation of Winamp 2.9 in HTML5 and Javascript https://github.com/captbaritone/winamp2-js\nWhat I Learned Burning $14k on YouTube Ads for Candy Japan https://www.candyjapan.com/behind-the-scenes/what-i-learned-advertising-on-youtube\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d136c19d4861d8c757ecf23594c7153f","permalink":"https://wubigo.com/post/2018-01-01-hacknewsfavorites2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/post/2018-01-01-hacknewsfavorites2018/","section":"post","summary":"Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.","tags":null,"title":"Hacknews favorites 2018","type":"post"},{"authors":null,"categories":null,"content":" Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1514592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514592000,"objectID":"d3ca72d17b15012cf75563e378e0a9b8","permalink":"https://wubigo.com/post/2017-12-30-booksireadthisyear/","publishdate":"2017-12-30T00:00:00Z","relpermalink":"/post/2017-12-30-booksireadthisyear/","section":"post","summary":"Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017","tags":null,"title":"Books I read this year","type":"post"},{"authors":null,"categories":null,"content":" Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90\n","date":1512172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512172800,"objectID":"d6c0dd10ab9b50518ec7bde144b723e2","permalink":"https://wubigo.com/post/2017-12-02-machinelearning101slidedeck/","publishdate":"2017-12-02T00:00:00Z","relpermalink":"/post/2017-12-02-machinelearning101slidedeck/","section":"post","summary":"Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90","tags":null,"title":"Machine Learning 101 slidedeck","type":"post"},{"authors":null,"categories":["IT"],"content":" On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data. Customization: Colors,labels,thickness of line, title, opacity, grid, figsize, ticks of axis and linestyle  How TensorBoard gets data from TensorFlow The first step in using TensorBoard is acquiring data from your TensorFlow run. For this, you need summary ops. Summary ops are ops, like tf.matmul or tf.nn.relu, which means they take in tensors, produce tensors, and are evaluated from within a TensorFlow graph. However, summary ops have a twist: the Tensors they produce contain serialized protobufs, which are written to disk and sent to TensorBoard. To visualize the summary data in TensorBoard, you should evaluate the summary op, retrieve the result, and then write that result to disk using a summary.FileWriter.\nlearning rate Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region The following formula shows the relationship.\nnew_weight = existing_weight — learning_rate * gradient  Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.\nAs such, it’s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate\nmost common ways to prevent overfitting in neural networks  Get more training data. Reduce the capacity of the network. Add weight regularization. Add dropout  activation function A function that takes the input signal and generates an output signal, but takes into account some kind of threshold is called an activation function\nCNN feature map state of art AI https://www.stateoftheart.ai/\nReferences  [1] https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 [2] http://wubigo.com/2017/01/numpy-notes/  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"5b3065c2f225a6419d8aeb3abb124d09","permalink":"https://wubigo.com/post/2017-08-01-deep_learning_with_python/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/post/2017-08-01-deep_learning_with_python/","section":"post","summary":"On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data.","tags":null,"title":"Deep Learning with Python","type":"post"},{"authors":null,"categories":null,"content":" Creating sample user  Create Service Account  dashboard-adminuser.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system   Create ClusterRoleBinding  asumming that cluster-admin exists(provisioned by kubeadmin or kops)\nadminuser-bind-clusteramdin.yaml\napiVersion: rbac.authorization.K8S.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.K8S.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl apply -f dashboard-adminuser.yaml   login with Bearer Token  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')  multi-tenant K8S clusters at network-level:  Namespaces Ingress rules allow/deny and ingress/egress Network Policies Network-aware Zones  Architect a multi-tenant system with kubernetes I don\u0026rsquo;t think there is one document out there really summaries everything. The link below is a bit old but can help outline some of the basics on how they build on K8S. Ultimately the primitives are the same but they abstract namespaces a bit and build it around RBAC. Coupled with a default vxlan (isolated) SDN plugin and their ingress routing, its a compelling multi-tenant solution that provides isolation and quotes at multiple levels.\nOpenshift really just adds some glue (a lot of it being devleoper workflow) on top of Kubernetes. What is nice is that RedHat continues to try and upstream features of origin into K8S where it makes sense.\nhttps://blog.openshift.com/building-kubernetes-bringing-google-scale-container-orchestration-to-the-enterprise/ https://www.reddit.com/r/kubernetes/comments/6qp24h/ask_kubernetes_how_would_you_architect_a/\n","date":1499904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499904000,"objectID":"ad79955de7ecb4cb226bfbd8b1e0dd5d","permalink":"https://wubigo.com/post/2017-07-13-k8snotes/","publishdate":"2017-07-13T00:00:00Z","relpermalink":"/post/2017-07-13-k8snotes/","section":"post","summary":"Creating sample user  Create Service Account  dashboard-adminuser.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system   Create ClusterRoleBinding  asumming that cluster-admin exists(provisioned by kubeadmin or kops)\nadminuser-bind-clusteramdin.yaml\napiVersion: rbac.authorization.K8S.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.K8S.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl apply -f dashboard-adminuser.yaml   login with Bearer Token  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')  multi-tenant K8S clusters at network-level:  Namespaces Ingress rules allow/deny and ingress/egress Network Policies Network-aware Zones  Architect a multi-tenant system with kubernetes I don\u0026rsquo;t think there is one document out there really summaries everything.","tags":["K8S","PAAS","DOCKER"],"title":"K8S notes","type":"post"},{"authors":null,"categories":null,"content":" proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm\n","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"66349127f34fa3acce2c6902605a99b7","permalink":"https://wubigo.com/post/2017-05-03-atomnote/","publishdate":"2017-05-03T00:00:00Z","relpermalink":"/post/2017-05-03-atomnote/","section":"post","summary":"proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm","tags":null,"title":"atom note","type":"post"},{"authors":null,"categories":[],"content":" 准备  安装util-linux  sudo apt install util-linux  /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.255.255.0 dns-nameservers 192.168.1.1  ip route default via 192.168.1.1 dev enp0s3 onlink 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev enp0s8 proto kernel scope link src 192.168.1.16  使用NAT  docker host network  assign a second ip to host interface\nexport SIP=192.168.1.117 sudo ip addr add $SIP/24 dev enp0s3  bind container to SIP host network\ndocker run -it --name web -p ${SIP}:80:80 nginx:1.14-alpine sudo iptables -L DOCKER -v -n Chain DOCKER (1 references) pkts bytes target prot opt in out source destination 7 528 ACCEPT tcp -- !docker0 docker0 0.0.0.0/0 172.17.0.2 tcp dpt:80    sudo iptables -t nat -I POSTROUTING -s 172.17.0.2 \\ -j SNAT --to-source 192.168.1.119 sudo iptables -t nat -L -n -v  使用LINUX网桥  查看网卡的ip  ifconfig enp0s8 enp0s3 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16   创建网桥br-enp0s8并把enp0s8的IP分配给网桥，把enp0s8连接到网桥  brctl addbr br-enp0s8 ip link set br-enp0s8 up ip addr add 192.168.1.16/24 dev br-enp0s8; \\ ip addr del 192.168.1.16/24 dev enp0s8; \\ brctl addif br-enp0s8 enp0s8; \\ ip route del default; \\ ip route add default via 192.168.1.1 dev br-enp0s8 ifconfig br-enp0s8 br-enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16 ifconfig enp0s8 enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68  br-enp0s8和enp0s8拥有相同的HWaddr(Mac地址)\nip route default via 192.168.1.1 dev br-enp0s8 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev br-enp0s8 proto kernel scope link src 192.168.1.16   启动容器  docker run -it --name web -p 80 nginx:1.14-alpine   创建veth接口对web-int/web-ext:  sudo ip link add web-int type veth peer name web-ext   连接veth一端web-ext到网桥  sudo brctl addif br-enp0s8 web-ext   连接veth的另一端web-int连接到容器的网络名字空间  sudo ip link set netns $(docker-pid web) dev web-int sudo nsenter -t $(docker-pid web) -n ip link set web-int up sudo nsenter -t $(docker-pid web) -n ip addr add 192.168.1.117/24 dev web-int   检查容器已经连接到web-int并且ip地址正确分配  docker exec -it web ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:84 errors:0 dropped:0 overruns:0 frame:0 TX packets:21 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:9396 (9.1 KiB) TX bytes:2348 (2.2 KiB) web-int Link encap:Ethernet HWaddr 5A:1D:90:CF:6B:2C inet addr:192.168.1.117 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  docker exec -it web ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 scope link src 172.17.0.2 192.168.1.0/24 dev web-int scope link src 192.168.1.117   设置web-int为容器路由默认接口  sudo nsenter -t $(docker-pid web) -n ip route del default sudo nsenter -t $(docker-pid web) -n ip route add default via 192.168.1.1 dev web-int   测试清理  docker rm web sudo ip link set br-enp0s8 down sudo brctl delbr br-enp0s8  ","date":1493075455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493075455,"objectID":"16cf3f49bb97c1d9ad4d0ad448b4417c","permalink":"https://wubigo.com/post/connect-container-to-host-network/","publishdate":"2017-04-25T07:10:55+08:00","relpermalink":"/post/connect-container-to-host-network/","section":"post","summary":"准备  安装util-linux  sudo apt install util-linux  /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.255.255.0 dns-nameservers 192.168.1.1  ip route default via 192.168.1.1 dev enp0s3 onlink 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.","tags":["DOCKER","NETWORK"],"title":"容器多种方式链接宿主网络","type":"post"},{"authors":null,"categories":null,"content":" Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/\n","date":1491091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491091200,"objectID":"29b330b2e5eaea4b52637c2de1262033","permalink":"https://wubigo.com/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","publishdate":"2017-04-02T00:00:00Z","relpermalink":"/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","section":"post","summary":"Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/","tags":null,"title":"Microservice platform Spring-cloud VS Kubernetes","type":"post"},{"authors":null,"categories":null,"content":" Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use. 2. Scroll to the \u0026ldquo;Alternate Domain Names (CNAMEs)\u0026rdquo; field and enter your domain/subdomain you\u0026rsquo;re using 3. Scroll down to SSL Certificate and change the option to \u0026ldquo;Custom SSL Certificate\u0026rdquo;, then select the certificate you just created in the drop-down list. Scroll the rest of the way down and click \u0026ldquo;Create Distribution\u0026rdquo;.\nChange Distribution CNAME ","date":1488412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488412800,"objectID":"2366dc628bf609e0193702a5b12908f5","permalink":"https://wubigo.com/post/2017-03-02-s3withcustomdomainnameviahttps/","publishdate":"2017-03-02T00:00:00Z","relpermalink":"/post/2017-03-02-s3withcustomdomainnameviahttps/","section":"post","summary":"Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use.","tags":null,"title":"S3 access with custom domain name via https","type":"post"},{"authors":null,"categories":null,"content":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"17dbab90328213a2dc7d8d2b1255dd35","permalink":"https://wubigo.com/post/2017-03-01-jekyll/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/post/2017-03-01-jekyll/","section":"post","summary":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","tags":null,"title":"jekyll notes","type":"post"},{"authors":null,"categories":[],"content":" Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com.\tIN\tA www.wubigo.com.\t1\tIN\tCNAME\twubigo.github.io. wubigo.github.io.\t3462\tIN\tA\t185.199.111.153 wubigo.github.io.\t3462\tIN\tA\t185.199.110.153 wubigo.github.io.\t3462\tIN\tA\t185.199.109.153 wubigo.github.io.\t3462\tIN\tA\t185.199.108.153  ","date":1487998304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487998304,"objectID":"d5e6285fd00911e2570af33a609fbf1e","permalink":"https://wubigo.com/post/dns-config-for-github-pages/","publishdate":"2017-02-25T12:51:44+08:00","relpermalink":"/post/dns-config-for-github-pages/","section":"post","summary":"Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com.\tIN\tA www.wubigo.com.\t1\tIN\tCNAME\twubigo.github.io. wubigo.github.io.\t3462\tIN\tA\t185.199.111.153 wubigo.github.io.\t3462\tIN\tA\t185.199.110.153 wubigo.github.io.\t3462\tIN\tA\t185.","tags":["DNS","GITHUB"],"title":"DNS配置Github Pages","type":"post"},{"authors":null,"categories":["IT"],"content":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install  helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false  ","date":1487852920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487852920,"objectID":"c720be75ff6e1de6ced199d874f29384","permalink":"https://wubigo.com/post/k8s-monitor/","publishdate":"2017-02-23T20:28:40+08:00","relpermalink":"/post/k8s-monitor/","section":"post","summary":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install  helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false  ","tags":["K8S","MONITOR"],"title":"K8S Monitor","type":"post"},{"authors":null,"categories":["IT"],"content":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","date":1487805691,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487805691,"objectID":"f7dd75f40aee72579bcfb6730c1fd8e9","permalink":"https://wubigo.com/post/ubungu-chinese/","publishdate":"2017-02-23T07:21:31+08:00","relpermalink":"/post/ubungu-chinese/","section":"post","summary":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","tags":["LINUX"],"title":"Ubungu Chinese locale","type":"post"},{"authors":null,"categories":null,"content":" Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks. The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined together the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of handwritten digits classification. The resulting network, dubbed \u0026ldquo;LeNet\u0026rdquo;, was used by the US Post Office in the 1990s to automate the reading of ZIP codes on mail envelopes\n逻辑回归(Logistic regression) Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\nIn logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (TRUE, success, pregnant, etc.) or 0 (FALSE, failure, non-pregnant, etc.).\nThe goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest\nhttps://www.medcalc.org/manual/logistic_regression.php\n神经网络 一个分类算法，其输出是样本属于某类别的概率值 P(y==k|x;Θ)\nactivation function is the sigmoid function self.activation_function = lambda x: scipy.special.expit(x) Instead of the usual def() definitions, we use the magic lambda to create a function there and then, quickly and easily. The function here takes x and returns scipy.special.expit(x) which is the sigmoid function. Functions created with lambda are nameless or anonymous, but here we’ve assigned it to the name self.activation_function(). All this means is that whenever someone needs to user the activation function, all they need to do is call self.activation_function()\n","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"4ba4deb87812a03bcc1368ba258e2527","permalink":"https://wubigo.com/post/2017-01-03-machinelearningbasic/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-01-03-machinelearningbasic/","section":"post","summary":"Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks.","tags":null,"title":"machine learning basic","type":"post"},{"authors":null,"categories":null,"content":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"79fe544dceb4c0e98497a333d4b12f8f","permalink":"https://wubigo.com/post/2017-02-03-numpynotes/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-02-03-numpynotes/","section":"post","summary":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","tags":null,"title":"numpy notes","type":"post"},{"authors":null,"categories":null,"content":" The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"9a5b10ed10e57e19a16308435c006b94","permalink":"https://wubigo.com/post/2017-01-01-hacknewsfavorites2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-hacknewsfavorites2017/","section":"post","summary":"The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn","tags":null,"title":"Hacknews favorites 2017","type":"post"},{"authors":null,"categories":null,"content":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3abac9b183c99b7bd38bc7e32bcbd0b5","permalink":"https://wubigo.com/post/2017-01-01-discoursenotes/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-discoursenotes/","section":"post","summary":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","tags":null,"title":"discourse notes","type":"post"},{"authors":null,"categories":null,"content":" Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory\nmail: { from: 'from@email.com', transport: 'SMTP', options: { host: \u0026quot;your-amazon-host\u0026quot;, port: 465, service: \u0026quot;SES\u0026quot;, auth: { user: \u0026quot;your-amazon-user\u0026quot;, pass: \u0026quot;your-amazon-password\u0026quot; } } }  Amazon SES credential can be generated from amazon control panel. From address must be registered and verified as sender.\nfavicon.ico cp /home/ubuntu/favicon.ico /var/www/ghost/versions/1.18.2/core/server/public/  ","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"1e116d493d15f537536b7ff2446fe23c","permalink":"https://wubigo.com/post/2016-12-30-ghostnotes/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-ghostnotes/","section":"post","summary":"Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory","tags":null,"title":"ghost notes","type":"post"},{"authors":null,"categories":null,"content":" good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3. Delegation skills: ability to translate ambiguous directives from above into actionable tasks for the team, ability to arrange tasks so that individuals have the creative freedom to figure out how to do it, predictability in decision-making, ability to shield team from external pressure, ability to give negative feedback immediately, ability to fire low performers before they fester into team morale and culture problems 4. Coaching skills: ability to give targeted advice, ability to match employee\u0026rsquo;s own aspirations and growth objectives to projects, enough technical expertise to point people in the right direction, ability to evaluate skills and hire for them\nDealing with ambiguity and developing resilience http://www.melanieallen.co.uk/articles/dealing-with-ambiguity/\nThe 2 Mental Shifts Highly Successful People Make https://medium.com/personal-growth/the-2-mental-shifts-highly-successful-people-make-7089450c2d7c\nwhy your programmers just want to code https://hackernoon.com/why-your-programmers-just-want-to-code-36da9973388e\nThree Powerful Conversations Managers Must Have To Develop Their People  Starting with kindergarten, tell me about your life\n Spot their lighthouse and bring it into focus “The idea is to try to get employees to start to talk to you about their dreams, or three to five of them if they don\u0026rsquo;t really want to commit to one idea. None of it should be time-bound — no 10-year plans. Ask what this person would be doing at the pinnacle of his or her career — when they’re feeling challenged, engaged and not wanting anything else.”\n Create a career action plan\n  leading snowflakes http://leadingsnowflakes.com/ Show How, Don\u0026rsquo;t Tell What - A Management Style\nProgressing from tech to leadership https://lcamtuf.blogspot.hk/2018/02/on-leadership.html\nShare your Manager README https://matthewnewkirk.com/2017/09/20/share-your-manager-readme/\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"0f689655e2902582d054924032be3cbd","permalink":"https://wubigo.com/post/2016-12-30-softwareleadweekly/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-softwareleadweekly/","section":"post","summary":"good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3.","tags":null,"title":"software lead weekly","type":"post"},{"authors":null,"categories":null,"content":" twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/\n","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"59adc335c5a4340a46dc2c9bf3b60801","permalink":"https://wubigo.com/post/2016-11-01-graphprocessing/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/post/2016-11-01-graphprocessing/","section":"post","summary":"twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/","tags":null,"title":"Graph Processing","type":"post"},{"authors":null,"categories":null,"content":" log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”\n“the throughput of a log remains more or less constant, since every message is written to disk anyway [18]. This behavior is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: such systems are fast when queues are short and become much slower when they start writing to”\n","date":1458518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458518400,"objectID":"f2461a9a52c992d8848c5d9cb177bea0","permalink":"https://wubigo.com/post/2016-03-21-streamdataprocessing/","publishdate":"2016-03-21T00:00:00Z","relpermalink":"/post/2016-03-21-streamdataprocessing/","section":"post","summary":"log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”","tags":null,"title":"streaming note","type":"post"},{"authors":null,"categories":null,"content":" Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"5626be8c6b83784a26e59051195a8098","permalink":"https://wubigo.com/post/2016-03-01-backendsforfrontends/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/post/2016-03-01-backendsforfrontends/","section":"post","summary":"Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud","tags":null,"title":"Backends for Frontends","type":"post"},{"authors":null,"categories":[],"content":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.wikipedia.org/wiki/ANSI_escape_code#Colors black) code=30 ;; red) code=31 ;; green) code=32 ;; yellow) code=33 ;; blue) code=34 ;; magenta) code=35 ;; cyan) code=36 ;; white) code=37 ;; esac if [ \u0026quot;$code\u0026quot; ]; then codes=( \u0026quot;${codes[@]}\u0026quot; \u0026quot;$code\u0026quot; ) fi fi local IFS=';' echo -en '\\033['\u0026quot;${codes[*]}\u0026quot;'m' } wrap_color() { text=\u0026quot;$1\u0026quot; shift color \u0026quot;$@\u0026quot; echo -n \u0026quot;$text\u0026quot; color reset echo } wrap_good() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; white): $(wrap_color \u0026quot;$2\u0026quot; green)\u0026quot; } wrap_bad() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; bold): $(wrap_color \u0026quot;$2\u0026quot; bold red)\u0026quot; } wrap_warning() { wrap_color \u0026gt;\u0026amp;2 \u0026quot;$*\u0026quot; red } check_flag() { if is_set_in_kernel \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled' elif is_set_as_module \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled (as module)' else wrap_bad \u0026quot;CONFIG_$1\u0026quot; 'missing' EXITCODE=1 fi } check_flags() { for flag in \u0026quot;$@\u0026quot;; do echo -n \u0026quot;- \u0026quot;; check_flag \u0026quot;$flag\u0026quot; done } check_command() { if command -v \u0026quot;$1\u0026quot; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then wrap_good \u0026quot;$1 command\u0026quot; 'available' else wrap_bad \u0026quot;$1 command\u0026quot; 'missing' EXITCODE=1 fi } check_device() { if [ -c \u0026quot;$1\u0026quot; ]; then wrap_good \u0026quot;$1\u0026quot; 'present' else wrap_bad \u0026quot;$1\u0026quot; 'missing' EXITCODE=1 fi } check_distro_userns() { source /etc/os-release 2\u0026gt;/dev/null || /bin/true if [[ \u0026quot;${ID}\u0026quot; =~ ^(centos|rhel)$ \u0026amp;\u0026amp; \u0026quot;${VERSION_ID}\u0026quot; =~ ^7 ]]; then # this is a CentOS7 or RHEL7 system grep -q \u0026quot;user_namespace.enable=1\u0026quot; /proc/cmdline || { # no user namespace support enabled wrap_bad \u0026quot; (RHEL7/CentOS7\u0026quot; \u0026quot;User namespaces disabled; add 'user_namespace.enable=1' to boot command line)\u0026quot; EXITCODE=1 } fi } if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;warning: $CONFIG does not exist, searching other paths for kernel config ...\u0026quot; for tryConfig in \u0026quot;${possibleConfigs[@]}\u0026quot;; do if [ -e \u0026quot;$tryConfig\u0026quot; ]; then CONFIG=\u0026quot;$tryConfig\u0026quot; break fi done if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;error: cannot find kernel config\u0026quot; wrap_warning \u0026quot; try running this script again, specifying the kernel config:\u0026quot; wrap_warning \u0026quot; CONFIG=/path/to/kernel/.config $0 or $0 /path/to/kernel/.config\u0026quot; exit 1 fi fi wrap_color \u0026quot;info: reading kernel config from $CONFIG ...\u0026quot; white echo echo 'Generally Necessary:' echo -n '- ' cgroupSubsystemDir=\u0026quot;$(awk '/[, ](cpu|cpuacct|cpuset|devices|freezer|memory)[, ]/ \u0026amp;\u0026amp; $3 == \u0026quot;cgroup\u0026quot; { print $2 }' /proc/mounts | head -n1)\u0026quot; cgroupDir=\u0026quot;$(dirname \u0026quot;$cgroupSubsystemDir\u0026quot;)\u0026quot; if [ -d \u0026quot;$cgroupDir/cpu\u0026quot; -o -d \u0026quot;$cgroupDir/cpuacct\u0026quot; -o -d \u0026quot;$cgroupDir/cpuset\u0026quot; -o -d \u0026quot;$cgroupDir/devices\u0026quot; -o -d \u0026quot;$cgroupDir/freezer\u0026quot; -o -d \u0026quot;$cgroupDir/memory\u0026quot; ]; then echo \u0026quot;$(wrap_good 'cgroup hierarchy' 'properly mounted') [$cgroupDir]\u0026quot; else if [ \u0026quot;$cgroupSubsystemDir\u0026quot; ]; then echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'single mountpoint!') [$cgroupSubsystemDir]\u0026quot; else echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'nonexistent??')\u0026quot; fi EXITCODE=1 echo \u0026quot; $(wrap_color '(see https://github.com/tianon/cgroupfs-mount)' yellow)\u0026quot; fi if [ \u0026quot;$(cat /sys/module/apparmor/parameters/enabled 2\u0026gt;/dev/null)\u0026quot; = 'Y' ]; then echo -n '- ' if command -v apparmor_parser \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_good 'apparmor' 'enabled and tools installed')\u0026quot; else echo \u0026quot;$(wrap_bad 'apparmor' 'enabled, but apparmor_parser missing')\u0026quot; echo -n ' ' if command -v apt-get \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(use \u0026quot;apt-get install apparmor\u0026quot; to fix this)')\u0026quot; elif command -v yum \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(your best bet is \u0026quot;yum install apparmor-parser\u0026quot;)')\u0026quot; else echo \u0026quot;$(wrap_color '(look for an \u0026quot;apparmor\u0026quot; package for your distribution)')\u0026quot; fi EXITCODE=1 fi fi flags=( NAMESPACES {NET,PID,IPC,UTS}_NS CGROUPS CGROUP_CPUACCT CGROUP_DEVICE CGROUP_FREEZER CGROUP_SCHED CPUSETS MEMCG KEYS VETH BRIDGE BRIDGE_NETFILTER NF_NAT_IPV4 IP_NF_FILTER IP_NF_TARGET_MASQUERADE NETFILTER_XT_MATCH_{ADDRTYPE,CONNTRACK,IPVS} IP_NF_NAT NF_NAT NF_NAT_NEEDED # required for bind-mounting /dev/mqueue into containers POSIX_MQUEUE ) check_flags \u0026quot;${flags[@]}\u0026quot; if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -lt 8 ]; then check_flags DEVPTS_MULTIPLE_INSTANCES fi echo echo 'Optional Features:' { check_flags USER_NS check_distro_userns } { check_flags SECCOMP } { check_flags CGROUP_PIDS } { CODE=${EXITCODE} check_flags MEMCG_SWAP MEMCG_SWAP_ENABLED if [ -e /sys/fs/cgroup/memory/memory.memsw.limit_in_bytes ]; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently enabled)' bold black)\u0026quot; EXITCODE=${CODE} elif is_set MEMCG_SWAP \u0026amp;\u0026amp; ! is_set MEMCG_SWAP_ENABLED; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently not enabled, you can enable it by setting boot option \u0026quot;swapaccount=1\u0026quot;)' bold black)\u0026quot; fi } { if is_set LEGACY_VSYSCALL_NATIVE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NATIVE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(dangerous, provides an ASLR-bypassing target with usable ROP gadgets.)' bold black)\u0026quot; elif is_set LEGACY_VSYSCALL_EMULATE; then echo -n \u0026quot;- \u0026quot;; wrap_good \u0026quot;CONFIG_LEGACY_VSYSCALL_EMULATE\u0026quot; 'enabled' elif is_set LEGACY_VSYSCALL_NONE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NONE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(containers using eglibc \u0026lt;= 2.13 will not work. Switch to' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' \u0026quot;CONFIG_VSYSCALL_[NATIVE|EMULATE]\u0026quot; or use \u0026quot;vsyscall=[native|emulate]\u0026quot;' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' on kernel command line. Note that this will disable ASLR for the,' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' VDSO which may assist in exploiting security vulnerabilities.)' bold black)\u0026quot; # else Older kernels (prior to 3dc33bd30f3e, released in v4.40-rc1) do # not have these LEGACY_VSYSCALL options and are effectively # LEGACY_VSYSCALL_EMULATE. Even older kernels are presumably # effectively LEGACY_VSYSCALL_NATIVE. fi } if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -le 5 ]; then check_flags MEMCG_KMEM fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 18 ]; then check_flags RESOURCE_COUNTERS fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 13 ]; then netprio=NETPRIO_CGROUP else netprio=CGROUP_NET_PRIO fi flags=( BLK_CGROUP BLK_DEV_THROTTLING IOSCHED_CFQ CFQ_GROUP_IOSCHED CGROUP_PERF CGROUP_HUGETLB NET_CLS_CGROUP $netprio CFS_BANDWIDTH FAIR_GROUP_SCHED RT_GROUP_SCHED IP_NF_TARGET_REDIRECT IP_VS IP_VS_NFCT IP_VS_PROTO_TCP IP_VS_PROTO_UDP IP_VS_RR ) check_flags \u0026quot;${flags[@]}\u0026quot; if ! is_set EXT4_USE_FOR_EXT2; then check_flags EXT3_FS EXT3_FS_XATTR EXT3_FS_POSIX_ACL EXT3_FS_SECURITY if ! is_set EXT3_FS || ! is_set EXT3_FS_XATTR || ! is_set EXT3_FS_POSIX_ACL || ! is_set EXT3_FS_SECURITY; then echo \u0026quot; $(wrap_color '(enable these ext3 configs if you are using ext3 as backing filesystem)' bold black)\u0026quot; fi fi check_flags EXT4_FS EXT4_FS_POSIX_ACL EXT4_FS_SECURITY if ! is_set EXT4_FS || ! is_set EXT4_FS_POSIX_ACL || ! is_set EXT4_FS_SECURITY; then if is_set EXT4_USE_FOR_EXT2; then echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext3 or ext4 as backing filesystem' bold black)\u0026quot; else echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext4 as backing filesystem' bold black)\u0026quot; fi fi echo '- Network Drivers:' echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags VXLAN | sed 's/^/ /' echo ' Optional (for encrypted networks):' check_flags CRYPTO CRYPTO_AEAD CRYPTO_GCM CRYPTO_SEQIV CRYPTO_GHASH \\ XFRM XFRM_USER XFRM_ALGO INET_ESP INET_XFRM_MODE_TRANSPORT | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ipvlan' blue)'\u0026quot;:' check_flags IPVLAN | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'macvlan' blue)'\u0026quot;:' check_flags MACVLAN DUMMY | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ftp,tftp client in container' blue)'\u0026quot;:' check_flags NF_NAT_FTP NF_CONNTRACK_FTP NF_NAT_TFTP NF_CONNTRACK_TFTP | sed 's/^/ /' # only fail if no storage drivers available CODE=${EXITCODE} EXITCODE=0 STORAGE=1 echo '- Storage Drivers:' echo ' - \u0026quot;'$(wrap_color 'aufs' blue)'\u0026quot;:' check_flags AUFS_FS | sed 's/^/ /' if ! is_set AUFS_FS \u0026amp;\u0026amp; grep -q aufs /proc/filesystems; then echo \u0026quot; $(wrap_color '(note that some kernels include AUFS patches but not the AUFS_FS flag)' bold black)\u0026quot; fi [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'btrfs' blue)'\u0026quot;:' check_flags BTRFS_FS | sed 's/^/ /' check_flags BTRFS_FS_POSIX_ACL | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'devicemapper' blue)'\u0026quot;:' check_flags BLK_DEV_DM DM_THIN_PROVISIONING | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags OVERLAY_FS | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'zfs' blue)'\u0026quot;:' echo -n \u0026quot; - \u0026quot;; check_device /dev/zfs echo -n \u0026quot; - \u0026quot;; check_command zfs echo -n \u0026quot; - \u0026quot;; check_command zpool [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 EXITCODE=$CODE [ \u0026quot;$STORAGE\u0026quot; = 1 ] \u0026amp;\u0026amp; EXITCODE=1 echo check_limit_over() { if [ $(cat \u0026quot;$1\u0026quot;) -le \u0026quot;$2\u0026quot; ]; then wrap_bad \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; wrap_color \u0026quot; This should be set to at least $2, for example set: sysctl -w kernel/keys/root_maxkeys=1000000\u0026quot; bold black EXITCODE=1 else wrap_good \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; fi } echo 'Limits:' check_limit_over /proc/sys/kernel/keys/root_maxkeys 10000 echo exit $EXITCODE   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh\n","date":1456394317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456394317,"objectID":"dd1820587bd381fc88912b6de01b433f","permalink":"https://wubigo.com/post/docker-check-config/","publishdate":"2016-02-25T17:58:37+08:00","relpermalink":"/post/docker-check-config/","section":"post","summary":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.","tags":["DOCKER"],"title":"Docker Check Config","type":"post"},{"authors":null,"categories":["IT"],"content":" K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。\n","date":1456313943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456313943,"objectID":"896fb0dffd51ac94259bf63b206f5f45","permalink":"https://wubigo.com/post/k8s-network-basic/","publishdate":"2016-02-24T19:39:03+08:00","relpermalink":"/post/k8s-network-basic/","section":"post","summary":"K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。","tags":["K8S","NETWORK"],"title":"K8S网络基础","type":"post"},{"authors":null,"categories":null,"content":" cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM\n","date":1455235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455235200,"objectID":"02b5377f7051d7bda55ee55bcbc7e96e","permalink":"https://wubigo.com/post/2016-02-12-openstacknotes/","publishdate":"2016-02-12T00:00:00Z","relpermalink":"/post/2016-02-12-openstacknotes/","section":"post","summary":"cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM","tags":null,"title":"openstack notes","type":"post"},{"authors":null,"categories":["IT"],"content":" Main external dependencies  go docker cri cni  external-dependencies\nKUBEADM IS CURRENTLY IN BETA\nkubeadm maturity build K8S  docker v17.03  sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 K8S.gcr.io/kube-apiserver-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 K8S.gcr.io/kube-controller-manager-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 K8S.gcr.io/kube-scheduler-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 K8S.gcr.io/kube-proxy-amd64:v1.11.7 docker pull mirrorgooglecontainers/pause:3.1 docker tag mirrorgooglecontainers/pause:3.1 K8S.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.18 docker tag mirrorgooglecontainers/etcd-amd64:3.2.18 K8S.gcr.io/etcd-amd64:3.2.18 docker pull coredns/coredns:1.1.3 docker tag coredns/coredns:1.1.3 K8S.gcr.io/coredns:1.1.3   cri-tools v1.11.0\ngit clone https://github.com/kubernetes-sigs/cri-tools.git $GOPATH/src/github.com/kubernetes-sigs/ git checkout tags/v1.13.0 -b v1.13.0 make $GOPATH/bin/crictl -version cp $GOPATH/bin/cri* /usr/local/bin/  install-go-1.10\n checkout v1.11.7\ngit clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/K8S.io/ git fetch --all git checkout tags/v1.11.7 -b v1.11.7  or\ngit clone -b v1.11.7 https://github.com/kubernetes/kubernetes.git  LOCAL ETCD INTEGRATION\n+++ source /home/bigo/go/src/K8S.io/kubernetes/hack/lib/etcd.sh ++++ ETCD_VERSION=3.2.24 ++++ ETCD_HOST=127.0.0.1 ++++ ++++ KUBE_INTEGRATION_ETCD_URL=http://127.0.0.1:2379  build v1.11.7\n  cd kubernetes git remote add upstream https://github.com/kubernetes/kubernetes.git git remote set-url --push upstream no_push git fetch upstream git tag|grep v1.11.7 git checkout tags/v1.11.7 -b \u0026lt;branch_name\u0026gt; docker pull mirrorgooglecontainers/kube-cross:v1.10.7-1 docker tag mirrorgooglecontainers/kube-cross:v1.10.7-1 K8S.gcr.io/kube-cross:v1.10.7-1  export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 bash -x ./build/run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1 ./_output/dockerized/bin/linux/amd64/kubeadm version| grep v1.11.7  or\nmake quick-release ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubeadm version| grep v1.11.7  deploy K8S with kubeadm  install kubectl\nsudo cp ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubectl /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubeadm /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubelet /usr/bin/  kubeadm kubectl bash completion\nkubeadm completion bash \u0026gt; ~/.kube/kubeadm_completion.bash.inc echo \u0026quot;source '$HOME/.kube/kubeadm_completion.bash.inc'\\n\u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc  install kubelet service\nsudo cp ./build/debs/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ./build/debs/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet journalctl -xeu kubelet  disable swap\nsudo swapoff -a  build cni v0.6.0\ngit clone -b v0.6.0 https://github.com/containernetworking/cni.git cd cni ./build.sh mkdir -p /opt/cni/bin cp bin/* /opt/cni/bin/  Configure NetworkManager for calio\n  NetworkManager manipulates the routing table for interfaces in the default network namespace where Calico veth pairs are anchored for connections to containers. This can interfere with the Calico agent’s ability to route correctly. Create the following configuration file at /etc/NetworkManager/conf.d/calico.conf to prevent NetworkManager from interfering with the interfaces:\n[keyfile] unmanaged-devices=interface-name:cali*;interface-name:tunl*   bootstrap a secure Kubernetes cluster debug level with -v\nsudo kubeadm init --kubernetes-version=v1.11.7 --pod-network-cidr 10.2.0.0/16 -v 4  configure kubectl\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  calico setup calico etcd setup\nkubectl apply -f calico.yaml  kubectl completion code for bash\n# Installing bash completion on Linux kubectl completion bash \u0026gt; ~/.kube/kubectl.bash.inc printf \u0026quot; # Kubectl shell completion source '$HOME/.kube/completion.bash.inc' \u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc  Control plane node isolation\n  By default, the cluster will not schedule pods on the master for security reasons. If you want to be able to schedule pods on the master, e.g. for a single-machine Kubernetes cluster for development, run:\nkubectl taint nodes --all node-role.kubernetes.io/master-  view cluster config kubectl describe configmaps kubeadm-config -n kube-system journalctl -xe | grep -i etcd  or\ncd /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml  ETCD liveness probe kubectl describe pods etcd-bigo-vm3 -n kube-system Liveness: exec [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] sudo curl -v -l https://127.0.0.1:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt --key /etc/kubernetes/pki/etcd/healthcheck-client.key  kubectl exec -it etcd-bigo-vm1 -n kube-system -- sh ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-cl ient.key member list  join a node  install docker v17.03 IPVS proxier load IPVS mod  apt install ebtables socat  install kubelet service get token  kubeadm token list  get token-ca-cert-hash  openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //'    all in one shell\n deploy-work-node.sh\n token recreate By default, tokens expire after 24 hours. Joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the master node:\nkubeadm token create  Deploying the Dashboard\n  kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml   depoly nginx and verify  creates a Deployment object and an associated ReplicaSet object with 2 pods\nkubectl run nginx1-14 --replicas=2 --labels=\u0026quot;run=nginx1.14\u0026quot; --image=nginx:1.14-alpine --port=80 POD_IP=$(kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 1) curl $POD_IP kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 2 |xargs printf -- 'http://%s\\n'|xargs curl  Kubernetes requires a none-stop app/CMD Docker container stop automatically after running **K8S will restart it at default if a container stop **\ntest/curl/Dockerfile\n***let kubectl never restart container\nFROM alpine:3.8 RUN apk add --no-cache curl CMD [\u0026quot;sh\u0026quot;] docker build . docker tag curl-alpine:1.0 kubectl run curl -it --image=curl-alpine:1.0 --restart=Neve sh   tear down cluster  kubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt;  Then, on the node being removed, reset all kubeadm installed state:\nkubeadm reset  The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:\niptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X  If you want to reset the IPVS tables, you must run the following command:\nipvsadm -C  sudo kubeadm init phase etcd local \u0026ndash;config=configfile.yaml -v4\n\u0026ndash;kubernetes-version=v1.11.7\nkubeadm init \u0026ndash;config\netcd: local: serverCertSANs: - \u0026quot;0.0.0.0\u0026quot; peerCertSANs: - \u0026quot;0.0.0.0\u0026quot; extraArgs: listen-client-urls: --listen-client-urls=https://0.0.0.0:2379  ","date":1454470707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454470707,"objectID":"3fe5fdad3cba29be83b5c92c809ab189","permalink":"https://wubigo.com/post/2016-02-03-k8s-local-development-setup/","publishdate":"2016-02-03T11:38:27+08:00","relpermalink":"/post/2016-02-03-k8s-local-development-setup/","section":"post","summary":"Setup a local development environment from source code with kubeadm","tags":["PAAS","K8S"],"title":"K8S local development setup from source code","type":"post"},{"authors":null,"categories":null,"content":" Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.\nTokens are generated by the gateway, and sent to the underlying microservices: as they share a common secret key, microservices are able to validate the token, and authenticate users using that token.\nThose tokens are self-sufficient: they have both authentication and authorization information, so microservices do not need to query a database or an external system. This is important in order to ensure a scalable architecture\n","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"3c82f77b18f4006263734d8d02804c2e","permalink":"https://wubigo.com/post/2016-02-01-microservice-notes/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/post/2016-02-01-microservice-notes/","section":"post","summary":"Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.","tags":["MICROSERVICE"],"title":"microservice notes","type":"post"},{"authors":null,"categories":[],"content":" ubuntu docker Post-installation steps  check to docker log for warning  journalctl -xu docker journalctl -xu docker.service   check-config   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n ","date":1453713065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453713065,"objectID":"cb36c07a249c5cfc1646698d05813209","permalink":"https://wubigo.com/post/docker-notes/","publishdate":"2016-01-25T17:11:05+08:00","relpermalink":"/post/docker-notes/","section":"post","summary":" ubuntu docker Post-installation steps  check to docker log for warning  journalctl -xu docker journalctl -xu docker.service   check-config   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n ","tags":["DOCKER"],"title":"Docker Notes","type":"post"},{"authors":null,"categories":null,"content":" git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.js to render Markdown as html \t\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Bigo release\u0026lt;/title\u0026gt; \u0026lt;xmp theme=\u0026quot;united\u0026quot; style=\u0026quot;display:none;\u0026quot;\u0026gt; \u0026lt;/xmp\u0026gt; \u0026lt;script src=\u0026quot;http://strapdownjs.com/v/0.2/strapdown.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt;  ","date":1443830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443830400,"objectID":"d65862ec22ce872b3ad144e0f9cedc9c","permalink":"https://wubigo.com/post/2015-10-03-nginx-git-log-as-relasenote/","publishdate":"2015-10-03T00:00:00Z","relpermalink":"/post/2015-10-03-nginx-git-log-as-relasenote/","section":"post","summary":"git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.","tags":null,"title":"Git log as release note","type":"post"},{"authors":null,"categories":null,"content":" Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com.\t285\tIN\tA\t185.199.110.153 wubigo.com.\t285\tIN\tA\t185.199.108.153 wubigo.com.\t285\tIN\tA\t185.199.111.153 wubigo.com.\t285\tIN\tA\t185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"9033a7185a446a46f551aacb1f1b98d7","permalink":"https://wubigo.com/post/2015-08-03-github-notes/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-github-notes/","section":"post","summary":"Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com.\t285\tIN\tA\t185.199.110.153 wubigo.com.\t285\tIN\tA\t185.199.108.153 wubigo.com.\t285\tIN\tA\t185.199.111.153 wubigo.com.\t285\tIN\tA\t185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider","tags":null,"title":"github notes","type":"post"},{"authors":null,"categories":null,"content":" GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters. With POST, form data appears within the message body of the HTTP request\nAuthors of services which use the HTTP protocol SHOULD NOT use GET based forms for the submission of sensitive data, because this will cause this data to be encoded in the Request-URI. Many existing servers, proxies, and user agents will log the request URI in some place where it might be visible to third parties. Servers can use POST-based form submission instead\nFinally, an important consideration when using GET for AJAX requests is that some browsers - IE in particular - will cache the results of a GET request. So if you, for example, poll using the same GET request you will always get back the same results, even if the data you are querying is being updated server-side. One way to alleviate this problem is to make the URL unique for each request by appending a timestamp.\nRestrictions on form data length Yes, since form data is in the URL and URL length is restricted. A safe URL length limit is often 2048 characters but varies by browser and web server.\nFrom the Dropbox developer blog:\n a browser doesn’t know exactly what a particular HTML form does, but if the form is submitted via HTTP GET, the browser knows it’s safe to automatically retry the submission if there’s a network error. For forms that use HTTP POST, it may not be safe to retry so the browser asks the user for confirmation first.  A \u0026ldquo;GET\u0026rdquo; request is often cacheable, whereas a \u0026ldquo;POST\u0026rdquo; request can hardly be. For query systems this may have a considerable efficiency impact, especially if the query strings are simple, since caches might serve the most frequent queries.\nhttp://www.diffen.com/difference/GET-vs-POST-HTTP-Requests\nrotate catalina out without restarting tomcat The catalina.out file is created by a shell redirection, ex \u0026ldquo;\u0026gt;\u0026gt; catalina.out 2\u0026gt;\u0026amp;1\u0026rdquo;. This catches anything written to System.out and System.err and places it into the catalina.out file. Given this, a good way to rotate catalina.out is to alter the script to pipe the output to a log rotation script rather than directly to a file. This will allow you to rotate the logs without restarting Tomcat and without copying the entire contents of the log to another file. It\u0026rsquo;s a pretty simple change to catalina.sh and it is described at this link.\n[http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2](http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2) [https://dzone.com/articles/how-rotate-tomcat-catalinaout](https://dzone.com/articles/how-rotate-tomcat-catalinaout) cat /dev/null \u0026gt; logs/catalina.out  tomcat connector config  \u0026lt;Connector address=\u0026quot;127.0.0.1\u0026quot; port=\u0026quot;8080\u0026quot; protocol=\u0026quot;org.apache.coyote.http11.Http11Nio2Protocol\u0026quot; connectionTimeout=\u0026quot;30000\u0026quot; redirectPort=\u0026quot;8443\u0026quot; executor=\u0026quot;tomcatThreadPool\u0026quot; minProcessors=\u0026quot;100\u0026quot; maxProcessors=\u0026quot;300\u0026quot; enableLookups=\u0026quot;false\u0026quot; acceptCount=\u0026quot;500\u0026quot; maxPostSize=\u0026quot;-1\u0026quot; disableUploadTimeout=\u0026quot;false\u0026quot; connectionUploadTimeout=\u0026quot;600000\u0026quot; compression=\u0026quot;on\u0026quot; compressionMinSize=\u0026quot;2048\u0026quot; noCompressionUserAgents=\u0026quot;gozilla, traviata\u0026quot; acceptorThreadCount=\u0026quot;2\u0026quot; compressableMimeType=\u0026quot;text/html,text/xml,text/plain,text/css,text/javascript,application/javascript\u0026quot; URIEncoding=\u0026quot;utf-8\u0026quot;/\u0026gt;  A CharacterEncodingFilter sets the body encoding, but not the URI encoding. Need to set URIEncoding=\u0026ldquo;UTF-8\u0026rdquo; as an attribute in all the connectors in Tomcat server.xml\nThe request.setCharacterEncoding(\u0026ldquo;UTF-8\u0026rdquo;); only sets the encoding of the request body (which is been used by POST requests), not the encoding of the request URI (which is been used by GET requests).\nenabling gzip with nginx and verifying that it\u0026rsquo;s working curl -H \u0026quot;Accept-Encoding: gzip,deflate\u0026quot; -I http://web/resource  https://www.digitalocean.com/community/tutorials/how-to-add-the-gzip-module-to-nginx-on-ubuntu-16-04\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"5096f8e4f05b72c0d0fc1652271e313c","permalink":"https://wubigo.com/post/2015-08-03-http/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-http/","section":"post","summary":"GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters.","tags":null,"title":"http  TIL","type":"post"},{"authors":null,"categories":null,"content":" NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-auth-pam --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-dav-ext-module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-echo --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-upstream-fair --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/ngx_http_substitutions_filter_module  nginxproxy.md\nStep 3 \u0026ndash; Configure /\nLet say we want to configure nginx to route requests for /, /blog, and /mail, respectively onto localhost:8080, localhost:8181, and localhost:8282.\n +--- host --------\u0026gt; node.js on localhost:8080 | users --\u0026gt; nginx --|--- host/blog ---\u0026gt; node.js on localhost:8181 | +--- host/mail ---\u0026gt; node.js on localhost:8282  To route /, you need to edit your nginx config file.\nIn the config file, find the server section:\nserver { listen 80; ... location / { ... } ... }  This section is simply telling nginx how it should serve HTTP requests.\nNow, change the location section to this snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } ... }  proxy_pass simply tells nginx to forward requests to / to the server listening on http://127.0.0.1:8080.\nStep 4 \u0026ndash; Reload nginx\u0026rsquo;s Configuration\nTo reload nginx\u0026rsquo;s configuration run: nginx -s reload on your machine.\nReferesh your browser. Do you see the output from your node.js application? If yes, you are all set. If no, there is a problem with your config.\nStep 5 \u0026ndash; Add /blog and /mail\nTo redirect /mail and /blog, you simply need to add new entries the location section in the config file:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { proxy_pass http://127.0.0.1:8181; } location /mail { proxy_pass http://127.0.0.1:8282; } ... }  Step 6 \u0026ndash; Reload Your nginx Configuration\nRun nginx -s reload on your machine.\nLog onto localhost:$PORT/blog in your browser. Do you see the output from your second node.js application?\nThen log onto localhost:$PORT/mail. Do you see the output from your third node.js application?\nIf yes \u0026amp; yes, you are all set. If no, there is a problem with your config.\nStep 7 \u0026ndash; Rewriting Requests\nNow as you might have noticed in Step 6, nginx sends the same HTTP request to your node.js web apps which results into a 404 error. Why? Because, your node.js web application serves requests from / not from /blog and /mail. But, nginx is sending requests to /blog and /mail.\nTo fix this issue, we need rewrite the URL so that it matches the URL you can serve on your node.js applications.\nTo correctly rewrite URLs change your config file to match the following snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { rewrite ^/blog(.*) /$1 break; proxy_pass http://127.0.0.1:8181; } location /mail { rewrite ^/mail(.*) /$1 break; proxy_pass http://127.0.0.1:8282; } ... }  This rewrite commands are simple regular expressions that transform strings like /blogWHAT_EVER and /mailWHAT_EVER to /WHAT_EVER in the HTTP requests.\nStep 8 \u0026ndash; Reload and Test.\nAll set?\nExercise 1\nConfigure your nginx to redirect URLs from /google to http://www.google.com\nStep 9 (optional) \u0026ndash; Redirecting Based on Host Name\nLet say you want to host example1.com, example2.com, and example3.com on your machine, respectively to localhost:8080, localhost:8181, and localhost:8282.\nNote: Since you don\u0026rsquo;t have access to a DNS server, you should add domain name entries to your /etc/hosts (you can\u0026rsquo;t do this on CDF machines):\n\u0026hellip; 127.0.0.1 example1.com example2.com example3.com \u0026hellip; To proxy eaxmple1.com we can\u0026rsquo;t use the location part of the default server. Instead we need to add another server section with a server_name set to our virtual host (e.g., example1.com, \u0026hellip;), and then a simple location section that tells nginx how to proxy the requests:\nserver { listen 80; server_name example1.com; location / { proxy_pass http://127.0.0.1:8080; } } server { listen 80; server_name example2.com; location / { proxy_pass http://127.0.0.1:8181; } } server { listen 80; server_name example3.com; location / { proxy_pass http://127.0.0.1:8282; } }  ","date":1435881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435881600,"objectID":"0039d8c563b2aa08c54a20528746eafd","permalink":"https://wubigo.com/post/2015-06-03-nginx-proxy-rewrite/","publishdate":"2015-07-03T00:00:00Z","relpermalink":"/post/2015-06-03-nginx-proxy-rewrite/","section":"post","summary":"NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.","tags":null,"title":"nginx-proxy-rewrite","type":"post"},{"authors":null,"categories":null,"content":" update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.\nCreate a new branch and switch to it (so all of your latest commits are stored here)\ngit checkout -b your_new_branch\nSwitch back to your previous working branch (assume it\u0026rsquo;s master)\ngit checkout master\nRemove the latest x commits, keep master clean\ngit reset \u0026ndash;hard HEAD~x # in your case, x = 3\nFrom this moment on, all the latest x commits are only in the new branch, not in your previous working branch (master) any more.\nIgnoring an already checked-in directory git rm -r --cached \u0026lt;your directory\u0026gt;  The -r option causes the removal of all files under your directory. The \u0026ndash;cached option causes the files to only be removed from git\u0026rsquo;s index, not your working copy. By default git rm  would delete \npush a new local branch to a remote Git repository and track it git checkout -b \u0026lt;branch\u0026gt; | git branch \u0026lt;branch\u0026gt; git push -u origin \u0026lt;branch\u0026gt;  Adding Only Untracked Files git add -i. Type a (for \u0026ldquo;add untracked\u0026rdquo;), then * (for \u0026ldquo;all\u0026rdquo;), then q (to quit)\nDiscard all Changes not staged for commit git checkout \u0026ndash; .\nCreate a new empty branch and import from svn git checkout --orphan \u0026lt;branchname\u0026gt; git rm --cached -r . svn checkout git add . git commit -m \u0026quot;backup from svn tag\u0026quot; git push --set-upstream origin \u0026lt;branchname\u0026gt;  save username and password in git git config credential.helper store then git pull  ~/.git-credentials\nI delete a Git branch both locally and remotely Executive Summary $ git push -d origin  $ git branch -d  Delete Local Branch To delete the local branch use:\n$ git branch -d branch_name or use: $ git branch -D branch_name As of Git v1.7.0, you can delete a remote branch using $ git push origin \u0026ndash;delete \ngit without proxy method 1\n$ env|grep proxy http_proxy=http://192.168.0.119:3128/ socks_proxy=socks://192.168.0.119:3128/ https_proxy=https://192.168.0.119:3128/ $ unset http_proxy $ git pull  method 2(proxy for certain git urls/domains)\n@web:~/workspace/git/pub$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [http] proxy = \u0026quot;\u0026quot; [https] proxy = \u0026quot;\u0026quot;  https://www.andrewpage.me/tracking-down-bugs-with-git-bisect/ https://medium.com/@fredrikmorken/why-you-should-stop-using-git-rebase-5552bee4fed1\ncheck the list of tags on the upstream repo without cloning or fetchingn git ls-remote https://github.com/go-delve/delve git ls-remote --tags https://github.com/go-delve/delve  the difference between origin and upstream on GitHub  upstream generally refers to the original repo that you have forked (see also \u0026ldquo;Definition of “downstream” and “upstream”\u0026rdquo; for more on upstream term) origin is your fork: your own repo on GitHub, clone of the original repo of GitHub  From the GitHub page:\n When a repo is cloned, it has a default remote called origin that points to your fork on GitHub, not the original repo it was forked from. To keep track of the original repo, you need to add another remote named upstream  git remote add upstream git://github.com/user/repo.git  You will use upstream to fetch from the original repo (in order to keep your local copy in sync with the project you want to contribute to).\ngit fetch upstream\n(git fetch alone would fetch from origin by default, which is not what is needed here)\n","date":1433289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433289600,"objectID":"fa9ccaaa4a347547a004bff4879385df","permalink":"https://wubigo.com/post/2015-06-03-git-notes/","publishdate":"2015-06-03T00:00:00Z","relpermalink":"/post/2015-06-03-git-notes/","section":"post","summary":"update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.","tags":null,"title":"git note","type":"post"},{"authors":null,"categories":null,"content":" understand of the HBase data model http://jimbojw.com/#understanding hbase\n","date":1428019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428019200,"objectID":"e8284e4c6ce95a8614ae8baf1b4a00f2","permalink":"https://wubigo.com/post/2015-04-03-hbase-notes/","publishdate":"2015-04-03T00:00:00Z","relpermalink":"/post/2015-04-03-hbase-notes/","section":"post","summary":"understand of the HBase data model http://jimbojw.com/#understanding hbase","tags":null,"title":"HBase notes","type":"post"},{"authors":null,"categories":null,"content":" 10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/\n","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"9fd7942affbbeffab10c766676cf5991","permalink":"https://wubigo.com/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","section":"post","summary":"10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/","tags":null,"title":"10 Highly Impactful Books You Should Definitely Check Out","type":"post"},{"authors":null,"categories":null,"content":" TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL. For making sure, your MySQL is UTF-8, run the following queries in your MySQL prompt: First query:\nmysql\u0026gt; show variables like \u0026lsquo;char%\u0026rsquo;;\ntomcat deploy for dev conf/Catalina/localhost/ROOT.xml \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;Context docBase=\u0026quot;/opt/etender\u0026quot; path=\u0026quot;\u0026quot; /\u0026gt; $TOMCAT_HOME/conf/server.xml \u0026lt;!-- \u0026lt;Context path=\u0026quot;/etender\u0026quot; docBase=\u0026quot;c:/WebRoot\u0026quot;\u0026gt; \u0026lt;/Context\u0026gt; --\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt;  show the last queries executed on MySQL temporarily If you prefer to output to a file:\nSET GLOBAL log_output = \u0026quot;FILE\u0026quot;; which is set by default. #set absolute path will report error,mysql.log=/var/lib/mysql/mysql.log SET GLOBAL general_log_file = \u0026quot;mysql.log\u0026quot;; SET GLOBAL general_log = 'ON'; tail -f /var/lib/mysql/mysql.log  Optimize Your Tomcat Installation on Ubuntu 14.04 hange JVM Heap Setting (-Xms -Xmx) of Tomcat – Configure setenv.sh file\ndefault no setenv.sh file under /bin directory. Have to create one with below parameters.\nXms=Xmx=1/2RAM( avoid having the costly memory allocation process running because the size of the allocated memory will be constant all the time) MaxPermSize=1/2mx\n$cat setenv.sh export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xms512m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xmx8192m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -XX:MaxPermSize=256m\u0026rdquo;\ncatalina.out to verify the setting in effect catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize=2048 catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs=org.apache.catalina.webresources org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xms4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xmx4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -XX:MaxPermSize=2000m\noptimize mysql showing current configuration variables mysql\u0026gt;SHOW VARIABLES LIKE \u0026lsquo;%max%\u0026rsquo;;\ninnodb_file_per_table = ON innodb_stats_on_metadata = OFF innodb_buffer_pool_instances = 8 innodb_buffer_pool_size = 1G query_cache_type = 0 query_cache_size = 0 innodb_log_file_size = 5242880 innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0 innodb_flush_method = O_DIRECT\ntomcat upstat on ubuntu14.04 /etc/init/tomcat.conf description \u0026ldquo;Tomcat Server\u0026rdquo;\nstart on runlevel [2345] stop on runlevel [!2345] respawn respawn limit 10 5\nsetuid tomcat setgid tomcat\nenv JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre env CATALINA_HOME=/opt/tomcat\n# Modify these options as needed env JAVA_OPTS=\u0026ldquo;-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom\u0026rdquo; env CATALINA_OPTS=\u0026ldquo;-Xms512M -Xmx1024M -server -XX:+UseParallelGC\u0026rdquo;\nexec $CATALINA_HOME/bin/catalina.sh run\n# cleanup temp directory after stop post-stop script rm -rf $CATALINA_HOME/temp/* end script\nsudo initctl reload-configuration Tomcat is ready to be run. Start it with this command: sudo initctl start tomcat\nsudo sh -c \u0026lsquo;echo manual \u0026gt;\u0026gt; /etc/init/tomcat.override\u0026rsquo;\ndeploy web app as the root context in tomcat in the $CATALINA_BASE/conf/[enginename]/[hostname]/ROOT.XML \u0026lt;?xml version=\u0026ldquo;1.0\u0026rdquo; encoding=\u0026ldquo;UTF-8\u0026rdquo;?\u0026gt; authbind tomcat sudo apt install authbind sudo touch /etc/authbind/byport/{443,80} sudo chmod 500 /etc/authbind/byport/{443,80} sudo chown tomcat:tomcat /etc/authbind/byport/{443,80}\nConfigure Tomcat sudo sed -i \u0026rsquo;s/8080/80/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml sudo sed -i \u0026rsquo;s/8443/443/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml #TOMCAT_HOME/bin/setenv.sh export CATALINA_OPTS=\u0026ldquo;-Djava.net.preferIPv4Stack=true\u0026rdquo; #startup.sh: exec authbind \u0026ndash;deep \u0026ldquo;$PRGDIR\u0026rdquo;/\u0026ldquo;$EXECUTABLE\u0026rdquo; start \u0026ldquo;$@\u0026rdquo;\nupload big files with Nginx (Reverse proxy+SSL negotiation) and Tomcat solution 1: config nginx $TOMCAT_HOME/bin/server.xml\ndisableUploadTimeout=false In nginx.conf add: http { # at the END of this segment! client_max_body_size 1000m; }\nsolution 2 : config tomcat maxSwallowSize The maximum number of request body bytes (excluding transfer encoding overhead) that will be swallowed by Tomcat for an aborted upload. An aborted upload is when Tomcat knows that the request body is going to be ignored but the client still sends it. If Tomcat does not swallow the body the client is unlikely to see the response. If not specified the default of 2097152 (2 megabytes) will be used. A value of less than zero indicates that no limit should be enforced.\nMySql - changing innodb_file_per_table for a live db solution 1: mysql\u0026gt;set global innodb_file_per_table = 1 (set value to on doesn\u0026rsquo;t effect for mysql 5.5 )\nCross Origin Resource Sharing (CORS) with nginx  location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. # try_files $uri $uri/ =404; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';  remove Nginx Server Signature(reset server header in response) /etc/nginx/nginx.conf server_tokens off;  # 跨域的常见解决方法 目前来讲没有不依靠服务器端来跨域请求资源的技术 1.jsonp 需要目标服务器配合一个callback函数。 2.window.name+iframe 需要目标服务器响应window.name。 3.window.location.hash+iframe 同样需要目标服务器作处理。 4.html5的 postMessage+ifrme 这个也是需要目标服务器或者说是目标页面写一个postMessage，主要侧重于前端通讯。 5.CORS 需要服务器设置header ：Access-Control-Allow-Origin。 6.nginx反向代理 这个方法一般很少有人提及，但是他可以不用目标服务器配合，不过需要你搭建一个中转nginx服务器，用于转发请求。\nlocation / { #alias D:\\\\develop\\\\project1dir\\\\appDist\\\\; #此文件夹可以是项目打包后的上线代码文件，也可以是第二个项目源代码文件 # Frontend Server proxy_pass http://localhost:8002/; #前端服务器地址，比如gulp+browser-sync开启的服务器，能看到代码实时更新效果 } location /api/ { rewrite ^/api/(.*)$ /$1 break; #所有对后端的请求加一个api前缀方便区分，真正访问的时候移除这个前缀 # API Server proxy_pass http://serverB.com; #将真正的请求代理到serverB,即真实的服务器地址，ajax的url为/api/user/1的请求将会访问http://www.serverB.com/user/1 }  update time \tsudo ntpdate ntp.ubuntu.com   https://help.ubuntu.com/lts/serverguide/NTP.html  ","date":1399075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399075200,"objectID":"c28933e174ef766366bbfa6f72dec90a","permalink":"https://wubigo.com/post/2014-05-03-doing_on_ubuntu/","publishdate":"2014-05-03T00:00:00Z","relpermalink":"/post/2014-05-03-doing_on_ubuntu/","section":"post","summary":"TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL.","tags":null,"title":"doing_on_ubuntu","type":"post"},{"authors":null,"categories":["IT"],"content":" The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie. don\u0026rsquo;t have any *.mydomain.com cookies), you can dramatically reduce the size (ie. number of packets sent) of the HTTP request, which would save on bandwidth and speed up requests significantly if you use cookies heavily on the main site. The page can benefit from a more simultaneous requests being made by the browser\nMost browsers will make simultaneous requests for page assets, like images, fonts, CSS, etc. The catch is that most browsers will only allow a limited number of open requests to a particular domain (somewhere around 5 I think). By spreading your assets across multiple sub-domains, you \u0026ldquo;trick\u0026rdquo; the browser, and allow more parallel requests, since the limit applies to each sub-domain.\nAdding an Alternate Domain Name Using the CloudFront Configure the DNS service for the domain to route traffic for the domain, such as example.com, to the CloudFront domain name for your distribution, such as d111111abcdef8.cloudfront.net. The method that you use depends on whether you\u0026rsquo;re using Amazon Route 53 as the DNS service provider for the domain:\nAmazon Route 53 Create an alias resource record set. With an alias resource record set, you don\u0026rsquo;t pay for Amazon Route 53 queries. In addition, you can create an alias resource record set for the root domain name (example.com), which DNS doesn\u0026rsquo;t allow for CNAMEs. For more information, see Routing Queries to an Amazon CloudFront Distribution in the Amazon Route 53 Developer Guide.\nAnother DNS service provider Use the method provided by your DNS service provider to add a CNAME resource record set to the hosted zone for your domain. This new CNAME resource record set will redirect DNS queries from your domain (for example, www.example.com) to the CloudFront domain name for your distribution (for example, d111111abcdef8.cloudfront.net). For more information, see the documentation provided by your DNS service provider.\nImportant If you already have an existing CNAME record for your domain name, update that resource record set or replace it with a new one that points to the CloudFront domain name for your distribution. In addition, confirm that your CNAME resource record set points to your distribution\u0026rsquo;s domain name and not to one of your origin servers.\nsteps to add cname record step 1: edit cloudfront distribution then add a Alternate Domain Name for cloudfront\nstep 2: add a CNAME resource record set by the DNS service provider\n","date":1396525143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396525143,"objectID":"687bdf0d2dfbe65200fb105b462c6165","permalink":"https://wubigo.com/post/2014-04-03-cdn/","publishdate":"2014-04-03T19:39:03+08:00","relpermalink":"/post/2014-04-03-cdn/","section":"post","summary":"The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie.","tags":["IAAS","NETWORK"],"title":"cdn note","type":"post"},{"authors":null,"categories":null,"content":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"c8218b6eddffe204e9229f561b8b4353","permalink":"https://wubigo.com/post/2014-03-03-boot/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-boot/","section":"post","summary":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","tags":null,"title":"SPRING-boot note","type":"post"},{"authors":null,"categories":null,"content":" Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io\n","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"31a1ec70c56747fca48bf551e86a68e8","permalink":"https://wubigo.com/post/2014-03-03-web-api-reference/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-web-api-reference/","section":"post","summary":"Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io","tags":null,"title":"web API reference","type":"post"},{"authors":null,"categories":null,"content":" steps to make ec2 access from outside  create vpc create internet gateway add route to the gateway into the routetable  RequestTimeTooSkewed error with S3 upload https://github.com/aws/aws-sdk-js/issues/399 https://aws.amazon.com/blogs/developer/clock-skew-correction/\ncloudfront set Origin Custom Headers https://w3guy.com/solution-font-origin-http-cdn-domain-blocked-loading-cors-policy/\n","date":1391385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391385600,"objectID":"2b9ce00fb2f3a520ff4f334760374e55","permalink":"https://wubigo.com/post/2014-02-03-aws-notes/","publishdate":"2014-02-03T00:00:00Z","relpermalink":"/post/2014-02-03-aws-notes/","section":"post","summary":"steps to make ec2 access from outside  create vpc create internet gateway add route to the gateway into the routetable  RequestTimeTooSkewed error with S3 upload https://github.com/aws/aws-sdk-js/issues/399 https://aws.amazon.com/blogs/developer/clock-skew-correction/\ncloudfront set Origin Custom Headers https://w3guy.com/solution-font-origin-http-cdn-domain-blocked-loading-cors-policy/","tags":null,"title":"aws FAQ","type":"post"},{"authors":null,"categories":null,"content":" 1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\nBecause we’re trying to actually do something on the server, this action is asynchronous. All asynchronous methods in the ArangoDB driver return promises but you can also pass a node-style callback instead:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;, function (err) { if (!err) console.log(\u0026lsquo;Database created\u0026rsquo;); else console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err); });\nKeep in mind that the new database you’ve created is only available once the callback is called or the promise is resolved. Throughout this tutorial we’ll use the promise API because they’re available in recent versions of Node.js as well as most modern browsers.\n3: db = require(\u0026lsquo;arangojs\u0026rsquo;).Database; db = new Database(\u0026lsquo;http://127.0.0.1:8529'); db.useBasicAuth(\u0026lsquo;root\u0026rsquo;, \u0026lsquo;123123\u0026rsquo;); db.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\ncheck arangodb status /etc/init.d/arangodb3 status\nenable remote connection /etc/arangodb3/arangod.conf #endpoint = tcp://[::]:8529\n","date":1388707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388707200,"objectID":"eda9722e1bb5103bd8e54f8995cb5bb1","permalink":"https://wubigo.com/post/2014-01-03-arangodb/","publishdate":"2014-01-03T00:00:00Z","relpermalink":"/post/2014-01-03-arangodb/","section":"post","summary":"1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );","tags":null,"title":"arrangodb note","type":"post"},{"authors":null,"categories":null,"content":" 自动化发布 很多网站选择周四作为发布日\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ec2354ee671052fa4120517131a73ab8","permalink":"https://wubigo.com/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","section":"post","summary":"自动化发布 很多网站选择周四作为发布日","tags":null,"title":"大型网站技术架构","type":"post"},{"authors":null,"categories":null,"content":" API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs\n","date":1357171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357171200,"objectID":"b51bc744bcd6ae992033af60c36f65b3","permalink":"https://wubigo.com/post/2013-01-03-api_design/","publishdate":"2013-01-03T00:00:00Z","relpermalink":"/post/2013-01-03-api_design/","section":"post","summary":"API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs","tags":null,"title":"API_design Note","type":"post"},{"authors":null,"categories":null,"content":" mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others. Having to wait until all of the preceding job’s tasks have completed slows down the execution of the workflow as a whole. * Mappers are often redundant: they just read back the same file that was just written by a reducer, and prepare it for the next stage of partitioning and sorting. In many cases, the mapper code could be part of the previous reducer: if the reducer output was partitioned and sorted in the same way as mapper output, then reducers could be chained together directly, without interleaving with mapper stages. * “Storing intermediate state in a distributed filesystem means those files are replicated across several nodes, which is often overkill for such temporary data”\nDevelop Apache Spark Apps with IntelliJ IDEA on Windows OS https://www.linkedin.com/pulse/develop-apache-spark-apps-intellij-idea-windows-os-samuel-yee\nSpark notes http://stackoverflow.com/questions/40796818/how-to-append-a-resource-jar-for-spark-submit Set SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the console, e.g. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja\u0026hellip; Refer to Print Launch Command of Spark Scripts (or org.apache.spark.launcher.Main Standalone Application where this environment variable is actually used). Tip Avoid using scala.App trait for a Spark app docker run -v pwd:/data -e SPARK_PRINT_LAUNCH_COMMAND=1 -it heuermh/adam adam-shell Avoid using scala.App trait for a Spark application’s main class in Scala as\nreported in SPARK-4170 Closure problems when running Scala app that \u0026ldquo;extends\nApp\u0026rdquo;.\nRefer to Executing Main — runMain internal method in this document.\nMake sure to use the same version of Scala as the one used to build your distribution of Spark. Pre-built distributions of Spark 1.x use Scala 2.10, while pre-built distributions of Spark 2.0.x use Scala 2.11. 10 down vote\nSteps to install Spark(1.6.2-bin-hadoop2.6)prebuild in local mode on windows:\nInstall Java 7 or later. To test java installation is complete, open command prompt type javaand hit enter. If you receive a message \u0026lsquo;Java\u0026rsquo; is not recognized as an internal or external command. You need to configure your environment variables, JAVA_HOME and PATHto point to the path of jdk.\nDownload and install Scala.\nSet SCALA_HOME in Control Panel\\System and Security\\System goto \u0026ldquo;Adv System settings\u0026rdquo; and add %SCALA_HOME%\\bin in PATH variable in environment variables.\nInstall Python 2.6 or later from Python Download link.\nDownload SBT. Install it and set SBT_HOME as an environment variable with value as \u0026lt;\u0026gt;. Download winutils.exe from HortonWorks repo or git repo. Since we don\u0026rsquo;t have a local Hadoop installation on Windows we have to download winutils.exe and place it in a bindirectory under a created Hadoop home directory. Set HADOOP_HOME = \u0026lt;\u0026gt; in environment variable.and add it to path env We will be using a pre-built Spark package, so choose a Spark pre-built package for Hadoop Spark download. Download and extract it.\nSet SPARK_HOME and add %SPARK_HOME%\\bin in PATH variable in environment variables.\nRun command: spark-shell\nOpen http://localhost:4040/ in a browser to see the SparkContext web UI.\n$ cat rdd1.txt chr1 10016 chr1 10017 chr1 10018 chr1 20026 scala\u0026gt; val lines = sc.textFile(\u0026ldquo;/data/rdd1.txt\u0026rdquo;)\nscala\u0026gt; case class Chrom(name: String, value: Long)\ndefined class Chrom\nscala\u0026gt; val chroms = lines.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom(r(0), r(1).toLong))\nchroms: org.apache.spark.rdd.RDD[Chrom] = MapPartitionsRDD[5] at map at :28\nscala\u0026gt; val df = chroms.toDF\n16/10/28 16:17:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n16/10/28 16:17:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\ndf: org.apache.spark.sql.DataFrame = [name: string, value: bigint]\nscala\u0026gt; df.show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|10016|\n|chr1|10017|\n|chr1|10018|\n|chr1|20026|\n|chr1|20036|\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; df.filter(\u0026lsquo;value \u0026gt; 30000).show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; case class Chrom2(name: String, value: Long, value: Long)\nscala\u0026gt; val chroms2 = rdd2.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom2(r(0), r(1).toLong, r(2).toLong))\nchroms2: org.apache.spark.rdd.RDD[Chrom2] = MapPartitionsRDD[35] at map at :28\nscala\u0026gt; val df2=chroms2.toDF\ndf2: org.apache.spark.sql.DataFrame = [name: string, min: bigint \u0026hellip; 1 more field]\nscala\u0026gt; df.join(df2, Seq(\u0026ldquo;name\u0026rdquo;)).where($\u0026ldquo;value\u0026rdquo;.between($\u0026ldquo;min\u0026rdquo;, $\u0026ldquo;max\u0026rdquo;)).groupBy($\u0026ldquo;name\u0026rdquo;).count().show()\n$./bin/spark-shell \u0026ndash;packages com.databricks:spark-csv_2.11:1.2\n.0\nYour csv file does not have the same number of fields in each row - this cannot be parsed as is into a DataFrame\nAs of Spark 2.0.0, DataFrame - the flagship data abstraction of previous versions of Spark SQL - is currently a mere type alias for Dataset[Row] :\nA Dataset is local if it was created from local collections using SparkSession.emptyDataset or SparkSession.createDataset methods and their derivatives like toDF. If so, the queries on the Dataset can be optimized and run locally, i.e. without using Spark executors.\n","date":1349308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349308800,"objectID":"1ca6d42c66a5123f3068b4aef27a8f87","permalink":"https://wubigo.com/post/2012-10-04-big-data-notes/","publishdate":"2012-10-04T00:00:00Z","relpermalink":"/post/2012-10-04-big-data-notes/","section":"post","summary":"mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others.","tags":null,"title":"Develop Apache Spark Apps with IntelliJ IDEA on Windows OS","type":"post"},{"authors":null,"categories":null,"content":" update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.7.11/bin/python python2.7.11\n$. ./python2.7.11/bin/activate $sudo pip install PyYAML Jinja2 httplib2 six Setting up Ansible to run out of checkout(~/.bashrc) export PATH=/home/ubuntu/ansible/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games export export PYTHONPATH=/home/ubuntu/ansible/lib: sudo PYTHONPATH=/home/ubuntu/ansible/lib ansible\nor /etc/sudoers\nDefaults env_reset\nDefaults env_keep += \u0026ldquo;PYTHONPATH\u0026rdquo;\nansible-galaxy(ansible \u0026gt; 2 otherwise of proxy bug) use role installed from galaxy $ansible-galaxy install angstwad.docker_ubuntu\n#create a play book to use the role\n hosts: all  sudo: yes\nroles:\n- angstwad.docker_ubuntu  #test to run the book\n$ansible-playbook t1.yml\nBuilding Role Scaffolding $ansible-galaxy init rolename\nBest Practices: Create a dedicated ansible node on the local net in the cloud to play book copy directory to remote node using scp dir remote:~/dir or using synchronize module instead of copy\nfacts about any system $ansible -m setup | less\n#facter comes as part of extra modules. to use the facter module, the \u0026ldquo;facter\u0026rdquo; and #\u0026ldquo;ruby-json\u0026rdquo; packages preinstalled on the target host.\n$ansible  -m facter | less\nvariables The following are the places from where Ansible accepts variables:\nThe default directory inside a role Inventory variables The host_vars and group_vars parameters defined in separate directories The host/group vars parameter defined in an inventory file Variables in playbooks and role parameters The vars directory inside a role and variables defined inside a play Extra variables provided with the -e option at runtime\nPatterns\nPatterns in Ansible are how we decide which hosts to manage.The following patterns address one or more groups. Groups separated by a colon indicate an “OR” configuration. This means the host may be in either one group or the other: hosts: name1:name2:group1:group2\nDisable SSH Host Key Checking For All Hosts set these options permanently in ~/.ssh/config (for the current user) or in /etc/ssh/ssh_config (for all users), either for all hosts or for a given set of IP addresses\nHost * StrictHostKeyChecking no UserKnownHostsFile=/dev/null\nAnsible looks for an ansible.cfg file in the following places, in this order:\nFile specified by the ANSIBLE_CONFIG environment variable\n./ansible.cfg (ansible.cfg in the current directory)\n~/.ansible.cfg (.ansible.cfg in your home directory)\n/etc/ansible/ansible.cfg\nAnsible will then move to the next task in the list, and go through these same four steps. It’s important to note that:\nAnsible runs each task in parallel across all hosts.\nAnsible waits until all hosts have completed a task before moving to the next task.\nAnsible runs the tasks in the order that you specify them.\nAnsible supports the ssh-agent program, so you don’t need to explicitly specify SSH key files in your inventory files. See “SSH Agent” for more details if you haven’t used ssh-agent before.\nYAML to get started with your first playbook:\nThe first line of a playbook should begin with \u0026ldquo;\u0026mdash; \u0026rdquo; (three hyphens) which indicates the beginning of the YAML document. Lists in YAML are represented with a hyphen followed by a white space. A playbook contains a list of plays; they are represented with \u0026ldquo;- \u0026ldquo;. Each play is an associative array, a dictionary, or a map in terms of key-value pairs. Indentations are important. All members of a list should be at the same indentation level. Each play can contain key-value pairs separated by \u0026ldquo;:\u0026rdquo; to denote hosts, variables, roles, tasks, and so on.\nrole dependencies pecify role dependency inside the meta subdirectory\nSafely limiting Ansible playbooks to a single machine There\u0026rsquo;s also a cute little trick that lets you specify a single host on the command line (or multiple hosts, I guess), without an intermediary inventory:\nansible-playbook -i \u0026ldquo;imac1-local,\u0026rdquo; user.yml Note the comma (,) at the end; this signals that it\u0026rsquo;s a list, not a file\n","date":1344038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1344038400,"objectID":"8d897195aec3cc72f94440472e18d4ed","permalink":"https://wubigo.com/post/2012-08-04-ansible/","publishdate":"2012-08-04T00:00:00Z","relpermalink":"/post/2012-08-04-ansible/","section":"post","summary":"update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.","tags":null,"title":"ansible note","type":"post"},{"authors":null,"categories":null,"content":" B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树\n","date":1341360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341360000,"objectID":"8dd0cb0fdbf378cdcc8265da15f2651c","permalink":"https://wubigo.com/post/2012-07-04-rdbms-nosql/","publishdate":"2012-07-04T00:00:00Z","relpermalink":"/post/2012-07-04-rdbms-nosql/","section":"post","summary":"B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树","tags":["RDBMS","NOSQL"],"title":"RDBMS vs NoSQL","type":"post"},{"authors":null,"categories":null,"content":" showing current configuration mysql\u0026gt;SHOW VARIABLES; mysqldump -u root -h 192.168.76.62 -pgld --all-databases \u0026gt; dump.sql mysqldump -u root -h db -pgld --all-databases \u0026gt; dump.sql  then import data in mysql shell\nmysql\u0026gt;source dump.sql\nupdate the database if directly upgrade from 5.0 to 5.5\nmysql_upgrade -u root -p\nservice mysql restart\nA typical mysqldump command to move data from an external database to an Amazon RDS DB instance looks similar to the following:\nmysqldump -u  \u0026ndash;databases  \u0026ndash;single-transaction \u0026ndash;compress \u0026ndash;order-by-primary -p | mysql -u  \u0026ndash;port= \u0026ndash;host= -p\n\u0026ndash;single-transaction – Use to ensure that all of the data loaded from the local database is consistent with a single point in time. If there are other processes changing the data while mysqldump is reading it, using this option helps maintain data integrity. \u0026ndash;compress – Use to reduce network bandwidth consumption by compressing the data from the local database before sending it to Amazon RDS. \u0026ndash;order-by-primary – Use to reduce load time by sorting each table\u0026rsquo;s data by its primary key.\nYou must create any stored procedures, triggers, functions, or events manually in your Amazon RDS database. If you have any of these objects in the database that you are copying, then exclude them when you run mysqldump by including the following arguments with your mysqldump command: \u0026ndash;routines=0 \u0026ndash;triggers=0 \u0026ndash;events=0.\nhow to remove mysql completely\nsudo apt-get remove \u0026ndash;purge mysql-server mysql-client mysql-common sudo apt-get autoremove sudo apt-get autoclean then try to install it again.\nsudo apt-get install mysql-server if you installing with dpkg command and if it show any dependency on other package then run command :\nsudo apt-get install -f\nWhat is disabled by default is remote root access. If you want to enable that, run this SQL command locally:\nGRANT ALL PRIVILEGES ON . TO \u0026lsquo;root\u0026rsquo;@\u0026lsquo;%\u0026rsquo; IDENTIFIED BY \u0026lsquo;\u0026rsquo; WITH GRANT OPTION; FLUSH PRIVILEGES; And then find the following line and comment it out in your my.cnf file, which usually lives on /etc/mysql/my.cnf on Unix/OSX systems. If it\u0026rsquo;s a Windows system, you can find it in the MySQL installation directory, usually something like C:\\Program Files\\MySQL\\MySQL Server 5.5\\ and the filename will be my.ini.\nChange line\nbind-address = 127.0.0.1 to\n#bind-address = 127.0.0.1 And restart the MySQL server for the changes to take effect.\nsudo apt-get install mysql-server\nupdate t_supplier_subproject set attachInfo=\u0026ldquo;;commit;\nupdate t_supplier set email=\u0026lsquo;gin_369@163.Cnn\u0026rsquo; where supplierID=7;\nselect * from t_supplier where userid=\u0026lsquo;6156354693465407499\u0026rsquo; and email=\u0026lsquo;gin_369@163.COM\u0026rsquo;;\nALTER TABLE t_quoted_adopt4tbq MODIFY adoptRemark VARCHAR(255);\n查询分包商数量 SELECT count(*) FROM etender.t_supplier;\nALTER TABLE etender.t_supplier CHANGE COLUMN email email VARCHAR(40) NULL DEFAULT \u0026ldquo; COMMENT \u0026lsquo;供应商邮箱\u0026rsquo;;\n新增列 询价类型 alter table t_project_query add column inquiryType varchar(255) DEFAULT \u0026lsquo;2\u0026rsquo; COMMENT \u0026lsquo;询价类型\u0026rsquo;;\nalter table t_quoted_billitem4tbq add column itemType varchar(255) DEFAULT \u0026lsquo;1\u0026rsquo; COMMENT \u0026lsquo;清单类型\u0026rsquo;;\nSELECT COUNT(*),t2.email AS \u0026lsquo;总包邮箱\u0026rsquo;, t1.userID FROM etender.t_user t2 LEFT JOIN etender.t_supplier t1 ON t1.userID = t2.userID WHERE t1.logicDelete !=1 GROUP BY t2.userID ;\nMaking a Copy of a Database $mysqldump -u root -pg1d etender \u0026gt; dump.sql $mysqladmin -u root -pg1d create hongq $mysql -u root -pg1d hongq \u0026lt; dump.sql  export to csv SELECT b.email,a.name supplierName,a.email supplierEmail,a.telephone,a.trade,a.level,a.address,a.contacts FROM t_supplier a LEFT JOIN t_user b ON a.userID = b.userid and a.logicDelete !=1 ORDER BY a.userID limit 1000,1000 INTO OUTFILE '/var/lib/mysql-files/subcon1000.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\u0026quot;' LINES TERMINATED BY '\\n'  Creating SSH Tunnel From Linux for mysql $ ssh -L 3306:rdb:3306 ubuntu@ec2\nuse ip instead of hostname to avoid channel X on ubuntu 16 \u0026ldquo;channel X: open failed: administratively prohibited\u0026rdquo;\n","date":1338768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338768000,"objectID":"1bc16a0874c5c39c07a78ba84a4b7ccd","permalink":"https://wubigo.com/post/2012-06-04-mysql/","publishdate":"2012-06-04T00:00:00Z","relpermalink":"/post/2012-06-04-mysql/","section":"post","summary":"showing current configuration mysql\u0026gt;SHOW VARIABLES; mysqldump -u root -h 192.168.76.62 -pgld --all-databases \u0026gt; dump.sql mysqldump -u root -h db -pgld --all-databases \u0026gt; dump.sql  then import data in mysql shell\nmysql\u0026gt;source dump.sql\nupdate the database if directly upgrade from 5.0 to 5.5\nmysql_upgrade -u root -p\nservice mysql restart\nA typical mysqldump command to move data from an external database to an Amazon RDS DB instance looks similar to the following:","tags":null,"title":"dbms note","type":"post"},{"authors":null,"categories":["IT"],"content":" network manager Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\n /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/var/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --conf-file=/dev/null --proxy-dnssec --enable-dbus=org.freedesktop.NetworkManager.dnsmasq --conf-dir=/etc/NetworkManager/dnsmasq.d  update git to 2.20 sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git=2.20.1-0ppa1~ubuntu16.04.1  static ip sudo systemctl stop network-manager sudo systemctl disable network-manager.service echo \u0026quot;manual\u0026quot; | sudo tee /etc/init/network-manager.override  ubuntu18.04\nsudo systemctl stop NetworkManager-wait-online.service sudo systemctl disable NetworkManager-wait-online.service sudo systemctl stop NetworkManager-dispatcher.service sudo systemctl disable NetworkManager-dispatcher.service sudo systemctl stop network-manager.service sudo systemctl disable network-manager.service ystemctl unmask networking systemctl enable networking systemctl restart networking  cat /etc/network/interfaces auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1  ubuntu18.04 only\necho \u0026quot;DNS=192.168.1.1\u0026gt;\u0026gt;/etc/systemd/resolved.conf systemctl restart systemd-resolved  ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  instll docker v17.03 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; sudo apt-get update apt-cache madison docker-ce|grep 17.03 sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial sudo usermod -aG docker $USER  docker to reclaim disk space  remove untagged images  docker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi   Clean up dead and exited containers(use the -v flag to remove the volumes along the container)  docker ps --filter status=dead --filter status=exited -aq \\ | xargs docker rm -v   docker volume cleanup  docker volume ls -qf dangling=true | xargs -r docker volume rm  IPVS for i in ip_vs_sh ip_vs ip_vs_rr ip_vs_wrr; do sudo modprobe $i; done  change the runlevel on systemd for VM sudo systemctl enable multi-user.target sudo systemctl set-default multi-user.target  List files in package $dpkg -L docker-ce /usr/bin/docker-containerd /usr/bin/docker-proxy /usr/bin/docker /usr/bin/docker-runc /usr/bin/dockerd /usr/bin/docker-containerd-ctr /usr/bin/docker-containerd-shim /usr/bin/docker-init /etc/init.d/docker /etc/default/docker /etc/init/docker.conf /lib/systemd/system/docker.service /lib/systemd/system/docker.socket  Find the latest file by modified date find /path -printf '%T+ %p\\n' | sort -r | head  ghost systemd service /etc/systemd/system/ghost.service\nRunning sudo command: ln -sf /var/www/ghost/system/files/ghost_localhost.service /lib/systemd/system/ghost_localhost.service Running sudo command: systemctl daemon-reload  ls /lib/systemd/system/ghost* sudo systemctl stop ghost_localhost  Admin URL As per the SSL section above, admin.url can be used to specify a different protocol for your admin panel. It can also be used to specify a different hostname (domain name). It cannot be used to affect the path at which the admin panel is served (this is always /ghost/).\n\u0026quot;admin\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://example.com\u0026quot; }  ubuntu@ip-192-168-114-240:/lib/systemd/system$ sudo systemctl disable ghost_54-169-190-39.service Removed symlink /etc/systemd/system/multi-user.target.wants/ghost_54-169-190-39.service. Removed symlink /etc/systemd/system/ghost_54-169-190-39.service.  Rotate Tomcat catalina.out https://dzone.com/articles/how-rotate-tomcat-catalinaout\n","date":1338723543,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338723543,"objectID":"c4634f210cd6781ba6828c973a0eff99","permalink":"https://wubigo.com/post/2012-06-03-linux/","publishdate":"2012-06-03T19:39:03+08:00","relpermalink":"/post/2012-06-03-linux/","section":"post","summary":"network manager Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\n /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/var/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --conf-file=/dev/null --proxy-dnssec --enable-dbus=org.freedesktop.NetworkManager.dnsmasq --conf-dir=/etc/NetworkManager/dnsmasq.d  update git to 2.20 sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git=2.","tags":["IAAS","LINUX"],"title":"linux note","type":"post"},{"authors":null,"categories":null,"content":" Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this. For EC2 users, make sure you use HVM based modern EC2 instances, like m3.medium. Otherwise fork() is too slow. Transparent huge pages must be disabled from your kernel. Use echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled to disable them, and restart your Redis process. If you are using a virtual machine, it is possible that you have an intrinsic latency that has nothing to do with Redis. Check the minimum latency you can expect from your runtime environment using ./redis-cli \u0026ndash;intrinsic-latency 100. Note: you need to run this command in the server not in the client. Enable and use the Latency monitor feature of Redis in order to get a human readable description of the latency events and causes in your Redis instance.  stop-writes-on-bgsave-error(save RDB snapshots) MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.\n$ redis-cli \u0026gt; config set stop-writes-on-bgsave-error no  Delete all the keys of the currently selected DB 127.0.0.1:6379\u0026gt;FLUSHALL or flushall  select database the number of databases is defined in the configuration file with the databases directive (the default value is 16). To switch between the databases, call SELECT. 127.0.0.1:6379\u0026gt;select \nDataType  DataType type = redisTemplate.type(key); if(DataType.NONE == type){ return null; }else if(DataType.STRING == type){ return super.redisTemplate.opsForValue().get(key); }else if(DataType.LIST == type){ return super.redisTemplate.opsForList().range(key, 0, -1); }else if(DataType.HASH == type){ return super.redisTemplate.opsForHash().entries(key); }else return null;  allowing remote connection to redis echo \u0026ldquo;bind 0.0.0.0\u0026rdquo; \u0026gt;\u0026gt; redis.conf\n1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.  ","date":1336003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336003200,"objectID":"22964e47b193e9ca7e1d767cea8d9b9b","permalink":"https://wubigo.com/post/2012-05-03-redis-notes/","publishdate":"2012-05-03T00:00:00Z","relpermalink":"/post/2012-05-03-redis-notes/","section":"post","summary":"Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this. For EC2 users, make sure you use HVM based modern EC2 instances, like m3.medium. Otherwise fork() is too slow. Transparent huge pages must be disabled from your kernel. Use echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled to disable them, and restart your Redis process. If you are using a virtual machine, it is possible that you have an intrinsic latency that has nothing to do with Redis.","tags":null,"title":"redis note","type":"post"},{"authors":null,"categories":null,"content":" NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.\n generate keys and certificates for wildcard * address as well. They will work on any machine. It will simplify certificates routine but increase security risks.\n  check X509v3 Subject Alternative Name(HOST) issued in server.pem\nopenssl x509 -in server.pem -text |grep DNS  Generate self-signed certificates  Download cfssl\nmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson}  Initialize a certificate authority\nmkdir ~/cfssl cd ~/cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json  Certificate types which are used inside Container Linux\n client certificate is used to authenticate client by server. . server certificate is used by server and verified by client for server identity. peer certificate is used by cluster as members communicate with each other in both ways.  Configure CA options ca-csr.json(Certificate Signing Request (CSR))\n{ \u0026quot;CN\u0026quot;: \u0026quot;wubigo CA\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;ecdsa\u0026quot;, \u0026quot;size\u0026quot;: 256 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BJ\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Bei Jing\u0026quot; } ] }   ca-config.json( set expiry to 8760h (1 year))\n{ \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;168h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;server\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot; ] }, \u0026quot;client\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;client auth\u0026quot; ] }, \u0026quot;peer\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } }   generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - ls ca-key.pem ca.csr ca.pem  Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within the CA\n Generate server certificate\ncfssl print-defaults csr \u0026gt; server.json  Most important values for server certificate are Common Name (CN) and hosts. substitute them accordingly:\n  ... \u0026quot;CN\u0026quot;: \u0026quot;server\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.6\u0026quot;, \u0026quot;wubigo.com\u0026quot;, \u0026quot;localhost\u0026quot;, \u0026quot;127.0.0.1\u0026quot; ], ...   generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  result following files:\nserver-key.pem server.csr server.pem  Generate peer certificate\n  cfssl print-defaults csr \u0026gt; member1.json\nSubstitute CN and hosts values, for example:\n... \u0026quot;CN\u0026quot;: \u0026quot;member1\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.7\u0026quot;, \u0026quot;member1.wubigo.com\u0026quot;, \u0026quot;member1.local\u0026quot;, \u0026quot;member1\u0026quot; ], ...   generate member1 certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1  member1-key.pem member1.csr member1.pem  Generate client certificate\ncfssl print-defaults csr \u0026gt; client.json  For client certificate ignore hosts values and set only Common Name (CN) to client value:\n... \u0026quot;CN\u0026quot;: \u0026quot;client\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;\u0026quot; ]\u0026quot;, ...  Generate client certificate:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client   get following files:\nclient-key.pem client.csr client.pem  ","date":1328054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328054400,"objectID":"c75fad4581620ab0c699c16ce7e01a99","permalink":"https://wubigo.com/post/2012-02-01-tls-notes/","publishdate":"2012-02-01T00:00:00Z","relpermalink":"/post/2012-02-01-tls-notes/","section":"post","summary":"NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.","tags":null,"title":"TLS notes","type":"post"},{"authors":null,"categories":["IT"],"content":" dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance. But the main reason that this confusion happens is because, it does not matter whatever tool you use to test the file system\u0026rsquo;s performance, what matter\u0026rsquo;s is the exact requirement.File system\u0026rsquo;s performance depends upon certain factors as follows.\nThe maximum rotational speed of your hard disk\nThe Allocated block size of a file system\nSeek Time\nThe performance rate of the file system\u0026rsquo;s metadata\nThe type of read/Write\nSeriously speaking its wonderful to realize that various different technologies made by different people and even different companies are working together in coordination inside a single box, and we call that box a computer. And its even more wonderful to realize that hard disk\u0026rsquo;s store\u0026rsquo;s almost all the information available in the world in digital format. Its a very complex thing to understand how really hard disks stores our data safely. Explaining different aspects of how a hard disk, and a file system on top of it, work together is beyond the scope of this article(But i will surely give it a try with couple of my posts about themwink)\nSo Lets begin our tutorial on file system benchmark test.\nIts advised that during this file system performance test, you must not run any other disk I/O intensive tasks. Otherwise your results about performance will be heavily deviated. Its better to stop all other process during this test.\nThe Simplest Performance Test Using dd command\nThe simplest read write performance test in Linux can be done with the help of dd command. This command is used to write or read from any block device in Linux. And you can do a lot of stuff with this command. The main plus point with this command, is that its readily available in almost all distributions out of the box. And is pretty easy to use.\nWith this dd command we will only be testing sequential read and sequential write.I will test the speed of my partition /dev/sda1 which is mounted on \u0026ldquo;/\u0026rdquo; (the only partition i have on my system)so can write the data to any where in my filesystem to test.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.0897865 seconds, 1.2 GB/s  In the above command you will be amazed to see that you have got 1.1GB/s. But dont be happy thats falsecheeky. Becasue the speed that dd reported to us is the speed with which data was cached to RAM memory, not to the disk. So we need to ask dd command to report the speed only after the data is synced with the disk.For that we need to run the below command.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 conv=fdatasync 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 2.05887 seconds, 50.9 MB/s  As you can clearly see that with the attribute fdatasync the dd command will show the status rate only after the data is completely written to the disk. So now we have the actual sequencial write speed. Lets go to an amount of data size thats larger than the RAM. Lets take 200MB of data in 64kb block size.\n[root@slashroot2 ~]# dd if=/dev/zero of=speedtest bs=64k count=3200 conv=fdatasync 3200+0 records in 3200+0 records out 209715200 bytes (210 MB) copied, 3.51895 seconds, 59.6 MB/s  as you can clearly see that the speed came to 59 MB/s. You need to note that ext3 bydefault if you do not specify the block size, gets formatted with a block size thats determined by the programes like mke2fs . You can verify yours with the following commands.\ntune2fs -l /dev/sda1 dumpe2fs /dev/sda1  For testing the sequential read speed with dd command, you need to run the below command as below.\n[root@myvm1 sarath]# dd if=speedtest of=/dev/null bs=64k count=24000 5200+0 records in 5200+0 records out 340787200 bytes (341 MB) copied, 3.42937 seconds, 99.4 MB/s  Performance Test using HDPARM\nNow lets use some other tool other than dd command for our tests. We will start with hdparm command to test the speed. Hdparm tool is also available out of the box in most of the linux distribution.\n[root@myvm1 ~]# hdparm -tT /dev/sda1 /dev/sda1: Timing cached reads: 5808 MB in 2.00 seconds = 2908.32 MB/sec Timing buffered disk reads: 10 MB in 3.12 seconds = 3.21 MB/sec  There are multiple things to understand here in the above hdparm results. the -t Option will show you the speed of reading from the cache buffer(Thats why its much much higher).\nThe -T option will show you the speed of reading without precached buffer(which from the above output is low 3.21 MB/sec as shown above. )\nthe hdparm output shows you both the cached reads and disk reads separately. As mentioned before hard disk seek time also matters a lot for your speed you can check your hard disk seek time with the following linux command. seek time is the time required by the hard disk to reach the sector where the data is stored.Now lets use this seeker tool to find out the seek time by the simple seek command.\n[root@slashroot2 ~]# seeker /dev/sda1 Seeker v3.0, 2009-06-17, http://www.linuxinsight.com/how_fast_is_your_disk.html Benchmarking /dev/sda1 [81915372 blocks, 41940670464 bytes, 39 GB, 39997 MB, 41 GiB, 41940 MiB] [512 logical sector size, 512 physical sector size] [1 threads] Wait 30 seconds.............................. Results: 87 seeks/second, 11.424 ms random access time (26606211 \u0026lt; offsets \u0026lt; 41937280284)  its clearly mentioned that my disk did a 86 seeks for sectors containing data per second. Thats ok for a desktop Linux machine but for servers its not at all ok.\nRead Write Benchmark Test using IOZONE:\nNow there is one tool out there in linux that will do all these test in one shot. Thats none other than \u0026ldquo;IOZONE\u0026rdquo;. We will do some benchmark test against my /dev/sda1 with the help of iozone.Computers or servers are always purchased keeping some purpose in mind. Some servers needs to be highend performance wise, some needs to be fast in sequencial reads,and some others are ordered keeping random reads in mind. IOZONE will be very much helpful in carrying out large number of permance benchmark test against the drives. The output produced by iozone is too much brief.\nThe default command line option -a is used for full automatic mode, in which iozone will test block sizes ranging from 4k to 16M and file sizes ranging from 64k to 512M. Lets do a test using this -a option and see what happens.\n[root@myvm1 ~]# iozone -a /dev/sda1 Auto Mode Command line used: iozone -a /dev/sda1 Output is in Kbytes/sec Time Resolution = 0.000001 seconds. Processor cache size set to 1024 Kbytes. Processor cache line size set to 32 bytes. File stride size set to 17 * record size. \u0026lt;div id=\u0026quot;xdvp\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;http://www.ecocertico.com/no-credit-check-direct-lenders\u0026amp;#10;\u0026quot;\u0026gt;creditors you never heard\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; random random bkwd record stride KB reclen write rewrite read reread read write read rewrite read fwrite frewrite fread freread 64 4 172945 581241 1186518 1563640 877647 374157 484928 240642 985893 633901 652867 1017433 1450619 64 8 25549 345725 516034 2199541 1229452 338782 415666 470666 1393409 799055 753110 1335973 2071017 64 16 68231 810152 1887586 2559717 1562320 791144 1309119 222313 1421031 790115 538032 694760 2462048 64 32 338417 799198 1884189 2898148 1733988 864568 1421505 771741 1734912 1085107 1332240 1644921 2788472 64 64 31775 811096 1999576 3202752 1832347 385702 1421148 771134 1733146 864224 942626 2006627 3057595 128 4 269540 699126 1318194 1525916 390257 407760 790259 154585 649980 680625 684461 1254971 1487281 128 8 284495 837250 1941107 2289303 1420662 779975 825344 558859 1505947 815392 618235 969958 2130559 128 16 277078 482933 1112790 2559604 1505182 630556 1560617 624143 1880886 954878 962868 1682473 2464581 128 32 254925 646594 1999671 2845290 2100561 554291 1581773 723415 2095628 1057335 1049712 2061550 2850336 128 64 182344 871319 2412939 609440 2249929 941090 1827150 1007712 2249754 1113206 1578345 2132336 3052578 128 128 301873 595485 2788953 2555042 2131042 963078 762218 494164 1937294 564075 1016490 2067590 2559306  Note: All the output you see above are in KB/Sec\nThe first column shows the file size used and second column shows the length of the record used.\nLets understand the output in some of the columns\nThe third Column-Write:This column shows the speed Whenever a new file is made in any file system under Linux. There is more overhead involved in the metadata storing. For example the inode for the file, and its entry in the journal etc. So creating a new file in a file system is always comparatively slower than overwriting an already created file.\nFourth column-Re-writing:This shows the speed reported in overwriting the file which is already created\nFifth column-Read:This reports the speed of reading an already existing file.\nseq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties submit jobs/job.{}.cfg seq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties clean jobs/job.{}.cfg  http://www.slashroot.in/linux-file-system-read-write-performance-test\n","date":1307101143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1307101143,"objectID":"5bdf905d3f48e9e1e5da49eaf0148f30","permalink":"https://wubigo.com/post/2011-06-03-io-performance/","publishdate":"2011-06-03T19:39:03+08:00","relpermalink":"/post/2011-06-03-io-performance/","section":"post","summary":"dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance.","tags":["IAAS","LINUX"],"title":"Linux File System Read Write Performance Test","type":"post"},{"authors":null,"categories":null,"content":" http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them. Near the bottom of your config file you will see a section for HPN related options; I used the following options from other guides I found:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nthe following are HPN related configuration options tcp receive buffer polling. disable in non autotuning kernels TcpRcvBufPoll yes\nallow the use of the none cipher #NoneEnabled no\ndisable hpn performance boosts. #HPNDisabled no\nbuffer size for hpn to non-hpn connections HPNBufferSize 8388608\nLinux supports both /proc and sysctl (using alternate forms of the variable names - e.g. net.core.rmem_max) for inspecting and adjusting network tuning parameters. The following is a useful shortcut for inspecting all tcp parameters:\nsysctl -a | fgrep tcp\nFor additional information on kernel variables, look at the documentation included with your kernel source, typically in some location such as /usr/src/linux-/Documentation/networking/ip-sysctl.txt. There is a very good (but slightly out of date) tutorial on network sysctl\u0026rsquo;s at http://ipsysctl-tutorial.frozentux.net/ipsysctl-tutorial.html.\nIf you would like to have these changes to be preserved across reboots, you can add the tuning commands to your the file /etc/rc.d/rc.local .\necho 1 \u0026gt; /proc/sys/net/ipv4/tcp_moderate_rcvbuf echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/wmem_max echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/rmem_max echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_rmem echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_wmem\noptimization start increase TCP max buffer size setable using setsockopt() net.ipv4.tcp_rmem = 4096 87380 8388608\nnet.ipv4.tcp_wmem = 4096 87380 8388608\nincrease Linux auto tuning TCP buffer limits min, default, and max number of bytes to use set max to at least 4MB, or higher if you use very high BDP paths net.core.rmem_max = 8388608\nnet.core.wmem_max = 8388608\nnet.core.netdev_max_backlog = 5000\nnet.ipv4.tcp_window_scaling = 1\noptimization end [1] http://www.psc.edu/index.php/hpn-ssh\n[2]http://stackoverflow.com/questions/8849240/why-when-i-transfer-a-file-through-sftp-it-takes-longer-than-ftp\n[3]http://www.cyberciti.biz/tips/sshd-server-optimization.html\n","date":1301788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301788800,"objectID":"9d351edd435efc376437776ae226dfa4","permalink":"https://wubigo.com/post/2011-04-03-high-performance-ssh/","publishdate":"2011-04-03T00:00:00Z","relpermalink":"/post/2011-04-03-high-performance-ssh/","section":"post","summary":"http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them.","tags":null,"title":"High Performance SSH/SCP","type":"post"},{"authors":null,"categories":null,"content":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB. However, for high end HPC clusters, the NFS server can quickly become a major bottleneck as it does not scale well when used in large cluster environments. The NFS server also becomes a single point of failure, for which the consequences of it crashing can become severe.\nSAN (Storage Area Networks) SAN file systems are capable of very high performance, but are extremely expensive to scale up since they are implemented using Fibre Channel and therefore, each node that connects to the SAN must have a Fibre Channel card to connect to the Fibre channel switch.\nLustre (a Global Parallel File System) The main advantage of Lustre, a global parallel file system, over NFS and SAN file systems is that it provides; wide scalability, both in performance and storage capacity; a global name space, and the ability to distribute very large files across many nodes. Because large files are shared across many nodes in the typical cluster environment, a parallel file system, such as Lustre, is ideal for high end HPC cluster I/O systems.\nA typical storage system consists of a variety of components, including disks, storage controllers, IO cards, storage servers, storage area network switches, and related management software. Fitting all these components together and tuning them to achieve optimal performance presents significant challenges.\nIf you are managing your own infrastructure in your own private data center, then you are bound to go through a selection of different storage offerings. Selecting a storage solution pretty much depends on your requirement. Before finalizing a particular storage option for your use case, a little bit of understanding about the technology is always helpful.\nI was actually going to write an article about object storage (which is the current hottest storage option in the cloud). But before going and discussing that part of the storage arena, I thought its better to discuss the two main storage methods which co-exists together from a very long time, used by companies internally for their needs.\nThe decision of your storage type will depend on many factors like the below ones.\nType of data that you want to store\nUsage pattern\nScaling concerns\nFinally your budget\nWhen you begin your career as a system administrator, you will often hear your colleagues talking about different storage methods like SAN, NAS, DAS etc. And without a little bit of digging, you are bound to get confused with different terms in storage. The confusion arises often because of the similarities between the different approaches in storage. The only hard and fast rule to stay up to date in technical terms, is to keep on reading stuffs (especially concepts behind a certain technology.)\nToday we will be discussing two different methods that defines the structure of storage in your environment. Your choice of the two in your architecture should only depend on your use case, and type of data that you store.\nBy the end of this tutorial, I hope you will have a clear picture about the main two types of storage methods, and what to select for your need.\nSAN (Storage Area Network) and NAS(Network Attached Storage)\nThe main things that differentiate each of these technologies are mentioned below.\nHow a storage is connected to a system. In short how the connection is made between the accessing system and the storage component (directly attached or network attached)\nType of cabling used to connect. In short this is the type of cabling done to connect a system to the storage component (eg. Ethernet \u0026amp; Fiber channel)\nHow are input and output requests done. In short this is the protocol used to conduct input and output requests (eg. SCSI, NFS, CIFS etc)\nRelated: How to monitor IO on linux\nLet\u0026rsquo;s discuss SAN first and then NAS, and at the end, let\u0026rsquo;s compare each of these technologies to clear the differences between them.\nSAN(Storage Area Network)\nToday\u0026rsquo;s applications are very much resource intensive, due to the kind of requests that needs to be processed simultaneously per second. Take example of an e-commerce website, where thousands of people are making orders per second, and all needs to be stored properly in the database for later retrieval. The storage technology used to store such high traffic data bases must be fast in request serving and response(in short it should be fast in Input and Output).\nRelated: Web server Performance test\nIn such cases(where you need high performance, and fast I/O ) we can use SAN.\nSAN is nothing but a high speed network that makes connections between storage devices and servers.\nTraditionally application servers used to have their own storage devices attached to them. Server\u0026rsquo;s talk to these devices by a protocol known as SCSI(Small Computer System Interface). SCSI is nothing but a standard used to communicate between servers and storage devices. All normal hard disks, tape drives etc uses SCSI. In the beginning the storage needs of a server was fulfilled by a storage devices that was included inside the server(the server used to talk to those internal storage device, using SCSI. This is very much similar to how a normal desktop talks to its internal hard disk.).\nDevices like Compact Disk drives are attached to the server(which are part of the server) using SCSI. The main advantage of SCSI for connecting devices to a server was its high throughput. Although this architecture is sufficient for low end requirements, there are few limitations like the below mentioned ones.\nThe server can only access data on the devices, which are directly attached to it. If something happens to the server, access to data will fail (because the storage device is part of the server and is attached to it using SCSI) There is a limit in the number of storage devices the server can access. In case the server needs more storage space, there will be no more space that can be attached, as the SCSI bus can accommodate only a finite number of devices. Also the server using the SCSI storage has to be near the storage device(because parallel SCSI, which is the normal implementation in most computer\u0026rsquo;s and servers, has some distance limitations. It can work up to 25 meters.)\nSome of these limitations can be overcame using DAS (Directly Attached Storage). The media used to directly connect storage to the server can be any one of SCSI, Ethernet, Fiber channel etc.). Low complexity, Low investment, Simplicity in deployment caused DAS to be adopted by many for normal requirement\u0026rsquo;s. The solution was good even performance wise, if used with faster mediums like fiber channel.\nEven an external USB drive attached to a server is also a DAS(well conceptually its DAS, as its directly attached to the server\u0026rsquo;s USB bus). But USB drives are normally not used due to the speed limitation of USB bus. Normally for heavy and large DAS storage solutions, the media used are SAS(Serially attached SCSI). Internally the storage device can use RAID(which normally is the case) or anything to provide storage volumes to servers. SAS storage options provide 6Gb/s speed these days.\nAn example of DAS storage device is Dell\u0026rsquo;s MD1220\nTo the server, a DAS storage will appear very much similar to its own internal drive or an external drive that you plugged in.\nAlthough DAS is good for normal needs and gives good performance, there are limitations like the number of servers that can access it. Storage device, or say DAS storage has to be near to the server (in the same rack or within the limits of the accepted distance of the medium used.).\nIt can be argued that, directly attached storage(DAS) is faster than any other storage methods. This is because it does not involve any overhead of data transfer over the network (all data transfer occurs on a dedicated connection between the server and the storage device. Mostly its Serially attached SCSI or SAS). However due to latest improvement\u0026rsquo;s in fiber channel and other caching mechanism\u0026rsquo;s, SAN also provides better speed\u0026rsquo;s similar to DAS, and in some cases, it surpasses the speed provided by a DAS.\nBefore getting inside SAN, let\u0026rsquo;s understand several media types and methods that are used to interconnect storage devices(when i say storage devices, please dont consider it as one single hard disk. Take it as an array of disk\u0026rsquo;s, probably in some RAID level. Consider it as something like Dell\u0026rsquo;s MD1200).\nwhat is SAS(Serially Attached SCSI), FC(Fibre Channel), and iSCSI (Internet Small Computer System Interface)?\nTraditionally the SCSI devices like the internal hard disk\u0026rsquo;s are connected to a shared parallel SCSI bus. This means all devices attached, will be using the same bus to send/receive data. But shared parallel connections are not good for high accuracy, and create issues during high speed transfers. However a serial connection between the device and the server can increase the overall throughput of the data transfer. SAS connections between storage devices and servers uses a dedicated 300 MB/Sec per disk. Think of SCSI bus that shares the same speed for all devices connected.\nSAS uses the same SCSI commands to send and receive data from a device. Also please do not think that SCSI is only used for internal storage. It is also used for external storage device to be connected to the server.\nIf data transfer performance and reliability is the choice, then using SAS is the best solution. In terms of reliability and error rate SAS disks are much better compared to the old SATA disks. SAS was designed by keeping performance in mind, due to which it is full-duplex. This means, data can be send and received simultaniously from a device using SAS. Also a single SAS host port can connect to multiple SAS drives using expanders. SAS uses point to point data transfer by using serial communication between devices (storage device, like disk drives \u0026amp; disk array\u0026rsquo;s) and hosts.\nThe first generation of SAS provided around 3Gb/s of speed. The second generation of SAS improved this to 6Gb/s. And the third generation (which is currently used by many organization\u0026rsquo;s for extremly high throughput) improved this to 12Gb/s.\nFiber Channel Protocol Fiber Channel is a relatively new interconnection technology used for fast data transfer. The main purpose of its design is to enable transport of data at faster rates with a very less/negligible delay. It can be used to interconnect workstations, peripherals, storage array\u0026rsquo;s etc.\nThe major factor that distinguishes fiber channel from other interconnecting method is that, it can manage both networking and I/O communication over a single channel using the same adapters.\nANSI (American National Standards Institute) standardized Fiber channel during 1988. When we say Fiber (in Fiber channel) do not think that it only supports optical fiber medium. Fiber is a term used for any medium used to interconnect in fiber channel protocol. You can even use copper wire for lower cost.\nPlease note the fact that fiber channel standard from ANSI supports networking, storage and data transfer. Fiber channel is not aware of the type of data that you transfer. It can send SCSI commands encapsulated inside a fiber channel frame(it does not have its own I/O commands to send and receive storage). The main advantage is that it can incorporate widely adopted protocols like SCSI and IP inside.\nThe components of making a fiber channel connection are mentioned below. The below requirement is very minimal to achieve a point to point connection. Typically this can be used for a direct connection between a storage array and a host.\nAn HBA (Host Bus Adapter) with Fiber channel port\nDriver for the HBA card\nCables to interconnect devices in HBA fiber channel port\nAs mentioned earlier, SCSI protocol is encapsulated inside fiber channel. So normally SCSI data has to be modified to a different format that fiber channel can deliver to the destination. And when the destination receives the data it then retranslates it to SCSI.\nYou might be thinking that why do we need this mapping and re-mapping, why cant we directly use SCSI to deliver data. Its because SCSI cannot deliver data to greater distances to large number of devices (or large number of hosts).\nFiber cannel can be used to interconnect systems as far as 10KM (if used with optical fibers. You can increase this distance by having repeaters in between). And you can also transfer data to an extent of 30m using a copper wire for lower cost in fiber cannel.\nWith the emergence of fiber channel switches from variety of major vendors, connecting many large number of storage devices and servers have now become an easy task(provided you have the budget to invest). The networking ability of fiber channel led to the advanced adoption of SAN(Storage Area Networks) for faster, long distance, and reliable data access. Most of the high computing environment\u0026rsquo;s(which requires fast and large volume data transfers) uses fiber channel SAN with optical fiber cables.\nThe current fiber channel standard (called as 16GFC) can transmit data at the rate of 1600MB/s(dont forget the fact that this standard was released in 2011). The upcoming standards in the coming years are expected to provide 3200MB/s and 6400MB/s speed.\niSCSI(Internet Small Computer System Interface )\niSCSI is nothing but an IP based standard for interconnecting storage arrays and hosts. It is used to carry SCSI traffic over IP networks. This is the simplest and cheap solution(although not the best) to connect to a storage device.\nThis is a nice technology for location independent storage. Because it can establish connection to a storage device using local area networks, Wide area network. Its a Storage Area Network interconnection standard. It does not require special cabling and equipments like the case of a fiber channel network.\nTo the system using a storage array with iSCSI, the storage appears as a locally attached disk. This technology came after fiber channel and was widely adopted due to it low cost.\nIts a networking protocol which is made on top of TCP/IP. You can guess that its not at all good performance wise, when compared with fiber channel(simply because everything is running over TCP with no special hardware and modifications to your architecture.)\niSCSI introduces a little bit of CPU load on the server, because the server has to do the extra processing for all storage requests over the network, with the regular TCP.\nRelated: Linux CPU performance Monitoring\niSCSI has the following disadvantages, compared to fiber channel\niSCSI introduces a little bit more latency compared to fiber channel, due to the overhead of IP headers Database applications have small read and write operations, which when done on iSCSI will introduce more latency iSCSI when done on the same LAN, which contains other normal traffic (other infrastructure traffic other than iSCSI), it will introduce a read/write lag or say low performance. The maximum speed/bandwidth is limited to your ethernet and network speed. Even if you aggregate multiple links, it does not scal to the level of a fiber channel.\nNAS(Network Attached Storage)\nThe simplest definition of NAS is \u0026ldquo;Any server that shares its own storage with others on the network and acts as a file server is the simplest form NAS\u0026rdquo;.\nPlease make a note of the fact that Network Attached Storage shares files over the network. Not storage device over the network.\nNAS will be using an ethernet connection for sharing files over the network. The NAS device will have an IP address, and then will be accessible over the network through that IP address. When you access files on a file server on your windows system, its basically NAS.\nThe main difference is in how your computer or the server treats a particular storage. If the computer treats a storage as part of itself(similar to how you attach a DAS to your server), in other words, if the server\u0026rsquo;s processor is responsible for managing the storage attached, it will be some sort of DAS. And if the computer/server treats the storage attached as another computer, which is sharing its data through the network, then its a NAS.\nDirectly attached storage(DAS) can be viewed as any other peripheral device like mouse keyboard etc. Because to the server/computer, its a directly attached storage device. However NAS is another server, or say an equipment having its own computing features that can share its own storage with others.\nEven SAN storage can also be considered as an equipment that has its own processing/computing power. So the main difference between NAS, SAN and DAS is how the server/computer accessing it sees. A DAS storage device appears to the server as part of itself. The server sees it as its own physical part. Although the DAS storage device might not be inside the server(its normally another device with its own storage array), the server sees it as its own internal part(DAS storage appears to the server as its own internal storage)\nWhen we talk about NAS, we need to call them shares rather than storage devices. Because NAS appears to a server as a shared folder instead of a shared device over the network. Please do not forget the fact that NAS devices are computers in themselves, who can share their storage space with others. When you share a folder with access control using SAMBA, its NAS.\nAlthough NAS is a cheaper option for your storage needs. It really does not suit for an enterprise level high performance application. Never ever think of using a database storage (which needs to be high performing) with a NAS. The main downside of using NAS is its performance issue, and dependency on network(most of the times, the LAN which is used for normal traffic is also used for sharing storage with NAS, which makes it more congested)\nRelated: Linux Network Performance Tuning\nWhen you share an export with NFS over the network, its also a form of NAS.\nRelated: NFS Tutorial in Linux\nA NAS is nothing but a device/equipmet/server attached to TCP/IP network, that shares its own storage with other\u0026rsquo;s. If you dig a little deeper, when a file read/write request is send to a NAS share attached to a server, the request is sent in the form of a CIFS(Common internet file system) or NFS(Network File system) requests over the network. The receiving end(NAS device), on receiving the NFS, CIFS request, will then convert it into the local storage I/O command set. This is the reason, why a NAS device has its own processing and computing power.\nSo NAS is file level storage(Because its basically a file sharing technology). This is because it hides the actual file system under the hood. It gives the users an interface to access its shared storage space using NFS, or CIFS.\nRelated: How to do NFS Performance Tuning in Linux\nA common use of NAS you can find is to provide each user with a home directory. These home directories are stored in a NAS device, and mounted to the computer, where the user logs in. As the home directory is networkly accessible, the user can log in from any computer on the network.\nAdvantages of NAS\nNAS has a less complex architecture compared to SAN Its cheaper to deploy in an existing architecture. No modification is required on your architecture, as a normal TCP/IP network is the only requirement\nDisadvantages of NAS NAS is slow Lowever throughput and high latency, due to which it cannot be used for high performance applications\nGetting Back to SAN\nNow let\u0026rsquo;s get back to our discussion of SAN(Storage area network) which we started earlier in the beginning.\nThe first and foremost thing to understand about SAN (apart from the things we already discussed in the beginning) is the fact that its a block level storage solution. And SAN is optimized for high volume of block level data transfer. SAN is performs best when used with fiber channel medium (optical fibers, and a fiber channel switch )\nBoth NAS and SAN solves the problem of keeping the storage device nearer to the server accessing it(which was the case with DAS). A SAN storage can be alloted to a server, which in tern can share it with other\u0026rsquo;s using NAS. Please do not forget the fact that the underlying disks on a DAS, NAS and a SAN can be in any form of a RAID (what makes the real difference is how the server access these storage devices, using which protocol and media).\nThe name Storage Area Network itself implies that the storage resides in its own dedicated network. Hosts can attach the storage device to itself using either Fiber channel, TCP/IP network (SAN uses iSCSI when used over tcp/ip network).\nSAN can be considered as a technology that combines the best features of both DAS and NAS. If you remember, DAS appears to the computer as its own storage device, and is known for good speed, DAS is also a block level storage solution(if you remember, we never talked of CIFS or NFS during DAS). NAS is known for its flexibility, primary access through network, access control etc. SAN combines the best features of both of these worlds together because\u0026hellip;.\nSAN storage also appears to the server as its own storage device Its a block level storage solution Good performance/speed Networking features using iSCSI\nSAN and NAS are not competing technologies, but were designed for different needs and purposes. As SAN is a block level storage solution, its best suited for high performance data base storage, email storage etc. Most modern SAN solutions provide, disk mirroring, archiving backup and replication features as well.\nSAN is a dedicated network of storage devices(can include tape drives storages, raid disk arrays etc) all working together to provide an excellent block level storage. While NAS is a single device/server/computing appliance, sharing its own storage over the network.\nMajor Differences between SAN and NAS\nSAN\nNAS\nBlock level data access\nFile Level Data access\nFiber channel is the primary media used with SAN.\nEthernet is the primary media used with NAS\nSCSI is the main I/O protocol\nNFS/CIFS is used as the main I/O protocol in NAS\nSAN storage appears to the computer as its own storage\nNAS appers as a shared folder to the computer\nIt can have excellent speeds and performance when used with fiber channel media\nIt can sometimes worsen the performance, if the network is being used for other things as well(which normally is the case)\nUsed primarily for higher performance block level data storage\nIs used for long distance small read and write operations\n[1] http://www.slashroot.in/san-vs-nas-difference-between-storage-area-network-and-network-attached-storage\n[2]http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/architecting-lustre-storage-white-paper.pdf\n","date":1296691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296691200,"objectID":"82375dedd8ea5f1b31f88c532e3a0709","permalink":"https://wubigo.com/post/2011-02-03-san/","publishdate":"2011-02-03T00:00:00Z","relpermalink":"/post/2011-02-03-san/","section":"post","summary":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB.","tags":null,"title":"NFS VS. SAN VS. lUSTRE","type":"post"},{"authors":null,"categories":null,"content":" Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; $cat /etc/kubernetes/manifests/etcd.yaml - etcd - --advertise-client-urls=https://192.168.1.11:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.1.11:2380 - --initial-cluster=bigo-vm1=https://192.168.1.11:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.11:2379 - --listen-peer-urls=https://192.168.1.11:2380 - --name=bigo-vm1 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: K8S.gcr.io/etcd:3.2.24 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo  check kube apiserver access of etcd with curl sudo curl -L -v https://192.168.1.11:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key * Trying 192.168.1.11... * Connected to 192.168.1.11 (192.168.1.11) port 2379 (#0) * found 1 certificates in /etc/kubernetes/pki/etcd/ca.crt * found 597 certificates in /etc/ssl/certs * ALPN, offering http/1.1 * SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256 * server certificate verification OK * server certificate status verification SKIPPED * common name: bigo-vm3 (matched) * server certificate expiration date OK * server certificate activation date OK * certificate public key: RSA * certificate version: #3 * subject: CN=bigo-vm3 * start date: Sun, 17 Feb 2019 14:15:39 GMT * expire date: Mon, 17 Feb 2020 14:15:40 GMT * issuer: CN=etcd-ca * compression: NULL * ALPN, server did not agree to a protocol \u0026gt; GET /v3/keys HTTP/1.1 \u0026gt; Host: 192.168.1.11:2379 \u0026gt; User-Agent: curl/7.47.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 404 Not Found \u0026lt; Content-Type: text/plain; charset=utf-8 \u0026lt; X-Content-Type-Options: nosniff \u0026lt; Date: Mon, 18 Feb 2019 02:56:03 GMT \u0026lt; Content-Length: 19 \u0026lt; 404 page not found * Connection #0 to host 192.168.1.11 left intact  ","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"a0a926156dd65e503d0d3db920b3a467","permalink":"https://wubigo.com/post/2011-02-01-etcd-notes/","publishdate":"2011-02-01T00:00:00Z","relpermalink":"/post/2011-02-01-etcd-notes/","section":"post","summary":"Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.","tags":null,"title":"ETCD notes","type":"post"},{"authors":null,"categories":null,"content":" TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk\n","date":1294012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1294012800,"objectID":"890ebb9e3a5d4cfcfbb67b297f57eeea","permalink":"https://wubigo.com/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","publishdate":"2011-01-03T00:00:00Z","relpermalink":"/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","section":"post","summary":"TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk","tags":null,"title":"the-difference-between-a-tmpfs-and-ramfs-ram-disk","type":"post"},{"authors":null,"categories":["IT"],"content":" push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.json \u0026lt;\u0026lt; EOF { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://registry.docker-cn.com\u0026quot;, \u0026quot;https://11h2ev58.mirror.aliyuncs.com\u0026quot;] } EOF scp daemon.json $VM:~/ tee d.sh \u0026lt;\u0026lt; EOF sudo mkdir -p /etc/docker sudo mv daemon.json /etc/docker sudo systemctl daemon-reload sudo systemctl restart docker EOF  ssh $VM \u0026lsquo;bash -s\u0026rsquo; \u0026lt; d.sh rm daemon.json\nclaim docker disk space docker-clean.sh\n#!/usr/bin/env bash # ignoring pipe fail of non-zero exit code set -o pipefail docker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi docker ps --filter status=dead --filter status=exited -aq | xargs docker rm -v [ ! -z \u0026quot;$VM\u0026quot; ] \u0026amp;\u0026amp; ssh $VM 'bash -s' \u0026lt; docker-clean.sh.sh  kube build export K8S_VERSION = v1.13.3 git clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/K8S.io/ git fetch --all git checkout tags/$K8S_VERSION -b v$K8S_VERSION  #!/usr/bin/env bash export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 cd $GOPATH/src/K8S.io/kubernetes/build/ bash -x ./run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1  kubeadm init #!/usr/bin/env bash cat \u0026lt;\u0026lt; EOF \u0026gt; init.sh #!/usr/bin/env bash sudo kubeadm reset -f sudo kubeadm init --kubernetes-version=v1.13.3 --pod-network-cidr 10.2.0.0/16 -v 4 \u0026gt; kubeadm.init.log 2\u0026gt;\u0026amp;1 mkdir -p $HOME/.kube sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config EOF ssh $VM 'bash -s' \u0026lt; init.sh rm init.sh  kube image pull then tag #!/usr/bin/env bash docker pull mirrorgooglecontainers/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause:3.1 docker pull mirrorgooglecontainers/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag mirrorgooglecontainers/kube-apiserver:v1.13.3 K8S.gcr.io/kube-apiserver:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 docker tag mirrorgooglecontainers/pause:3.1 K8S.gcr.io/pause:3.1 docker tag mirrorgooglecontainers/etcd:3.2.24 K8S.gcr.io/etcd:3.2.24 docker tag coredns/coredns:1.2.6 K8S.gcr.io/coredns:1.2.6  prepare kubelet for kubeadm deploy  build\ncd build run.sh make scp ~/go/src/K8S.io/kubernetes/_output/dockerized/bin/linux/amd64/kube??? vm1:~/   deploy K8S master #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi if [ ! -z \u0026quot;$KV\u0026quot; ]; then KV = v1.13.3 echo \u0026quot;VAR VM is not set ,set to $KV\u0026quot; exit fi if [ ! -z \u0026quot;$PN\u0026quot; ]; then PN = 10.2.0.0/16 echo \u0026quot;VAR PN is not set, set to $PN\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf # sudo kubeadm init --kubernetes-version=$KV --pod-network-cidr 10.2.0.0/16 -v 4 if [ -d \u0026quot;$HOME/.kube\u0026quot; ]; then mkdir -p $HOME/.kube fi sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config curl https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/calico.yaml\u0026gt; calico.yaml # calico etcd setup sed -i -e \u0026quot;s/\\(^etcd_endpoints: \\\u0026quot;http.*$\\)/etcd_endpoints: \\\u0026quot;https:\\/\\/$VM:2379\\\u0026quot;/g\u0026quot; calico.yaml # etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; sed -i -e 's/etcd_ca: \\\u0026quot;\\\u0026quot; \\# \\\u0026quot;\\/calico-secrets/etcd-ca\\\u0026quot;/etcd_ca: \\\u0026quot;\\/calico-secrets\\/etcd-ca\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_cert: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/etcd_cert: \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_key: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/etcd_key: \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/g' calico.yaml CA=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) CERT=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0) KEY=$(sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0) sed -i -e \u0026quot;s/# etcd-ca: null/etcd-ca: $CA/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-cert: null/etcd-cert: $CERT/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-key: null/etcd-key: $KEY/g\u0026quot; calico.yaml kubectl apply -f calico.yaml EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  deploy K8S working node #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf EOF TOKEN=$(kubeadm token list) CA_HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //') cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash kubeadm join 192.168.1.11:6443 --token $TOKEN --discovery-token-ca-cert-hash sha256:$CA_HASH EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  replace spaces in file names using a bash script find -name \u0026quot;* *\u0026quot; -type d | rename 's/ /_/g' # do the directories first find -name \u0026quot;* *\u0026quot; -type f | rename 's/ //g'  docker PID $PATH/docker-pid\n#!/usr/bin/env bash exec docker inspect --format '{{ .State.Pid }}' \u0026quot;$@\u0026quot;  #!/usr/bin/env bash\n$PATH/docker-ip\n#!/usr/bin/env bash exec docker inspect --format '{{ .NetworkSettings.IPAddress }}' \u0026quot;$@\u0026quot;  ","date":1293881943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293881943,"objectID":"0ac1ed1f5371d94cd4838907db81c365","permalink":"https://wubigo.com/post/2011-01-01-shell-script/","publishdate":"2011-01-01T19:39:03+08:00","relpermalink":"/post/2011-01-01-shell-script/","section":"post","summary":"push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.","tags":["SHELL","LINUX"],"title":"shell script","type":"post"},{"authors":null,"categories":null,"content":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/co1/tiller:v2.12.3\n ","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2f8ee50015c93c05ef78deeb590ca3e7","permalink":"https://wubigo.com/post/2011-01-01-dockerfile/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/post/2011-01-01-dockerfile/","section":"post","summary":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/co1/tiller:v2.12.3\n ","tags":null,"title":"Dockerfile","type":"post"},{"authors":null,"categories":null,"content":" transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams. QUIC provides similar multi-stream semantics. SCTP also allows data to be transferred over two outgoing paths when a host is connected to two or more networks, optional delivery of out-of-order data, and a number of other features. SCTP’s flow- and congestion-control algorithms are essentially the same as in TCP.\ntunnel The tunneling protocol works by using the data portion of a packet (the payload) to carry the packets that actually provide the service.\nTypically, the delivery protocol operates at an equal or higher level in the layered model than the payload protocol. As an example of network layer over network layer, Generic Routing Encapsulation (GRE), a protocol running over IP (IP protocol number 47), often serves to carry IP packets, with RFC 1918 private addresses, over the Internet using delivery packets with public IP addresses\nDHCP In addition to host IP address assignment, DHCP also allows a host to learn additional information, such as its subnet mask, the address of its first-hop router (often called the default gateway), and the address of its local DNS server.\nlink layer implement Is a host’s link layer implemented in hardware or software? Is it implemented on a separate card or chip, and how does it interface with the rest of a host’s hardware and operating system components? For the most part, the link layer is implemented in a network adapter, also sometimes known as a network interface card (NIC). At the heart of the network adapter is the link-layer controller, usually a single, special-purpose chip that implements many of the link-layer services (framing, link access, error detection, and so on). Thus, much of a link-layer controller’s functionality is implemented in hardware\nlink-layer switches do not have link-layer addresses associated with their interfaces that connect to hosts and routers. This is because the job of the link-layer switch is to carry datagrams between hosts and routers; a switch does this job transparently, that is, without the host or router having to explicitly address the frame to the intervening switch\nMPLS MPLS performs switching based on labels, without needing to consider the IP address of a packet. The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables.\nMPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols. This is one simple form of traffic engineering using MPLS\nIt can be used to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a precomputed failover path in response to link failure MPLS can, and has, been used to implement so-called virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses its MPLS-enabled network to connect together the customer’s various networks. MPLS can be used to isolate both the resources and addressing used by the customer’s VPN from that of other users crossing the ISP’s network\nWhy we use the Linux kernel\u0026rsquo;s TCP stack Since the Linux kernel cannot sustain the 10G packet rate, then some bypass technologies for a fast path are used. The main bypass technologies are either based on a limited set of features such as Open vSwitch (OVS) with its DPDK user space implementation or based on a full feature and offload of Linux processing such as 6WIND Virtual Accelerator.\nhttps://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1264118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264118400,"objectID":"b7eb6215e0ea142fd2bbaac922a070a0","permalink":"https://wubigo.com/post/2010-01-22-computer-networking/","publishdate":"2010-01-22T00:00:00Z","relpermalink":"/post/2010-01-22-computer-networking/","section":"post","summary":"transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams.","tags":null,"title":"computer networking","type":"post"},{"authors":null,"categories":null,"content":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","date":1262649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262649600,"objectID":"70b96e6c4b5506535b6e45525eca92f7","permalink":"https://wubigo.com/post/2010-01-05-%E5%90%8D%E8%A8%80/","publishdate":"2010-01-05T00:00:00Z","relpermalink":"/post/2010-01-05-%E5%90%8D%E8%A8%80/","section":"post","summary":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","tags":null,"title":"名言","type":"post"},{"authors":null,"categories":null,"content":" turn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  ","date":1262476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262476800,"objectID":"15cda36938065ff8abd3117931fa057d","permalink":"https://wubigo.com/post/2010-01-03-windows-note/","publishdate":"2010-01-03T00:00:00Z","relpermalink":"/post/2010-01-03-windows-note/","section":"post","summary":" turn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  ","tags":null,"title":"windows notes","type":"post"},{"authors":null,"categories":["IT"],"content":" visibility and Atomicity in the absence of synchronization, there are a number of reasons a thread might not immediately ‐ or ever ‐ see the results of an operation in another thread. Compilers may generate instructions in a different order than the \u0026ldquo;obvious\u0026rdquo; one suggested by the source code, or store variables in registers instead of in memory; processors may execute instructions in parallel or out of order; caches may vary the order in which writes to variables are committed to main memory; and values stored in processor‐local caches may not be visible to other processors. These factors can prevent a thread from seeing the most up‐to‐date value for a variable and can cause memory actions in other threads to appear to happen out of order ‐ if you don\u0026rsquo;t use adequate synchronization.\nLock and ReentrantLock Before Java 5.0, the only mechanisms for coordinating access to shared data were synchronized and volatile. Java 5.0 adds another option: ReentrantLock. Contrary to what some have written, ReentrantLock is not a replacement for intrinsic locking, but rather an alternative with advanced features for when intrinsic locking proves too limited\nIntrinsic locking works fine in most situations but has some functional limitations ‐ it is not possible to interrupt a thread waiting to acquire a lock, or to attempt to acquire a lock without being willing to wait for it forever. Intrinsic locks also must be released in the same block of code in which they are acquired; this simplifies coding and interacts nicely with exception handling, but makes non‐blockstructured locking disciplines impossible\nReadWriteLock The locking strategy implemented by read‐write locks allows multiple simultaneous readers but only a single writer. In practice, read‐write locks can improve performance for frequently accessed read‐mostly data structures on multiprocessor systems; under other conditions they perform slightly worse than exclusive locks due to their greater complexity. Whether they are an improvement in any given situation is best determined via profiling; because ReadWriteLock uses Lock for the read and write portions of the lock, it is relatively easy to swap out a read‐write lock for an exclusive one if profiling determines that a read‐write lock is not a win. hashtable  Hashtable is synchronized, whereas HashMap is not. This makes HashMap better for non-threaded applications, as unsynchronized Objects typically perform better than synchronized ones. Hashtable does not allow null keys or values. HashMap allows one null key and any number of null values. One of HashMap\u0026rsquo;s subclasses is LinkedHashMap, so in the event that you\u0026rsquo;d want predictable iteration order (which is insertion order by default), you could easily swap out the HashMap for a LinkedHashMap. This wouldn\u0026rsquo;t be as easy if you were using Hashtable.  HashTable is obsolete in Java 1.7 and it is recommended to use ConcurrentMap implementation\nJava Memory Model The Java Memory Model is specified in terms of actions, which include reads and writes to variables, locks and unlocks of monitors, and starting and joining with threads. The JMM defines a partial ordering [2] called happens‐before on all actions within the program\nsoft reference four different degrees of reference strength: strong, soft, weak, and phantom, in order from strongest to weakest\nSoftReferences aren\u0026rsquo;t required to behave any differently than WeakReferences, but in practice softly reachable objects are generally retained as long as memory is in plentiful supply. This makes them an excellent foundation for a cache, such as the image cache described above\nchecked exception The cardinal rule in deciding whether to use a checked or an unchecked exception is this: use checked exceptions for conditions from which the caller can reasonably be expected to recover. By throwing a checked exception, you force the caller to handle the exception in a catch clause or to propagate it outward. Each checked exception that a method is declared to throw is therefore a potent indication to the API user that the associated condition is a possible outcome of invoking the method.\n","date":1262345943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262345943,"objectID":"d3042f9c95904fa8652edae56c53b1d0","permalink":"https://wubigo.com/post/2010-01-01-java-notes/","publishdate":"2010-01-01T19:39:03+08:00","relpermalink":"/post/2010-01-01-java-notes/","section":"post","summary":"visibility and Atomicity in the absence of synchronization, there are a number of reasons a thread might not immediately ‐ or ever ‐ see the results of an operation in another thread. Compilers may generate instructions in a different order than the \u0026ldquo;obvious\u0026rdquo; one suggested by the source code, or store variables in registers instead of in memory; processors may execute instructions in parallel or out of order; caches may vary the order in which writes to variables are committed to main memory; and values stored in processor‐local caches may not be visible to other processors.","tags":["JAVA"],"title":"java notes","type":"post"},{"authors":null,"categories":null,"content":" certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le\nNginx configuration to enable ACME Challenge support  #Rule for legitimate ACME Challenge requests (like /.well-known/acme-challenge/xxxxxxxxx) #We use ^~ here, so that we don't check other regexes (for speed-up). We actually MUST cancel #other regex checks, because in our other config files have regex rule that denies access to files with dotted names. location ^~ /.well- known/acme-challenge/ { #Set correct content type. According to this: #https://community.letsencrypt.org/t/using-the-webroot-domain-verification-method/1445/29 #Current specification requires \u0026quot;text/plain\u0026quot; or no content header at all. #It seems that \u0026quot;text/plain\u0026quot; is a safe option. default_type \u0026quot;text/plain\u0026quot;; #This directory must be the same as in /etc/letsencrypt/cli.ini #as \u0026quot;webroot-path\u0026quot; parameter. Also don't forget to set \u0026quot;authenticator\u0026quot; parameter #there to \u0026quot;webroot\u0026quot;. #Do NOT use alias, use root! Target directory is located here: #/var/www/common/letsencrypt/.well-known/acme-challenge/ root /var/www/letsencrypt; } #Hide /acme-challenge subdirectory and return 404 on all requests. #It is somewhat more secure than letting Nginx return 403. #Ending slash is important! location = /.well-known/acme-challenge/ { return 404; }  nginx need restart after certbot renew to avoid sec_error_expired_certificate ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b2a09ac3f3428209e3c3505d95b51b0f","permalink":"https://wubigo.com/post/letsencrypt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/letsencrypt/","section":"post","summary":"certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.0.17 Loaded Configuration File\t/etc/php/7.0/fpm/php.ini sudo apt-get install php-geoip php-dev libgeoip-dev sudo pecl install geoip sudo nano /etc/php/7.0/fpm/php.ini [PHP] ;AFTER THE PHP SECTION NOT BEFORE extension=geoip.so [gd] ;AFTER THE gd SECTION NOT BEFORE geoip.custom_directory=/usr/share/nginx/html/piwik/misc cd /usr/share/nginx/html/piwik/misc sudo wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz sudo gunzip GeoLiteCity.dat.gz PECL extension won't recognize the database if it's named GeoLiteCity.dat so make sure it is named GeoIPCity.dat: sudo mv GeoLiteCity.dat GeoIPCity.dat Restart the Apache Web Server: sudo service nginx restart Step Five - Configure Piwik to use GeoIP PECL Open your browser and login into your Piwik page, go to settings, Geolocation, and choose GeoIP (PECL) as your location provider. Updating Previous Visits and Updating the GeoIP Database sudo apt-get install php-mysql sudo php /usr/share/nginx/html/piwik/console usercountry:attribute 2017-01-01,2017-08-10  nginx http { geoip_country /usr/share/nginx/html/piwik/misc/GeoIP.dat; geoip_city /usr/share/nginx/html/piwik/misc/GeoIPCity.dat;  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e16ad23ea6e49aa8256164d577385637","permalink":"https://wubigo.com/post/piwik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/piwik/","section":"post","summary":"Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.e. \u0026ldquo;postgres\u0026rdquo;, to the database called \u0026ldquo;postgres\u0026rdquo; (1st argument to psql).\nSet a password for the \u0026ldquo;postgres\u0026rdquo; database role using the command:\n\\password postgres and give your password when prompted. The password text will be hidden from the console for security purposes.\nType Control+D or \\q to exit the posgreSQL prompt.\nCreate database\nTo create the first database, which we will call \u0026ldquo;mydb\u0026rdquo;, simply type:\nsudo -u postgres createdb mydb\n#sudo nano /etc/postgresql/9.3/main/pg_hba.conf and change the line host all all 0.0.0.0/0 md5\nDatabase administrative login by Unix domain socket local all postgres peer to\nDatabase administrative login by Unix domain socket local all postgres md5 Now you should reload the server configuration changes and connect pgAdmin III to your PostgreSQL database server.\nsudo /etc/init.d/postgresql reload\ncayley_v0.6.1_linux_amd64$ cat cayley.cfg { \u0026quot;listen_host\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;sql\u0026quot;, \u0026quot;db_path\u0026quot;: \u0026quot;postgres://postgres:psql@db/cayley?sslmode=disable\u0026quot;, \u0026quot;read_only\u0026quot;: false } $cayley init --config=cayley.cfg $cayley http --config=cayley.cfg $cayley load --config=cayley.cfg --quads=data/testdata.nq  Cayley looks in the following locations for the configuration file Command line flag The environment variable $CAYLEY_CFG /etc/cayley.cfg  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c5f2097b456fdf36712de2e9110e7c5a","permalink":"https://wubigo.com/post/psql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/psql/","section":"post","summary":"Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53d0412037dd477a59505788df13a330","permalink":"https://wubigo.com/post/smtp_ec2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/smtp_ec2/","section":"post","summary":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/","tags":null,"title":"","type":"post"}]
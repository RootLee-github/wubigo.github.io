[{"authors":["admin"],"categories":null,"content":" 超过10年的公有云/混合云/超融合系统开发运营经验，实施过多个行业例如电信，医疗，教育，建筑, 媒体行业AT2C案例 管理超过60PB的数据中心，并提供基于hadoop生态的数据仓库管理和数据分析服务 精通软件系统架构(ToGAF, CS/BS, DDA, SOA, 微服务，函数计算)  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://wubigo.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":" 超过10年的公有云/混合云/超融合系统开发运营经验，实施过多个行业例如电信，医疗，教育，建筑, 媒体行业AT2C案例 管理超过60PB的数据中心，并提供基于hadoop生态的数据仓库管理和数据分析服务 精通软件系统架构(ToGAF, CS/BS, DDA, SOA, 微服务，函数计算)  ","tags":null,"title":"Wu Bigo","type":"author"},{"authors":null,"categories":[],"content":" 计算 尽管函数计算和容器的快速崛起，EC2依然是AWS的业务焦点，\n主要的新功能包括：\n 基于Nitro平台的针对HPC和机器学习的负载的实例\n 基于定制芯片Inferencia针对机器学习实例\n 标准实例支持100Gb网络带宽\n  网络  传输网关支持多播\n 加速的网络到网络的VPN链接\n  存储和数据分析  S3 Access Points和数据湖\n ES搜索支持S3\n 联合查询支持关系数据库，REDSHIFT数据仓库，S3数据湖，而不需要移动数据\n AQUA查询加速器\n 数据湖导出(REDSHIFT数据仓库查询结果能直接导出到S3，并以Parquet格式存放)\n 托管的Cassandra服务\n  函数计算  Provisioned Concurrency for Lambda Functions\n EKS正式支持FARGATE\n RDS PROXY\n  先锋基金IT迁移架构图 私有云架构包括4千万行的单体应用，hadoop数据仓库(20PB)和PaaS(2015年) 数据仓库和PaaS迁移到AWS PaaS实施基于EDA架构的改造 PaaS迁移到Fargate ","date":1575776253,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575776253,"objectID":"f6dcdf6a8ac6b35027a93018cf80a2ca","permalink":"https://wubigo.com/post/aws-reinvent-2019-keynote/","publishdate":"2019-12-08T11:37:33+08:00","relpermalink":"/post/aws-reinvent-2019-keynote/","section":"post","summary":" 计算 尽管函数计算和容器的快速崛起，EC2依然是AWS的业务焦点，\n主要的新功能包括：\n 基于Nitro平台的针对HPC和机器学习的负载的实例\n 基于定制芯片Inferencia针对机器学习实例\n 标准实例支持100Gb网络带宽\n  网络  传输网关支持多播\n 加速的网络到网络的VPN链接\n  存储和数据分析  S3 Access Points和数据湖\n ES搜索支持S3\n 联合查询支持关系数据库，REDSHIFT数据仓库，S3数据湖，而不需要移动数据\n AQUA查询加速器\n 数据湖导出(REDSHIFT数据仓库查询结果能直接导出到S3，并以Parquet格式存放)\n 托管的Cassandra服务\n  函数计算  Provisioned Concurrency for Lambda Functions\n EKS正式支持FARGATE\n RDS PROXY\n  先锋基金IT迁移架构图 私有云架构包括4千万行的单体应用，hadoop数据仓库(20PB)和PaaS(2015年) 数据仓库和PaaS迁移到AWS PaaS实施基于EDA架构的改造 PaaS迁移到Fargate ","tags":["AWS","CLOUD"],"title":"AWS Re:Invent 2019主题汇总-P1","type":"post"},{"authors":null,"categories":[],"content":" Provisioned Concurrency for Lambda Functions To provide customers with improved control over their mission-critical app performance on serverless, AWS introduces Provisioned Concurrency, which is a Lambda feature and works with any trigger. For example, you can use it with WebSockets APIs, GraphQL resolvers, or IoT Rules. This feature gives you more control when building serverless applications that require low latency, such as web and mobile apps, games, or any service that is part of a complex transaction.\nThis is a feature that keeps functions initialized and hyper-ready to respond in double-digit milliseconds. This addition is helpful for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, or synchronous APIs.\nOn enabling Provisioned Concurrency for a function, the Lambda service will initialize the requested number of execution environments so they can be ready to respond to invocations.\nTo know more Provisioned Concurrency in detail, read the Provisioned Concurrency for Lambda Functions\n","date":1575774066,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575774066,"objectID":"369ab3782cadacee3e3cbfe41f008f18","permalink":"https://wubigo.com/post/lambda-provisioned-concurrency/","publishdate":"2019-12-08T11:01:06+08:00","relpermalink":"/post/lambda-provisioned-concurrency/","section":"post","summary":"Provisioned Concurrency for Lambda Functions To provide customers with improved control over their mission-critical app performance on serverless, AWS introduces Provisioned Concurrency, which is a Lambda feature and works with any trigger. For example, you can use it with WebSockets APIs, GraphQL resolvers, or IoT Rules. This feature gives you more control when building serverless applications that require low latency, such as web and mobile apps, games, or any service that is part of a complex transaction.","tags":["LAMBDA","SERVERLESS"],"title":"Lambda Provisioned Concurrency","type":"post"},{"authors":null,"categories":[],"content":"aws sts get-caller-identity aws s3control list-access-points --account-id 46569194568 aws s3control create-access-point --name my-access-point --account-id 46569194568 --bucket wubigo aws s3control get-access-point --account-id \u0026quot;46569194568\u0026quot; --name my-access-point { \u0026quot;Name\u0026quot;: \u0026quot;my-access-point\u0026quot;, \u0026quot;PublicAccessBlockConfiguration\u0026quot;: { \u0026quot;IgnorePublicAcls\u0026quot;: true, \u0026quot;BlockPublicPolicy\u0026quot;: true, \u0026quot;BlockPublicAcls\u0026quot;: true, \u0026quot;RestrictPublicBuckets\u0026quot;: true }, \u0026quot;CreationDate\u0026quot;: \u0026quot;2019-12-04T14:24:38Z\u0026quot;, \u0026quot;Bucket\u0026quot;: \u0026quot;wubigo\u0026quot;, \u0026quot;NetworkOrigin\u0026quot;: \u0026quot;Internet\u0026quot; }  ","date":1575469238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575469238,"objectID":"ab48a8832e073dbb44caa5d57c02350e","permalink":"https://wubigo.com/post/aws-s3-access-point/","publishdate":"2019-12-04T22:20:38+08:00","relpermalink":"/post/aws-s3-access-point/","section":"post","summary":"aws sts get-caller-identity aws s3control list-access-points --account-id 46569194568 aws s3control create-access-point --name my-access-point --account-id 46569194568 --bucket wubigo aws s3control get-access-point --account-id \u0026quot;46569194568\u0026quot; --name my-access-point { \u0026quot;Name\u0026quot;: \u0026quot;my-access-point\u0026quot;, \u0026quot;PublicAccessBlockConfiguration\u0026quot;: { \u0026quot;IgnorePublicAcls\u0026quot;: true, \u0026quot;BlockPublicPolicy\u0026quot;: true, \u0026quot;BlockPublicAcls\u0026quot;: true, \u0026quot;RestrictPublicBuckets\u0026quot;: true }, \u0026quot;CreationDate\u0026quot;: \u0026quot;2019-12-04T14:24:38Z\u0026quot;, \u0026quot;Bucket\u0026quot;: \u0026quot;wubigo\u0026quot;, \u0026quot;NetworkOrigin\u0026quot;: \u0026quot;Internet\u0026quot; }  ","tags":["AWS","S3"],"title":"Aws S3 Access Point","type":"post"},{"authors":null,"categories":[],"content":" JAVA 这两年最重要的项目就是GRAAL的正式版发布。\nGRAAL能做什么？\n 让解释性程序例如JAVA, JS 运行的更快: AOT编译为宿主二进制可执行文件,  启动时间小于100ms， 像C, GO, ERLANG一样的执行速度\n 更低的内存占用：只占用传统的JVM应用20%的内存\n  听起来是不是该项目为函数计算做准备的？\n是，但不完全是。\nGRAAL的官方目标是提供一个统一的虚拟机执行平台，支持如下运行环境：\n JavaScrip Python Ruby R JVM 语言（Java, Scala, Groovy, Kotlin, Clojure） LLVM语言 (C , C++)  而且不同语言之间零成本互相调用\n安装 wget https://github.com/oracle/graal/releases/download/vm-19.2.1/graalvm-ce-linux-amd64-19.2.1.tar.gz tar zxvf graalvm-ce-linux-amd64-19.2.1.tar.gz export PATH=$PATH:$GRAAL_HOME/bin   检查  js --version GraalVM JavaScript (GraalVM CE Native 19.2.1)   安装native-image  gu install native-image  gu available Downloading: Component catalog from www.graalvm.org ComponentId Version Component name Origin -------------------------------------------------------------------------------- llvm-toolchain 19.2.1 LLVM.org toolchain github.com native-image 19.2.1 Native Image github.com python 19.2.1 Graal.Python github.com R 19.2.1 FastR github.com ruby 19.2.1 TruffleRuby github.com  使用Polyglot Shell polyglot --jvm --shell  创建JAVA编写的可执行二进制文件  安装glibc-devel, zlib-devel (头文件C库 and zlib) 和 gcc  sudo apt-get install libz-dev  HelloWorld.java\npublic class HelloWorld { public static void main(String... args) { System.out.println(\u0026quot;Hello World\u0026quot;); } }   编译  javac HelloWorld.java native-image -cp . HelloWorld Build on Server(pid: 20375, port: 45977) [helloworld:20375] classlist: 199.03 ms [helloworld:20375] (cap): 1,866.60 ms [helloworld:20375] setup: 5,938.57 ms [helloworld:20375] (typeflow): 17,532.76 ms [helloworld:20375] (objects): 8,477.10 ms [helloworld:20375] (features): 2,365.65 ms [helloworld:20375] analysis: 28,469.52 ms [helloworld:20375] (clinit): 861.90 ms [helloworld:20375] universe: 2,785.89 ms [helloworld:20375] (parse): 9,430.36 ms [helloworld:20375] (inline): 1,623.19 ms [helloworld:20375] (compile): 11,158.60 ms [helloworld:20375] compile: 22,588.75 ms [helloworld:20375] image: 687.25 ms [helloworld:20375] write: 1,153.04 ms [helloworld:20375] [total]: 62,321.01 ms   执行  ./helloworld Hello World  部署到容器 微服务 http://sparkjava.com/\nhttps://quarkus.io/get-started/\nhttps://github.com/spring-projects/spring-framework/wiki/GraalVM-native-image-support\n参考 https://royvanrijn.com/blog/2018/09/part-2-native-microservice-in-graalvm/\n","date":1572315423,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572315423,"objectID":"e513e714d31637dba20991cb752cca29","permalink":"https://wubigo.com/post/java-last-2-years/","publishdate":"2019-10-29T10:17:03+08:00","relpermalink":"/post/java-last-2-years/","section":"post","summary":"JAVA 这两年最重要的项目就是GRAAL的正式版发布。\nGRAAL能做什么？\n 让解释性程序例如JAVA, JS 运行的更快: AOT编译为宿主二进制可执行文件,  启动时间小于100ms， 像C, GO, ERLANG一样的执行速度\n 更低的内存占用：只占用传统的JVM应用20%的内存\n  听起来是不是该项目为函数计算做准备的？\n是，但不完全是。\nGRAAL的官方目标是提供一个统一的虚拟机执行平台，支持如下运行环境：\n JavaScrip Python Ruby R JVM 语言（Java, Scala, Groovy, Kotlin, Clojure） LLVM语言 (C , C++)  而且不同语言之间零成本互相调用\n安装 wget https://github.com/oracle/graal/releases/download/vm-19.2.1/graalvm-ce-linux-amd64-19.2.1.tar.gz tar zxvf graalvm-ce-linux-amd64-19.2.1.tar.gz export PATH=$PATH:$GRAAL_HOME/bin   检查  js --version GraalVM JavaScript (GraalVM CE Native 19.2.1)   安装native-image  gu install native-image  gu available Downloading: Component catalog from www.","tags":["JAVA","GRAAL"],"title":"JAVA这两年","type":"post"},{"authors":null,"categories":[],"content":" 第一代 2003年， jBPM 1.0发布。\n运行环境：J2EE\n过程定义语言：jPDL(当时工作流厂商都有各自的过程定义语言和建模工具)\n当时的主流的技术： applets, Swing桌面和EJB\n第二代 2004年，jBPM 2.0发布\n同时jBPM加入JBoss基金会.\n运行环境：任何JAVA环境(POJO实现过程运行时)，不需要应用服务器\n第三代 2005年, jBPM 3.0发布，支持BPEL\n过程定义语言：过程虚拟机\n架构： 与二代相比，架构发生了巨大变化。可以操作的业务功能大范围扩展，不仅通\n过JAVA实现状态机，而且支持建模\nHIBERNETE作为持久机制并同时提供会话对象的概念，\n工作流引擎所有的相关性交互都纳入contextual block范畴\n这为以后的工作流命令设计模式和命令拦截设计模式的广泛应用打下良好的基础\n第四代 2009年， jBPM 4.0 alpha版发布.\n过程虚拟机成功工作流引擎的核心。\n过程定义语言：BPMN, jPDL 和 BPEL\n因为团队人员离开并启动Activiti，正式版没能发布。\n主要改进：\n 无状态的服务API 运行时和历史数据的分离： 保证运行时持久的性能  第五代 2010年, Activiti 1发布\n改变：\n 版权从LGPL转到APACHE.\n 过程定义语言：BPMN(唯一)\n 从性能和扩展性加强PVM\n 多租户支持\n 轻量级架构\n  第六代 2017年，flowable 6.0发布。\n改变：\n 过程模型：放弃PVM,使用原生BPMN， 实现真正的动态过程执行和复杂的过程迁移\n 数据远完全抽象：支持NoSQL\n CMMN支持\n 函数式工作流\n  ","date":1572146762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572146762,"objectID":"e0dfe5b99639eb697878a92a13287890","permalink":"https://wubigo.com/post/digital-evolution-of-open-source-business-process-management/","publishdate":"2019-10-27T11:26:02+08:00","relpermalink":"/post/digital-evolution-of-open-source-business-process-management/","section":"post","summary":"第一代 2003年， jBPM 1.0发布。\n运行环境：J2EE\n过程定义语言：jPDL(当时工作流厂商都有各自的过程定义语言和建模工具)\n当时的主流的技术： applets, Swing桌面和EJB\n第二代 2004年，jBPM 2.0发布\n同时jBPM加入JBoss基金会.\n运行环境：任何JAVA环境(POJO实现过程运行时)，不需要应用服务器\n第三代 2005年, jBPM 3.0发布，支持BPEL\n过程定义语言：过程虚拟机\n架构： 与二代相比，架构发生了巨大变化。可以操作的业务功能大范围扩展，不仅通\n过JAVA实现状态机，而且支持建模\nHIBERNETE作为持久机制并同时提供会话对象的概念，\n工作流引擎所有的相关性交互都纳入contextual block范畴\n这为以后的工作流命令设计模式和命令拦截设计模式的广泛应用打下良好的基础\n第四代 2009年， jBPM 4.0 alpha版发布.\n过程虚拟机成功工作流引擎的核心。\n过程定义语言：BPMN, jPDL 和 BPEL\n因为团队人员离开并启动Activiti，正式版没能发布。\n主要改进：\n 无状态的服务API 运行时和历史数据的分离： 保证运行时持久的性能  第五代 2010年, Activiti 1发布\n改变：\n 版权从LGPL转到APACHE.\n 过程定义语言：BPMN(唯一)\n 从性能和扩展性加强PVM\n 多租户支持\n 轻量级架构\n  第六代 2017年，flowable 6.0发布。\n改变：\n 过程模型：放弃PVM,使用原生BPMN， 实现真正的动态过程执行和复杂的过程迁移\n 数据远完全抽象：支持NoSQL","tags":["BPM","工作流"],"title":"开源的工作流引擎技术演进历史","type":"post"},{"authors":null,"categories":[],"content":"心理学家：21世纪最重要的工作技能\n专注是21世纪最重要的工作技能，\n可是很多人没有意识到这一点。\n1971年的时候心理学家西蒙就说过：“大量的信息\n意味着另一种东西变得很稀缺：注意力”。 几十年之前这是个事实，\n在21世纪，注意力变的更加珍贵。\n工作环境正在发生快速的变化，在不远的将来，\n在这个世界只存在两种人：\n 一种注意力被别人完全控制和操纵的人\n 一种注意力不可被打扰的人\n  研究人员告诉我们专注和注意力人们创新发展的原材料\n下面是一些常见的工作干扰：\n 聊天群\n 会议\n 电话\n 邮件\n 同事\n  ","date":1570868399,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570868399,"objectID":"0cb72b155cad394555a5f8fb31a9ffe2","permalink":"https://wubigo.com/post/no.-1-work-skill-of-the-future/","publishdate":"2019-10-12T16:19:59+08:00","relpermalink":"/post/no.-1-work-skill-of-the-future/","section":"post","summary":"心理学家：21世纪最重要的工作技能\n专注是21世纪最重要的工作技能，\n可是很多人没有意识到这一点。\n1971年的时候心理学家西蒙就说过：“大量的信息\n意味着另一种东西变得很稀缺：注意力”。 几十年之前这是个事实，\n在21世纪，注意力变的更加珍贵。\n工作环境正在发生快速的变化，在不远的将来，\n在这个世界只存在两种人：\n 一种注意力被别人完全控制和操纵的人\n 一种注意力不可被打扰的人\n  研究人员告诉我们专注和注意力人们创新发展的原材料\n下面是一些常见的工作干扰：\n 聊天群\n 会议\n 电话\n 邮件\n 同事\n  ","tags":[],"title":"心理学家：21世纪最重要的工作技能","type":"post"},{"authors":null,"categories":[],"content":"","date":1570786790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570786790,"objectID":"f711712646ea1553fb09bd8cd950c6b6","permalink":"https://wubigo.com/post/function-computing-rise-like-s3/","publishdate":"2019-10-11T17:39:50+08:00","relpermalink":"/post/function-computing-rise-like-s3/","section":"post","summary":"","tags":[],"title":"函数计算颠覆对象执行环境(像S3对象存储一样)","type":"post"},{"authors":null,"categories":[],"content":" 在windows中使用docker有多种方式：\n docker WIN10 desktop\n WSL\n  本文主要介绍在WSL中使用docker\n前提条件  Windows 10 Version 1803以上(支持cgroups)\n Ubuntu for WSL 16.0.4 LTS(WSL支持的最新版本)\n Docker 17.09\n  安装WSL install WSL\n安装Ubuntu for WSL 16.0.4 LTS install ubuntu in WSL\n安装DOCKER  启动ubuntu in WSL(以管理员身份运行)  c:\\\u0026gt;wsl -d Ubuntu-16.04   安装docker  $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; $ sudo apt-get install docker-ce=17.09.0~ce-0~ubuntu $ sudo usermod -aG docker $USER   启动docker服务  /usr/local/sbin/start_docker.sh\n#!/usr/bin/env bash sudo cgroupfs-mount sudo service docker start  在系统启动的时候运行docker服务 在控制面版\\管理工具\\计划任务创建任务\n操作的参数如下：\n-c \u0026quot;sudo /bin/sh /usr/local/sbin/start_docker.sh\u0026quot;  备份WSL 在备份前保存并关闭所有的WSL进程，否则正在运行的进程被强行关闭\nc:\\\u0026gt;wsl -l c:\\\u0026gt;wsl --export Ubuntu-16.04 Ubuntu-16.wsl.export.tar  REF\n1\n","date":1570515679,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570515679,"objectID":"8fde9949f2d493404d430487d0ef5aea","permalink":"https://wubigo.com/post/docker-within-wsl/","publishdate":"2019-10-08T14:21:19+08:00","relpermalink":"/post/docker-within-wsl/","section":"post","summary":"在windows中使用docker有多种方式：\n docker WIN10 desktop\n WSL\n  本文主要介绍在WSL中使用docker\n前提条件  Windows 10 Version 1803以上(支持cgroups)\n Ubuntu for WSL 16.0.4 LTS(WSL支持的最新版本)\n Docker 17.09\n  安装WSL install WSL\n安装Ubuntu for WSL 16.0.4 LTS install ubuntu in WSL\n安装DOCKER  启动ubuntu in WSL(以管理员身份运行)  c:\\\u0026gt;wsl -d Ubuntu-16.04   安装docker  $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; $ sudo apt-get install docker-ce=17.","tags":["DOCKER","WIN"],"title":"在WSL中使用Docker","type":"post"},{"authors":null,"categories":[],"content":" 昨天看到AWS STEP FUNCTIONS支持嵌套的工作流\n，当时就被震惊了。 AWS早些年推出SWF框架提供工作流服务，\n后来工作流服务就没有大的工作，SWF框架的核心开发者一部分\n离职去了UBER开发Cadence。没想到沉寂了多年的AWS会在STEP FUNCTIONS\n支持工作流，看来这个千亿规模的市场又快被颠覆了。\n20多年来，工作流都是超大型企业的配置专利，而STEP function的推出\n可以预计，高大上的工作流服务会很快走进很多中小企业工作台并被普及应用。\n回顾一下自己的工作历史：\n从以前的数据中心服务器SA(2006), 到虚拟主机ESX，\n再到OPENSTACK(2010)搭建混合云，\n然后利用K8S(2014)搭建PaaS，现在SERVERLESS,\n计算架构正在发生快速的演进。本文梳理了算力演进历史和未来\n内容\n 从虚拟主机到容器\n 从容器到unikernel\n 函数计算的蓬勃发展\n  什么是虚拟计算 hypervisor分类  Type-1 裸机  KVM, QEMU, VMWare, Virtualbox\n Type-2 托管  XenServer， Hyper-v， KVM, ESX, Xen\n虚拟机的问题  贵  一台云主机(8G/4Core/500MBps)在2017年的超过6000元/年\n 操作慢  一个普通的镜像在2G左右，再加上JAVA应用，一个镜像需要10G.\n启动，备份非常不便。\n容器计算 |虚拟机| 容器 |:\u0026mdash;|:\u0026mdash;|:\u0026mdash;| 构件| 完整的操作系统和应用| 微内核和应用 虚拟技术| 硬件虚拟化| 操作系统虚拟化\n容器计算带来什么好处  容器镜像小  alpine的容量是2M\n 容器占用的硬件资源更少  一台PC可以启动上百个容器\n 容器启动快  一般几毫秒可以启动\n 容器不用备份  容器创建只需要一个Dockerfile，容器镜像是只读镜像\n 容器和微服务器架构，DevOps, CI/CD天然融合  容器存在的问题  安全  容器共享操作系统内核，具体较低的隔离级别，\n如果内核出问题，其他的容器也会处于风险之中\n 网络  如何在足够的隔离级别和复杂的高效网络连接权衡\n容器编排  K8S\n EKS\n SWARM\n MARATHON\n MAGNM\n FLEET\n  用户轻松在计算集群里面部署，管理，扩展基于容器的应用而\n不用关心容器和服务器的绑定，系统扩容等问题\n容器网络  CNM  DOCKER规范，libnetwork实现\n内置的驱动包括none, host, bridge , overlay, MACvlan\n$ docker network ls NAME DRIVER SCOPE 68343147e103 bridge bridge local 5d7df1d8f633 docker_default bridge local d3990aab14a9 host host local fe4ec77439f4 none null local   CNI  COREOS规范，被K8S, MESOS, CLOUD Foundry采用\njson格式的网络模式定义\ndocker的核心组件  LXC\n AUFS\n  DOCKER的优势  镜像不可修改\n 部署没有第三方依赖\n 注册器简单且容易扩展\n 容易回滚\n  对DOCKER的误解：\n 如果学习了docker就不必学其他的系统知识了\n 每个容器只能有一个进程\n 用容器了就不必使用其他的配置管理工具了\n 必须使用容器才能达到高效敏捷和一致性状态优势\n  Open Containers Initiative UniKernel 函数计算的兴起 No server is easier to manage than no-server\n背景：\n 前端技术演进   强大的原生客户/移动客户端让开发者通过调用各种云服务  编写大规模互联网应用，替换传统的后台服务\n HTTP/S应用接口及基于token的安全认证成为工业标准   后台的定制化开发  基于云服务器定制开发\nServerLess VS PaaS PasS可能是ServerLess的一个迭代\n总结  虚拟机是一种更成熟的技术，更安全\n 容器虚拟化是一个更适合微服务器架构的方案\n 虚拟机和容器并不是互斥而是互补\n 函数计算是一下个\u0026hellip;\n  ","date":1569455317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569455317,"objectID":"6d2763c07718c307f3026de164a600e2","permalink":"https://wubigo.com/post/from-vm-to-container-to-serverless/","publishdate":"2019-09-26T07:48:37+08:00","relpermalink":"/post/from-vm-to-container-to-serverless/","section":"post","summary":"昨天看到AWS STEP FUNCTIONS支持嵌套的工作流\n，当时就被震惊了。 AWS早些年推出SWF框架提供工作流服务，\n后来工作流服务就没有大的工作，SWF框架的核心开发者一部分\n离职去了UBER开发Cadence。没想到沉寂了多年的AWS会在STEP FUNCTIONS\n支持工作流，看来这个千亿规模的市场又快被颠覆了。\n20多年来，工作流都是超大型企业的配置专利，而STEP function的推出\n可以预计，高大上的工作流服务会很快走进很多中小企业工作台并被普及应用。\n回顾一下自己的工作历史：\n从以前的数据中心服务器SA(2006), 到虚拟主机ESX，\n再到OPENSTACK(2010)搭建混合云，\n然后利用K8S(2014)搭建PaaS，现在SERVERLESS,\n计算架构正在发生快速的演进。本文梳理了算力演进历史和未来\n内容\n 从虚拟主机到容器\n 从容器到unikernel\n 函数计算的蓬勃发展\n  什么是虚拟计算 hypervisor分类  Type-1 裸机  KVM, QEMU, VMWare, Virtualbox\n Type-2 托管  XenServer， Hyper-v， KVM, ESX, Xen\n虚拟机的问题  贵  一台云主机(8G/4Core/500MBps)在2017年的超过6000元/年\n 操作慢  一个普通的镜像在2G左右，再加上JAVA应用，一个镜像需要10G.\n启动，备份非常不便。\n容器计算 |虚拟机| 容器 |:\u0026mdash;|:\u0026mdash;|:\u0026mdash;| 构件| 完整的操作系统和应用| 微内核和应用 虚拟技术| 硬件虚拟化| 操作系统虚拟化\n容器计算带来什么好处  容器镜像小  alpine的容量是2M","tags":["ARCHITECTURE"],"title":"虚拟机 -\u003e 容器 -\u003e 函数计算","type":"post"},{"authors":null,"categories":[],"content":" AWS Step Functions VS Adds Support for Nested Workflows\n","date":1569396179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569396179,"objectID":"6ba3521d4c612a9916b380d7328b06ae","permalink":"https://wubigo.com/post/step-functions/","publishdate":"2019-09-25T15:22:59+08:00","relpermalink":"/post/step-functions/","section":"post","summary":"AWS Step Functions VS Adds Support for Nested Workflows","tags":[],"title":"Step Functions 常见问题","type":"post"},{"authors":null,"categories":[],"content":"https://news.ycombinator.com/item?id=20726906\nISP Starter Kit\nhttp://www.wispa.org/Resources/HOW-TO-START-A-WISP\nwireless fiber\n5G mobile broadband\nhttps://www.huawei.com/en/press-events/news/2019/1/huawei-releases-wireless-fiber-solution\nhttps://www.techdirt.com/articles/20190904/08392642916/colorado-town-offers-1-gbps-60-after-years-battling-comcast.shtml\n","date":1568597961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568597961,"objectID":"63df43e8426dce6be14bd570794ecbb2","permalink":"https://wubigo.com/post/start-your-own-isp/","publishdate":"2019-09-16T09:39:21+08:00","relpermalink":"/post/start-your-own-isp/","section":"post","summary":"https://news.ycombinator.com/item?id=20726906\nISP Starter Kit\nhttp://www.wispa.org/Resources/HOW-TO-START-A-WISP\nwireless fiber\n5G mobile broadband\nhttps://www.huawei.com/en/press-events/news/2019/1/huawei-releases-wireless-fiber-solution\nhttps://www.techdirt.com/articles/20190904/08392642916/colorado-town-offers-1-gbps-60-after-years-battling-comcast.shtml","tags":["ISP","BROADBAND","WISP","NETWORK"],"title":"宽带服务","type":"post"},{"authors":null,"categories":[],"content":" DOCKER DEAMON PROXY  systemd level  /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;HTTPS_PROXY=http://192.168.1.1:8080/\u0026quot;   service level  /etc/default/docker\nexport http_proxy=\u0026quot;http://127.0.0.1:3128/\u0026quot;   daemon level  /etc/docker/daemon\nDOCKER CLIENT PROXY ~/.docker/config.json\n \u0026quot;proxies\u0026quot;:{ \u0026quot;default\u0026quot;:{} }  ","date":1568329676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568329676,"objectID":"b847c0c713511dfb1c6a90cc8b40db80","permalink":"https://wubigo.com/post/docker-proxy/","publishdate":"2019-09-13T07:07:56+08:00","relpermalink":"/post/docker-proxy/","section":"post","summary":" DOCKER DEAMON PROXY  systemd level  /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;HTTPS_PROXY=http://192.168.1.1:8080/\u0026quot;   service level  /etc/default/docker\nexport http_proxy=\u0026quot;http://127.0.0.1:3128/\u0026quot;   daemon level  /etc/docker/daemon\nDOCKER CLIENT PROXY ~/.docker/config.json\n \u0026quot;proxies\u0026quot;:{ \u0026quot;default\u0026quot;:{} }  ","tags":["DOCKER"],"title":"Docker Proxy for daemon and client","type":"post"},{"authors":null,"categories":[],"content":" [TOC]\nAWS领先的设计理念和强大的技术生态\n使你身陷其中，学习你要用它，开发你要\n用它，上线还要用它。 一年下来项目还没有\n正式商用，已经有十几万的研发费用。\n今天向你推荐 localstack（与openstack啥关系？私有云+公有云），\n让你使用AWS免费，至少在项目POC或开发测试阶段免费。\n有了它， 你不用再焦急的等待老板审批公有云的\n计算，存储，数据库资源开发申请。\n是不是这个项目听起来很激动？\n那如何使用localstack呢？\n安装localstack localstack是一个非常活跃的正在快速成长中的项目，\n建议通过源代码安装\n 下载源代码  git clone git@github.com:localstack/localstack.git git fetch --all git checkout tags/v0.10.3 -b v0.10.3   启用需要使用的AWS服务  修改配置文件，启用你需要使用的AWS服务:ec2,s3,iot,kafka等。\n注意服务的名字必须来自服务名字列表， 否则不识别\n启用服务就是修改下边的配置文件\nlocalstack\\docker-compose.yml\n SERVICES=${SERVICES-ec2,ecs,stepfunctions,iam,lambda,dynamodb,apigateway,s3,sns} DATA_DIR=${DATA_DIR-/tmp/localstack/data } volumes: - \u0026quot;${TMPDIR:-d:/tmp/localstack}:/tmp/localstack\u0026quot;  make sure driver D is shared in docker desktop daemon\ndocker-compose up localstack_1 | Starting mock S3 (http port 4572)... localstack_1 | Starting mock SNS (http port 4575)... localstack_1 | Starting mock IAM (http port 4593)... localstack_1 | Starting mock API Gateway (http port 4567)... localstack_1 | Starting mock DynamoDB (http port 4569)... localstack_1 | Starting mock Lambda service (http port 4574)... localstack_1 | Starting mock CloudWatch Logs (http port 4586)... localstack_1 | Starting mock StepFunctions (http port 4585)...  系统消息显示需要的服务/端口已经启动。\n到目前为至，localstack已经安装完毕。\n记录并保存localstack的操作数据 if volumes in docker settings\nLocalstack is recording all API calls in JSON file.\nWhen the container restarts, it will re-apply these calls -\nthis is how we are able to keep our data between restarts\ndocker-compose.yml\n- DATA_DIR=/tmp/localstack/data  下边，我们来验证公有云服务是否可用。\n安装AWS客户端  安装到虚拟环境  (venv) d:\\code\\venv\u0026gt;pip install awscli  可以安装到系统环境\n 配置AWS CLI  (venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:  验证服务编排  试用stepfunctions服务  上面显示stepfunctions服务在4585端口，下面的端口要和\n配置保持一致\n(venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 list-activities   创建一个HelloWorld工作流   (venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 create-state-machine --definition \u0026quot;{\\\u0026quot;Comment\\\u0026quot;: \\\u0026quot;A Hello World example of the Amazon States Language using a Pass state\\\u0026quot;,\\\u0026quot;StartAt\\\u0026quot;: \\\u0026quot;HelloWorld\\\u0026quot;,\\\u0026quot;States\\\u0026quot;: {\\\u0026quot;HelloWorld\\\u0026quot;: {\\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Pass\\\u0026quot;,\\\u0026quot;End\\\u0026quot;: true}}}\u0026quot; --name \u0026quot;HelloWorld\u0026quot; --role-arn \u0026quot;arn:aws:iam::012345678901:role/DummyRole\u0026quot;   显示创建的工作流  (venv) d:\\code\\venv\u0026gt;aws stepfunctions --endpoint-url http://localhost:4585 list-state-machines { \u0026quot;stateMachines\u0026quot;: [ { \u0026quot;creationDate\u0026quot;: 1568199315.809, \u0026quot;stateMachineArn\u0026quot;: \u0026quot;arn:aws:states:us-east-1:123456789012:stateMachine:HelloWorld\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;HelloWorld\u0026quot; } ] }  工作流已经创建，你可以启动工作流，\n添加Lambda，部署微服务，添加微服务到到工作流，\n所有公有云的计算，存储，API调用，上行宽带费用\n通过使用localstack一切免费。\n赶快加入项目，贡献你的力量\nhttps://localstack.cloud/\n参考  https://hub.docker.com/r/amazon/aws-stepfunctions-local https://docs.aws.amazon.com/lambda/latest/dg/limits.html  ","date":1568245584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568245584,"objectID":"aeaaca1145fc29cef7b588fe20beb857","permalink":"https://wubigo.com/post/use-public-cloud-for-free/","publishdate":"2019-09-12T07:46:24+08:00","relpermalink":"/post/use-public-cloud-for-free/","section":"post","summary":"[TOC]\nAWS领先的设计理念和强大的技术生态\n使你身陷其中，学习你要用它，开发你要\n用它，上线还要用它。 一年下来项目还没有\n正式商用，已经有十几万的研发费用。\n今天向你推荐 localstack（与openstack啥关系？私有云+公有云），\n让你使用AWS免费，至少在项目POC或开发测试阶段免费。\n有了它， 你不用再焦急的等待老板审批公有云的\n计算，存储，数据库资源开发申请。\n是不是这个项目听起来很激动？\n那如何使用localstack呢？\n安装localstack localstack是一个非常活跃的正在快速成长中的项目，\n建议通过源代码安装\n 下载源代码  git clone git@github.com:localstack/localstack.git git fetch --all git checkout tags/v0.10.3 -b v0.10.3   启用需要使用的AWS服务  修改配置文件，启用你需要使用的AWS服务:ec2,s3,iot,kafka等。\n注意服务的名字必须来自服务名字列表， 否则不识别\n启用服务就是修改下边的配置文件\nlocalstack\\docker-compose.yml\n SERVICES=${SERVICES-ec2,ecs,stepfunctions,iam,lambda,dynamodb,apigateway,s3,sns} DATA_DIR=${DATA_DIR-/tmp/localstack/data } volumes: - \u0026quot;${TMPDIR:-d:/tmp/localstack}:/tmp/localstack\u0026quot;  make sure driver D is shared in docker desktop daemon\ndocker-compose up localstack_1 | Starting mock S3 (http port 4572)... localstack_1 | Starting mock SNS (http port 4575).","tags":["IAAS","CLOUD","LOCALSTACK","AWS"],"title":"免费使用公有云服务","type":"post"},{"authors":null,"categories":[],"content":"https://www.sdxcentral.com/networking/virtualization/definitions/how-does-micro-segmentation-help-security-explanation/\n","date":1558862784,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558862784,"objectID":"5b4d0919be97be90f51b0fd30369154e","permalink":"https://wubigo.com/post/secucity-micro-segmentation/","publishdate":"2019-05-26T17:26:24+08:00","relpermalink":"/post/secucity-micro-segmentation/","section":"post","summary":"https://www.sdxcentral.com/networking/virtualization/definitions/how-does-micro-segmentation-help-security-explanation/","tags":["SECURITY"],"title":"Secucity Micro Segmentation","type":"post"},{"authors":null,"categories":[],"content":" give the wireless network higher priority than the wired WIRELESS CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Uncheck it. That will enable a text box named \u0026ldquo;Interface metric\u0026rdquo;. Fill in a number. It needs to be larger than 1 (reserved for loopback) and the number(30) you choose for the wired network.\nWIRED CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Again Uncheck \u0026ldquo;Automatic metric\u0026rdquo;, and fill in a number in the \u0026ldquo;Interface metric\u0026rdquo; box. It needs to be larger than 1 but smaller than the number above (15).\n","date":1557478676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557478676,"objectID":"35fa5639b14ce7b07a0f9984c5843689","permalink":"https://wubigo.com/post/wireless-and-wired-connection-both-at-a-same-time-in-windows/","publishdate":"2019-05-10T16:57:56+08:00","relpermalink":"/post/wireless-and-wired-connection-both-at-a-same-time-in-windows/","section":"post","summary":"give the wireless network higher priority than the wired WIRELESS CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Uncheck it. That will enable a text box named \u0026ldquo;Interface metric\u0026rdquo;. Fill in a number. It needs to be larger than 1 (reserved for loopback) and the number(30) you choose for the wired network.\nWIRED CONNECTION \u0026gt; \u0026quot;Internet Protocol Version 4 (TCP/IPv4) Properties\u0026quot; \u0026gt; advanced TCP/IP setting \u0026gt; Automatic metric  Again Uncheck \u0026ldquo;Automatic metric\u0026rdquo;, and fill in a number in the \u0026ldquo;Interface metric\u0026rdquo; box.","tags":["NETWORK","WINDOWS"],"title":"Wireless and Wired Connection Both at a Same Time in Windows","type":"post"},{"authors":null,"categories":[],"content":"AppInfo 启动类型必须是自动或手动，\n否则，msinstaller， services.msc， regedit\n等都会报错：\nThe Service command cannot be started, either because it is disabled or because it has no enabled devices associated with it  AppInfo svchost.exe Facilitates the running of interactive applications with additional administrative privileges. Users will be unable to launch applications with the additional administrative privileges they may require to perform desired user tasks. These tools include regedit. Although safe to disable, this is not recommended since you need to boot into safe mode to enable again.  ","date":1557114714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557114714,"objectID":"e07f0c24af37e7520af65940e921a17e","permalink":"https://wubigo.com/post/windows-application-information-service/","publishdate":"2019-05-06T11:51:54+08:00","relpermalink":"/post/windows-application-information-service/","section":"post","summary":"AppInfo 启动类型必须是自动或手动，\n否则，msinstaller， services.msc， regedit\n等都会报错：\nThe Service command cannot be started, either because it is disabled or because it has no enabled devices associated with it  AppInfo svchost.exe Facilitates the running of interactive applications with additional administrative privileges. Users will be unable to launch applications with the additional administrative privileges they may require to perform desired user tasks. These tools include regedit. Although safe to disable, this is not recommended since you need to boot into safe mode to enable again.","tags":["WINDOWS"],"title":"Windows Application Information Service","type":"post"},{"authors":null,"categories":[],"content":" 微服务认证和授权有很多方案，\n这里比较各种主流方案的优缺点，\n为你的业务系统选择MAA方案提供指南\n   方案 优点 缺点     分布式会话管理 简单，成熟，服务器统一管理 扩展性比较差   客户令牌     单点登录     API网关令牌管理     第三方应用授权     SSL/TLS 双向认证      方案  分布式会话管理  会话信息由服务器存储\n实现方式：\n Sticky session Session replication Centralized session storage   客户令牌  令牌由客户持有\nJWT: 头，负载和签名\n 头  { \u0026quot;typ\u0026quot;: \u0026quot;JWT\u0026quot;, \u0026quot;alg\u0026quot;: \u0026quot;HS256\u0026quot; }   负载  { \u0026quot;id\u0026quot;: 123, \u0026quot;name\u0026quot;: \u0026quot;hi tico\u0026quot;, \u0026quot;is_admin\u0026quot;: true, \u0026quot;expire\u0026quot;: 1558213420 }   签名  HMACSHA256( base64UrlEncode(header) + \u0026quot;.\u0026quot; + base64UrlEncode(payload), secret )   单点登录\n API网关令牌管理\n   第三方应用授权   API 令牌  例如github的API个人令牌\n OAUTH  Someone may wonder why an Authorization Code is used to request Access Token, rather than returning the Access Token to the client directly from the authorization server. The reason why OAuth is designed in this way is to pass through the user agent (browser) during the process of redirecting to the client’s Callback URL. If the Access Token is passed directly, there is a risk of being stolen.\nBy using the authorization code, the client directly interacts with the authorization server when applying for the access token, and the authorization server also authorize the client when processing the client’s token request, so it’s prevent others from forging the client’s identity to use the authentication code\n SSL/TLS 双向认证  例如istio citadel私有证书中心\n[1]. https://medium.com/tech-tajawal/microservice-authentication-and-authorization-solutions-e0e5e74b248a\n","date":1556442487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556442487,"objectID":"29e3bf1dd3d55d889c347594eb902323","permalink":"https://wubigo.com/post/microservices-authentication-and-authorization/","publishdate":"2019-04-28T17:08:07+08:00","relpermalink":"/post/microservices-authentication-and-authorization/","section":"post","summary":"微服务认证和授权有很多方案，\n这里比较各种主流方案的优缺点，\n为你的业务系统选择MAA方案提供指南\n   方案 优点 缺点     分布式会话管理 简单，成熟，服务器统一管理 扩展性比较差   客户令牌     单点登录     API网关令牌管理     第三方应用授权     SSL/TLS 双向认证      方案  分布式会话管理  会话信息由服务器存储\n实现方式：\n Sticky session Session replication Centralized session storage   客户令牌  令牌由客户持有\nJWT: 头，负载和签名\n 头  { \u0026quot;typ\u0026quot;: \u0026quot;JWT\u0026quot;, \u0026quot;alg\u0026quot;: \u0026quot;HS256\u0026quot; }   负载  { \u0026quot;id\u0026quot;: 123, \u0026quot;name\u0026quot;: \u0026quot;hi tico\u0026quot;, \u0026quot;is_admin\u0026quot;: true, \u0026quot;expire\u0026quot;: 1558213420 }   签名  HMACSHA256( base64UrlEncode(header) + \u0026quot;.","tags":["MICROSERVICE","UAA"],"title":"微服务认证和授权（MAA）指南","type":"post"},{"authors":null,"categories":[],"content":"之前一直用pycharm,今天把code升级到1.3.2的时候， 突然提示我安装python扩展，决定试试。 结果发现python的解释器设置有问题， 总是设置为系统的解释器， 而虚拟环境的解释器不起作用。\napt remove --purge python3.5 reboot  结果ubuntu桌面启动不了。好多应用程序例如chrome，virtualbox都消失了， 造成了很大的麻烦。\nCtrl+Alt+F1进入虚拟控制台登录\napt install python3.5 apt install ubuntu-desktop  重新安装chrome和virtualbox\ncd /etc/apt/sources.list.d sudo mv google-chrome.list.save google-chrome.list apt update apt install google-chrome-stable  ","date":1553407449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553407449,"objectID":"85d87c64d29ba663f9fe082e68adbc96","permalink":"https://wubigo.com/post/linux-python3.5-remove/","publishdate":"2019-03-24T14:04:09+08:00","relpermalink":"/post/linux-python3.5-remove/","section":"post","summary":"之前一直用pycharm,今天把code升级到1.3.2的时候， 突然提示我安装python扩展，决定试试。 结果发现python的解释器设置有问题， 总是设置为系统的解释器， 而虚拟环境的解释器不起作用。\napt remove --purge python3.5 reboot  结果ubuntu桌面启动不了。好多应用程序例如chrome，virtualbox都消失了， 造成了很大的麻烦。\nCtrl+Alt+F1进入虚拟控制台登录\napt install python3.5 apt install ubuntu-desktop  重新安装chrome和virtualbox\ncd /etc/apt/sources.list.d sudo mv google-chrome.list.save google-chrome.list apt update apt install google-chrome-stable  ","tags":["LINUX","PYTHON"],"title":"Linux删除Python3.5","type":"post"},{"authors":null,"categories":[],"content":" 本地流线型开发 本地流线型开发\n集成开发，测试部署 IDE\n","date":1553385716,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553385716,"objectID":"192502c0143ab7c3e4de72aa84030803","permalink":"https://wubigo.com/post/k8s-app-development-toolbox/","publishdate":"2019-03-24T08:01:56+08:00","relpermalink":"/post/k8s-app-development-toolbox/","section":"post","summary":"本地流线型开发 本地流线型开发\n集成开发，测试部署 IDE","tags":["K8S","APP"],"title":"K8s高效应用开发工具集","type":"post"},{"authors":null,"categories":[],"content":" K8S POD Command Override OCR\ndocker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default OCR Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.61.1 (x86_64-alpine-linux-musl) libcurl/7.61.1 LibreSSL/2.0.0 zlib/1.2.11 libssh2/1.8.0 nghttp2/1.32.0 Release-Date: 2018-09-05  kubectl apply -f cmd-override-pod.yaml kubectl logs command-override Usage: curl [options...] \u0026lt;url\u0026gt; --abstract-unix-socket \u0026lt;path\u0026gt; Connect via abstract Unix domain socket --anyauth Pick any authentication method -a, --append Append to target file when uploading  ","date":1552891750,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552891750,"objectID":"e060b27024638440ab348f813d905de1","permalink":"https://wubigo.com/post/k8s-pod-command-override/","publishdate":"2019-03-18T14:49:10+08:00","relpermalink":"/post/k8s-pod-command-override/","section":"post","summary":"K8S POD Command Override OCR\ndocker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default OCR Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.","tags":["K8S"],"title":"K8s Pod Command Override","type":"post"},{"authors":null,"categories":[],"content":" Node-level Logging  System component logs      RUN IN CONTAINER(Y/N) Systemd(W/WO) LOGGER LOCATION     kubelet and container runtime  W/O /var/log   kubelet and container runtime  W journald   scheduler Y  /var/log   kube-proxy Y  /var/log    /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  Cluster-level logging  Use a node-level logging agent that runs on every node. Include a dedicated sidecar container for logging in an application pod. Push logs directly to a backend from within an application  具体实现\n EFK  ","date":1552789507,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552789507,"objectID":"62a4013c2742426ceb75ec69f930a7ea","permalink":"https://wubigo.com/post/k8s-logging/","publishdate":"2019-03-17T10:25:07+08:00","relpermalink":"/post/k8s-logging/","section":"post","summary":"Node-level Logging  System component logs      RUN IN CONTAINER(Y/N) Systemd(W/WO) LOGGER LOCATION     kubelet and container runtime  W/O /var/log   kubelet and container runtime  W journald   scheduler Y  /var/log   kube-proxy Y  /var/log    /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  Cluster-level logging  Use a node-level logging agent that runs on every node.","tags":["K8S"],"title":"K8s Logging","type":"post"},{"authors":null,"categories":[],"content":" git clone git@github.com:wubigo/kubernetes.git git remote add upstream https://github.com/kubernetes/kubernetes.git git fetch --all git checkout tags/v1.13.3 -b v1.13.3 git branch -av|grep 1.13 * fix-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2. remotes/origin/release-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2.  管理POD func (kl *Kubelet) syncPod(o syncPodOptions) error {  ","date":1551669200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551669200,"objectID":"c919bd285f8023e9d2964065904c018d","permalink":"https://wubigo.com/post/k8s-core-development/","publishdate":"2019-03-04T11:13:20+08:00","relpermalink":"/post/k8s-core-development/","section":"post","summary":" git clone git@github.com:wubigo/kubernetes.git git remote add upstream https://github.com/kubernetes/kubernetes.git git fetch --all git checkout tags/v1.13.3 -b v1.13.3 git branch -av|grep 1.13 * fix-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2. remotes/origin/release-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2.  管理POD func (kl *Kubelet) syncPod(o syncPodOptions) error {  ","tags":["K8S"],"title":"K8S CORE DEVELOPMENT","type":"post"},{"authors":null,"categories":[],"content":" 基于腾讯云Go SDK开发\n下载开发工具集 go get -u github.com/tencentcloud/tencentcloud-sdk-go  为集群准备CVM 从本地开发集群K8S读取安全凭证secretId和secretKey配置信息， 然后把安全凭证传送给SDK客户端\nsecretId, secretKey:= K8SClient.Secrets(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;cloud-pass\u0026quot;) credential := CloudCommon.NewCredential(\u0026quot;secretId\u0026quot;, \u0026quot;secretKey\u0026quot;) client, _ := cvm.NewClient(credential, regions.Beijing)  request := cvm.NewAllocateHostsRequest() request.FromJsonString(K8SClient.Configs(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;K8S-TENCENT-PROD\u0026quot;)) response, err := client.AllocateHosts(request)  通过ANSIBLE在CVM搭建K8S集群 Ansible.Hosts().Get(response.ToJsonString())  调用ANSIBLE开始在CVM部署K8S集群\n","date":1551615889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551615889,"objectID":"ae96d6074f37a8b4118582872daa3c52","permalink":"https://wubigo.com/post/dev-on-tencent-cloud-sdk-in-go/","publishdate":"2019-03-03T20:24:49+08:00","relpermalink":"/post/dev-on-tencent-cloud-sdk-in-go/","section":"post","summary":"基于腾讯云Go SDK开发\n下载开发工具集 go get -u github.com/tencentcloud/tencentcloud-sdk-go  为集群准备CVM 从本地开发集群K8S读取安全凭证secretId和secretKey配置信息， 然后把安全凭证传送给SDK客户端\nsecretId, secretKey:= K8SClient.Secrets(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;cloud-pass\u0026quot;) credential := CloudCommon.NewCredential(\u0026quot;secretId\u0026quot;, \u0026quot;secretKey\u0026quot;) client, _ := cvm.NewClient(credential, regions.Beijing)  request := cvm.NewAllocateHostsRequest() request.FromJsonString(K8SClient.Configs(\u0026quot;namespace=tencent\u0026quot;).Get(\u0026quot;K8S-TENCENT-PROD\u0026quot;)) response, err := client.AllocateHosts(request)  通过ANSIBLE在CVM搭建K8S集群 Ansible.Hosts().Get(response.ToJsonString())  调用ANSIBLE开始在CVM部署K8S集群","tags":["K8S","API","SDK","TENCENT"],"title":"通过SDK操纵公有云","type":"post"},{"authors":null,"categories":[],"content":"转录语音数据集\nmozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors\nhttps://blog.mozilla.org/blog/2019/02/28/sharing-our-common-voices-mozilla-releases-the-largest-to-date-public-domain-transcribed-voice-dataset/\n","date":1551385179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551385179,"objectID":"7890348034d7b9c1f7a05490f0731485","permalink":"https://wubigo.com/post/speech_recognition-transcribed-voice-dataset/","publishdate":"2019-03-01T04:19:39+08:00","relpermalink":"/post/speech_recognition-transcribed-voice-dataset/","section":"post","summary":"转录语音数据集\nmozilla crowdsources the largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors\nhttps://blog.mozilla.org/blog/2019/02/28/sharing-our-common-voices-mozilla-releases-the-largest-to-date-public-domain-transcribed-voice-dataset/","tags":["RNN","DEEPLEARNING","SPEECH_RECOGNITION"],"title":"mozilla crowdsources the largest dataset of human voices","type":"post"},{"authors":null,"categories":[],"content":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nK8S网络基础 K8S网络基础\nCNI插件 CNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","date":1550996323,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550996323,"objectID":"4dbacfc0727205a4c013414bf43243c1","permalink":"https://wubigo.com/post/k8s-cni/","publishdate":"2019-02-24T16:18:43+08:00","relpermalink":"/post/k8s-cni/","section":"post","summary":" 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。\n在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。\nCNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。\nPOD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。\n当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)\nK8S网络基础 K8S网络基础\nCNI插件 CNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。\n Flannel\n Kube-router\nKube-router\n OpenVSwitch\n Calico\nCalico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求\nCNI-Calico\n Weave Net\n 网桥\nCNI 网桥\n  ","tags":["K8S","CNI","NETWORK"],"title":"K8S CNI操作指引","type":"post"},{"authors":null,"categories":[],"content":" 前言 利用公有云或混合云的架构是企业IT应用落地实施的最高效方式。\n企业在上云的过程中，主要面临以下问题：\n 公有云开发环境搭建：引起这个问题的主要原因有两个\n 开发成本变化：固定成本变成线性成本 网络带宽：宽带问题成为开发中的大难题  遗留应用迁移：公有云迁移并没有一个固定的方案，必须结合遗留应用的 架构并结合公有云厂商的基础服务的特性采用最合理的有效方案。\n 应用架构的快速迭代：容器架构才刚刚落地， 函数计算又异军突起\n  内容 WEB应用架构演进：虚机，容器，函数计算  微服务架构软件交付流程   函数计算架构软件交付流程  函数计算之道 函数计算进行时\n公有云服务本地开发环境搭建 开发者可以免费使用公有云服务进行开发测试\n免费使用公有云服务\n函数计算本地开发环境搭建 搭建一个完全本地的函数计算开发环境\n函数计算的本地开发环境搭建有两种方式：\n 基于localstack+terraform+serverless   开发环境本地搭建参考\n 基于kubeless/knative+k8s   函数计算真实应用场景开发案例 函数计算应用案例\n函数计算云服务 ","date":1550911549,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550911549,"objectID":"f440d17ccc61ed1834d85ba75f354e42","permalink":"https://wubigo.com/project/noserver/","publishdate":"2019-02-23T16:45:49+08:00","relpermalink":"/project/noserver/","section":"project","summary":" 前言 利用公有云或混合云的架构是企业IT应用落地实施的最高效方式。\n企业在上云的过程中，主要面临以下问题：\n 公有云开发环境搭建：引起这个问题的主要原因有两个\n 开发成本变化：固定成本变成线性成本 网络带宽：宽带问题成为开发中的大难题  遗留应用迁移：公有云迁移并没有一个固定的方案，必须结合遗留应用的 架构并结合公有云厂商的基础服务的特性采用最合理的有效方案。\n 应用架构的快速迭代：容器架构才刚刚落地， 函数计算又异军突起\n  内容 WEB应用架构演进：虚机，容器，函数计算  微服务架构软件交付流程   函数计算架构软件交付流程  函数计算之道 函数计算进行时\n公有云服务本地开发环境搭建 开发者可以免费使用公有云服务进行开发测试\n免费使用公有云服务\n函数计算本地开发环境搭建 搭建一个完全本地的函数计算开发环境\n函数计算的本地开发环境搭建有两种方式：\n 基于localstack+terraform+serverless   开发环境本地搭建参考\n 基于kubeless/knative+k8s   函数计算真实应用场景开发案例 函数计算应用案例\n函数计算云服务 ","tags":[],"title":"函数计算","type":"project"},{"authors":null,"categories":["IT"],"content":" setup for gitlab tee .gitlab-ci.yml \u0026lt;\u0026lt; EOF image: monachus/hugo variables: GIT_SUBMODULE_STRATEGY: recursive pages: script: - hugo artifacts: paths: - public only: - master EOF git init echo \u0026quot;/public\u0026quot; \u0026gt;\u0026gt; .gitignore  post  hugo new post//index.md\n deploy  hugo publish the public to web server\n Configuration Lookup Order confit/_default/\n ./config.toml ./config.yaml ./config.json  confit/_default/config.toml\n hugo build destination  publishDir   Number of items per page in paginated lists  paginate = 20   taxonomies\n by tag by author   tag = \u0026quot;tags\u0026quot; author = \u0026quot;authors\u0026quot;  confit/_default/menus.toml\n Navigation Links widget enable/disabl under content/home/ folder Navigation Links widget display order  weight = 1  config/_default/languages.toml\n多语言显示\nlanguageCode = \u0026quot;en-us\u0026quot; languageCode = \u0026quot;zh-Hans\u0026quot;  Blog set content/post/_index.md\n post view as Card  view = 3  content/home/posts.md\n Number of recent posts to list.  count = 20  theme https://sourcethemes.com/academic/zh/docs/page-builder/#icons\n","date":1550806707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550806707,"objectID":"a81ccaae6503c87d521581d175a11183","permalink":"https://wubigo.com/post/blog_on_hugo/","publishdate":"2019-02-22T11:38:27+08:00","relpermalink":"/post/blog_on_hugo/","section":"post","summary":"Decide to gave hugo a shot after many years of being jekyll","tags":["BLOG"],"title":"Blog on hugo way","type":"post"},{"authors":null,"categories":[],"content":"https://git-lfs.github.com/\n","date":1549936588,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549936588,"objectID":"2a72f1b479ea4bb4d3d3037de25eb76c","permalink":"https://wubigo.com/post/git-extension-for-versioning-large-files/","publishdate":"2019-02-12T09:56:28+08:00","relpermalink":"/post/git-extension-for-versioning-large-files/","section":"post","summary":"https://git-lfs.github.com/","tags":["GIT"],"title":"git管理大文件","type":"post"},{"authors":null,"categories":["IT"],"content":" version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods  kubectl get pods --field-selector=status.phase=Pending   images list  kubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.go:389] Default route transits interface \u0026quot;enp0s3\u0026quot; I0217 07:28:13.308349 14495 interface.go:196] Interface enp0s3 is up I0217 07:28:13.309611 14495 interface.go:244] Interface \u0026quot;enp0s3\u0026quot; has 2 addresses :[192.168.1.9/24 fe80::a00:27ff:fe75:f493/64]. I0217 07:28:13.310328 14495 interface.go:211] Checking addr 192.168.1.9/24. I0217 07:28:13.311219 14495 interface.go:218] IP found 192.168.1.9 I0217 07:28:13.311961 14495 interface.go:250] Found valid IPv4 address 192.168.1.9 for interface \u0026quot;enp0s3\u0026quot;. I0217 07:28:13.312688 14495 interface.go:395] Found active IP 192.168.1.9 I0217 07:28:13.313427 14495 version.go:163] fetching Kubernetes version from URL: https://dl.K8S.io/release/stable-1.txt I0217 07:28:23.320683 14495 version.go:94] could not fetch a Kubernetes version from the internet: unable to get URL \u0026quot;https://dl.K8S.io/release/stable-1.txt\u0026quot;: Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled (Client.Timeout exceeded while awaiting headers) I0217 07:28:23.321520 14495 version.go:95] falling back to the local client version: v1.13.3 I0217 07:28:23.330622 14495 feature_gate.go:206] feature gates: \u0026amp;{map[]} K8S.gcr.io/kube-apiserver:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 K8S.gcr.io/pause:3.1 K8S.gcr.io/etcd:3.2.24 K8S.gcr.io/coredns:1.2.6   pull images beforehand  kubeadm config images pull -v 4  init phase kubeadm config print init-defaults \u0026gt;adm.defaults.yaml git diff adm.defaults.yaml -imageRepository: K8S.gcr.io +imageRepository: mirrorgooglecontainers sudo kubeadm init phase preflight --config=./adm.defaults.yaml -v 4  Self-hosting the Kubernetes control plane As of 1.8, you can experimentally create a self-hosted Kubernetes control plane. This means that key components such as the API server, controller manager, and scheduler run as DaemonSet pods configured via the Kubernetes API instead of static pods configured in the kubelet via static files. To create a self-hosted cluster, pass the flag \u0026ndash;feature-gates=SelfHosting=true to kubeadm init.\n  https://kubernetes.io/docs/setup/independent/setup-ha-etcd-with-kubeadm/ https://discuss.kubernetes.io/t/question-about-etcd-cluster-with-kubeadm-in-1-11/1228\nkubectl get configmaps --all-namespaces kubectl describe configmaps kubeadm-config -n kube-system kubectl -n kube-system get deployment coredns -o yaml | \\ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \\ kubectl apply -f - kubectl scale --current-replicas=2 --replicas=1 deployments.apps/nginx1-14 kubectl logs calico-node-4mb5z -n kube-system  ","date":1549856307,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549856307,"objectID":"3b9aedf05dacf02f125f31d5b45ac452","permalink":"https://wubigo.com/post/kubeamd-cheat-sheet/","publishdate":"2019-02-11T11:38:27+08:00","relpermalink":"/post/kubeamd-cheat-sheet/","section":"post","summary":"version notes\nsome only works on 1.13\nkubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;13\u0026quot;, GitVersion:\u0026quot;v1.13.3\u0026quot;, GitCommit:\u0026quot;721bfa751924da8d1680787490c54b9179b1fed0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2019-02-16T15:29:34Z\u0026quot;, GoVersion:\u0026quot;go1.11.5\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix\n get all Pending pods  kubectl get pods --field-selector=status.phase=Pending   images list  kubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.","tags":["K8S"],"title":"kubeamd cheat sheet","type":"post"},{"authors":null,"categories":null,"content":" track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently. In contrast, HTTP hosted on Transmission Control Protocol (TCP) can be blocked if any of the multiplexed data streams has an error.\n reduced connection and transport latency, and bandwidth estimation in each direction to avoid congestion. It also moves control of the congestion avoidance algorithms into the application space at both endpoints, rather than the kernel space, which it is claimed will allow these algorithms to improve more rapidly. Additionally, the protocol can be extended with forward error correction (FEC) to further improve performance when errors are expected\n  [1]. revive Gopherspace\n[2]. gopher client\n","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"ef107e3fcd63362281edd05426733f5a","permalink":"https://wubigo.com/post/2019-02-01-http3notes/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/post/2019-02-01-http3notes/","section":"post","summary":"track http redirection  http://wubigo.com/post -\u0026gt; http://wubigo.com/post/ -\u0026gt; https://wubigo.com/post/\ncurl -IL http://wubigo.com/post\n HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post Via: 1.1 varnish X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Content-Length: 0 HTTP/1.1 301 Moved Permanently Strict-Transport-Security: max-age=31556952 Location: http://wubigo.com/post/ Access-Control-Allow-Origin: * X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 301 Moved Permanently Location: https://wubigo.com/post/ X-Cache: HIT X-Cache-Hits: 1 HTTP/1.1 200 OK Access-Control-Allow-Origin: * Cache-Control: max-age=600 X-Cache: HIT X-Cache-Hits: 1  main goal  HTTP/2\u0026rsquo;s multiplexed connections, allowing multiple streams of data to reach all the endpoints independently.","tags":["HTTP","WEB"],"title":"HTTP/3","type":"post"},{"authors":null,"categories":[],"content":"https://medium.com/people-ai-engineering/building-a-data-lake-in-aws-9c1fb3876e23\nhttps://towardsdatascience.com/building-a-data-pipeline-from-scratch-on-aws-35f139420ebc\n","date":1545783637,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545783637,"objectID":"cd13d4456c2c6d7e5887404751af8f25","permalink":"https://wubigo.com/post/aws-s3-data-lake/","publishdate":"2018-12-26T08:20:37+08:00","relpermalink":"/post/aws-s3-data-lake/","section":"post","summary":"https://medium.com/people-ai-engineering/building-a-data-lake-in-aws-9c1fb3876e23\nhttps://towardsdatascience.com/building-a-data-pipeline-from-scratch-on-aws-35f139420ebc","tags":["AWS","S3","DATAlAKE"],"title":"Aws S3 Data Lake","type":"post"},{"authors":null,"categories":[],"content":" restore snapshot shell snapshot.sh\n!#/bin/bash wget https://nodejs.org/dist/v12.13.1/node-v12.13.1-linux-x64.tar.xz tar xvf node-v12.13.1-linux-x64.tar.xz export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH wget https://manning-content.s3.amazonaws.com/download/0/ddbbd36-251d-42ef-9934-55e5a881a336/FinalSourceCode.zip sudo apt update sudo apt install unzip unzip FinalSourceCode.zip mv Final\\ Source\\ Code/ sls sudo apt install python-pip pip install awscli which aws_completer cp ~/.bashrc ~/.bashrc_orig tee -a ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '/home/ubuntu/.local/bin/aws_completer' aws export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH EOF  aws configure npm install claudia -g claudia -v 5.11.0 cd chapter-03 npm install claudia create \\ --region ap-northeast-1 \\ --api-module api packaging files npm install -q --no-audit --production npm WARN pizza-api@1.0.0 No repository field. saving configuration { \u0026quot;lambda\u0026quot;: { \u0026quot;role\u0026quot;: \u0026quot;pizza-api-executor\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;pizza-api\u0026quot;, \u0026quot;region\u0026quot;: \u0026quot;ap-northeast-1\u0026quot; }, \u0026quot;api\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;4auh8wzh16\u0026quot;, \u0026quot;module\u0026quot;: \u0026quot;api\u0026quot;, \u0026quot;url\u0026quot;: \u0026quot;https://4auh8wzh16.execute-api.ap-northeast-1.amazonaws.com/latest\u0026quot; } }  IAM policy to that allows Lambda function to communicate with database  aws iam put-role-policy \\ \u0026gt; --role-name pizza-api-executor \\ \u0026gt; --policy-name PizzaApiDynamoDB \\ \u0026gt; --policy-document file://./roles/dynamodb.json  package.json\n\u0026quot;dependencies\u0026quot;: { \u0026quot;claudia-api-builder\u0026quot;: \u0026quot;^4.1.2\u0026quot; },  aws apigateway get-resources --rest-api-id \u0026quot;4auh8wzh16\u0026quot; { \u0026quot;path\u0026quot;: \u0026quot;/pizzas/{id}\u0026quot;, \u0026quot;resourceMethods\u0026quot;: { \u0026quot;OPTIONS\u0026quot;: {}, \u0026quot;GET\u0026quot;: {} }, \u0026quot;id\u0026quot;: \u0026quot;i9rknj\u0026quot;, \u0026quot;pathPart\u0026quot;: \u0026quot;{id}\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;el67kl\u0026quot; }  claudia update updating REST API apigateway.setAcceptHeader { \u0026quot;FunctionName\u0026quot;: \u0026quot;pizza-api\u0026quot;, \u0026quot;FunctionArn\u0026quot;: \u0026quot;arn:aws:lambda:ap-northeast-1:465691908928:function:pizza-api:2\u0026quot;, \u0026quot;Runtime\u0026quot;: \u0026quot;nodejs12.x\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;arn:aws:iam::465691908928:role/pizza-api-executor\u0026quot;, \u0026quot;Handler\u0026quot;: \u0026quot;api.proxyRouter\u0026quot;, \u0026quot;CodeSize\u0026quot;: 17910, \u0026quot;Description\u0026quot;: \u0026quot;A pizza API, an example app from \\\u0026quot;Serverless applications with Claudia.js\\\u0026quot;\u0026quot;, \u0026quot;Timeout\u0026quot;: 3, \u0026quot;MemorySize\u0026quot;: 128, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-12-16T02:01:14.766+0000\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;RDF1eXIMV2PKlTQZt9uUSayhdREMECZzTQaP92WWCqg=\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;KMSKeyArn\u0026quot;: null, \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;MasterArn\u0026quot;: null, \u0026quot;RevisionId\u0026quot;: \u0026quot;d0e645d2-86e5-44b2-b20c-f95526be2094\u0026quot;, \u0026quot;State\u0026quot;: \u0026quot;Active\u0026quot;, \u0026quot;StateReason\u0026quot;: null, \u0026quot;StateReasonCode\u0026quot;: null, \u0026quot;LastUpdateStatus\u0026quot;: \u0026quot;Successful\u0026quot;, \u0026quot;LastUpdateStatusReason\u0026quot;: null, \u0026quot;LastUpdateStatusReasonCode\u0026quot;: null, \u0026quot;url\u0026quot;: \u0026quot;https://4auh8wzh16.execute-api.ap-northeast-1.amazonaws.com/latest\u0026quot; }  ","date":1544885679,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544885679,"objectID":"5ffc2bf1744a2fb7e68c8113e32d364b","permalink":"https://wubigo.com/post/serverless-nodejs/","publishdate":"2018-12-15T22:54:39+08:00","relpermalink":"/post/serverless-nodejs/","section":"post","summary":"restore snapshot shell snapshot.sh\n!#/bin/bash wget https://nodejs.org/dist/v12.13.1/node-v12.13.1-linux-x64.tar.xz tar xvf node-v12.13.1-linux-x64.tar.xz export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH wget https://manning-content.s3.amazonaws.com/download/0/ddbbd36-251d-42ef-9934-55e5a881a336/FinalSourceCode.zip sudo apt update sudo apt install unzip unzip FinalSourceCode.zip mv Final\\ Source\\ Code/ sls sudo apt install python-pip pip install awscli which aws_completer cp ~/.bashrc ~/.bashrc_orig tee -a ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '/home/ubuntu/.local/bin/aws_completer' aws export PATH=/home/ubuntu/node-v12.13.1-linux-x64/bin:$PATH EOF  aws configure npm install claudia -g claudia -v 5.11.0 cd chapter-03 npm install claudia create \\ --region ap-northeast-1 \\ --api-module api packaging files npm install -q --no-audit --production npm WARN pizza-api@1.","tags":["AWS","SLS","CLOUD","NODE","SERVERLESS"],"title":"函数计算Nodejs实例","type":"post"},{"authors":null,"categories":[],"content":" Debugging the Build Process Gatsby’s build and develop steps run as a Node.js application\nwhich you can debug using standard tools for Node.js applications.\nDebugging with Node.js’ built-in console console.log(args)  VS Code Debugger (Auto-Config)  Preferences: Type node debug into the search bar. Make sure the Auto Attach option is set to on.\n launch.json\n  launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;node\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Gatsby develop\u0026quot;, \u0026quot;skipFiles\u0026quot;: [ \u0026quot;\u0026lt;node_internals\u0026gt;/**\u0026quot; ], \u0026quot;program\u0026quot;: \u0026quot;D:/Downloads/node-v12.13.1-win-x64/node_modules/gatsby/dist/bin/gatsby\u0026quot;, \u0026quot;args\u0026quot;: [\u0026quot;develop\u0026quot;], \u0026quot;stopOnEntry\u0026quot;: false, \u0026quot;runtimeArgs\u0026quot;: [\u0026quot;--nolazy\u0026quot;], \u0026quot;sourceMaps\u0026quot;: false } ] }  After putting a breakpoint in gatsby-node.js and\nusing the Start debugging command from VS Code you can see the final result\n","date":1544672714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544672714,"objectID":"825511b8380d188aff010bb8d4e2fd8e","permalink":"https://wubigo.com/post/gatsby-debug/","publishdate":"2018-12-13T11:45:14+08:00","relpermalink":"/post/gatsby-debug/","section":"post","summary":"Debugging the Build Process Gatsby’s build and develop steps run as a Node.js application\nwhich you can debug using standard tools for Node.js applications.\nDebugging with Node.js’ built-in console console.log(args)  VS Code Debugger (Auto-Config)  Preferences: Type node debug into the search bar. Make sure the Auto Attach option is set to on.\n launch.json\n  launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes.","tags":["NODEJS","NODE","REACT"],"title":"Gatsby Debug","type":"post"},{"authors":null,"categories":[],"content":" Client Side Rendering(CSR) Rendering an app in a browser, generally using the DOM\nThe initial HTML rendered by the server is a placeholder and\nthe entire user interface and data rendered in the browser\nonce all your scripts load.\nPROS  Rich site interactions Fast rendering after the initial load Partial real-time updates Cheaper to host \u0026amp; scale  CONS  SEO and index issues Mostly initial bundle.js load duration Performance issues on old mobile devices/slow networks Social Media crawlers and sharing problems (SMO)  Server Side Rendering(SSR) Server rendering generates the full HTML for a page\non the server in response to navigation\nPROS  Consistent SEO Performance, initial page load Works well with Social Media crawlers and platforms (SMO)  CONS  Frequent requests Slow page rendering (TTFB — Time to first byte) Complex architecture (For universal approach)  ","date":1544663666,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544663666,"objectID":"88e883cc53d6fb37f06200f4abf2c454","permalink":"https://wubigo.com/post/server-side-rendering/","publishdate":"2018-12-13T09:14:26+08:00","relpermalink":"/post/server-side-rendering/","section":"post","summary":"Client Side Rendering(CSR) Rendering an app in a browser, generally using the DOM\nThe initial HTML rendered by the server is a placeholder and\nthe entire user interface and data rendered in the browser\nonce all your scripts load.\nPROS  Rich site interactions Fast rendering after the initial load Partial real-time updates Cheaper to host \u0026amp; scale  CONS  SEO and index issues Mostly initial bundle.js load duration Performance issues on old mobile devices/slow networks Social Media crawlers and sharing problems (SMO)  Server Side Rendering(SSR) Server rendering generates the full HTML for a page","tags":[],"title":"Server Side Rendering","type":"post"},{"authors":null,"categories":[],"content":" JavaScript Arrow Functions https://zendev.com/2018/10/01/javascript-arrow-functions-how-why-when.html\n","date":1543896709,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543896709,"objectID":"f581293e8ce222e32aebe81a5e21c3ea","permalink":"https://wubigo.com/post/javascript-arrow-functions/","publishdate":"2018-12-04T12:11:49+08:00","relpermalink":"/post/javascript-arrow-functions/","section":"post","summary":"JavaScript Arrow Functions https://zendev.com/2018/10/01/javascript-arrow-functions-how-why-when.html","tags":["FUNCTION","FP","JS"],"title":"JavaScript Arrow Functions","type":"post"},{"authors":null,"categories":[],"content":" 微服务安全要点  通信链路加密 灵活的服务访问控制，包括细粒度访问策略 访问日志审计 服务提供方可替代性(batteries included)和可集成性  基本概念  安全标识  在K8S，安全标识(service account)代表一个用户，一个服务或一组服务。\n 安全命名  安全命名定义可运行服务的安全标识\n微服务认证  传输层认证 终端用户认证  每一个终端请求通过JWT(JSON Web Token)校验, 支持Auth0, Firebase。\nhttps://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838\n","date":1543622508,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622508,"objectID":"58d61e8fb20ba75d82e752e80e209eda","permalink":"https://wubigo.com/post/microservice-security-aaa/","publishdate":"2018-12-01T08:01:48+08:00","relpermalink":"/post/microservice-security-aaa/","section":"post","summary":"微服务安全要点  通信链路加密 灵活的服务访问控制，包括细粒度访问策略 访问日志审计 服务提供方可替代性(batteries included)和可集成性  基本概念  安全标识  在K8S，安全标识(service account)代表一个用户，一个服务或一组服务。\n 安全命名  安全命名定义可运行服务的安全标识\n微服务认证  传输层认证 终端用户认证  每一个终端请求通过JWT(JSON Web Token)校验, 支持Auth0, Firebase。\nhttps://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838","tags":["K8S","SECURITY"],"title":"微服务安全：认证，授权和审计(AAA)","type":"post"},{"authors":null,"categories":[],"content":" AWS leverages a standard JSON Identity and Access Management (IAM)\npolicy document format across many services to control authorization\nto resources and API actions\nterraform https://www.terraform.io/docs/providers/aws/r/iam_role_policy.html\nresource \u0026quot;aws_iam_role_policy\u0026quot; \u0026quot;s3_policy\u0026quot; { name = \u0026quot;s3_policy\u0026quot; role = \u0026quot;${aws_iam_role.lambda_s3_role.id}\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ListObjectsInBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [\u0026quot;s3:ListBucket\u0026quot;], \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name\u0026quot;] }, { \u0026quot;Sid\u0026quot;: \u0026quot;AllObjectActions\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name/*\u0026quot;] } ] } EOF } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_s3_role\u0026quot; { name = \u0026quot;lambda_s3_role\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot; } ] } EOF }  ","date":1543593247,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543593247,"objectID":"0acd972780f73573e42c0a6fbe945911","permalink":"https://wubigo.com/post/aws-iam-policy/","publishdate":"2018-11-30T23:54:07+08:00","relpermalink":"/post/aws-iam-policy/","section":"post","summary":"AWS leverages a standard JSON Identity and Access Management (IAM)\npolicy document format across many services to control authorization\nto resources and API actions\nterraform https://www.terraform.io/docs/providers/aws/r/iam_role_policy.html\nresource \u0026quot;aws_iam_role_policy\u0026quot; \u0026quot;s3_policy\u0026quot; { name = \u0026quot;s3_policy\u0026quot; role = \u0026quot;${aws_iam_role.lambda_s3_role.id}\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;ListObjectsInBucket\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [\u0026quot;s3:ListBucket\u0026quot;], \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name\u0026quot;] }, { \u0026quot;Sid\u0026quot;: \u0026quot;AllObjectActions\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: [\u0026quot;arn:aws:s3:::bucket-name/*\u0026quot;] } ] } EOF } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_s3_role\u0026quot; { name = \u0026quot;lambda_s3_role\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.","tags":["SERVERLESS","TERRAFORM","AWS","IAM"],"title":"Aws IAM Policy","type":"post"},{"authors":null,"categories":[],"content":" 运行环境 terraform -v Terraform v0.12.16 + provider.aws v2.39.0  创建函数 main.js\n'use strict' exports.handler = function(event, context, callback) { var response = { statusCode: 200, headers: { 'Content-Type': 'text/html; charset=utf-8' }, body: '\u0026lt;p\u0026gt;Hello world!\u0026lt;/p\u0026gt;' } callback(null, response) }  zip ../example.zip main.js  上传 awslocal s3api create-bucket --bucket=terraform-serverless-example awslocal s3 cp example.zip s3://terraform-serverless-example/v1.0.0/example.zip  创建资源 lambda.tf\nresource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;example\u0026quot; { function_name = \u0026quot;ServerlessExample\u0026quot; # The bucket name as created earlier with \u0026quot;aws s3api create-bucket\u0026quot; s3_bucket = \u0026quot;terraform-serverless-example\u0026quot; s3_key = \u0026quot;v1.0.0/example.zip\u0026quot; # \u0026quot;main\u0026quot; is the filename within the zip file (main.js) and \u0026quot;handler\u0026quot; # is the name of the property under which the handler function was # exported in that file. handler = \u0026quot;main.handler\u0026quot; runtime = \u0026quot;nodejs10.x\u0026quot; role = aws_iam_role.lambda_exec.arn } # IAM role which dictates what other AWS services the Lambda function # may access. resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;lambda_exec\u0026quot; { name = \u0026quot;serverless_example_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF {\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot; } ] } EOF }  api_gateway.tf\nresource \u0026quot;aws_api_gateway_rest_api\u0026quot; \u0026quot;example\u0026quot; { name = \u0026quot;ServerlessExample\u0026quot; description = \u0026quot;Terraform Serverless Application Example\u0026quot; } resource \u0026quot;aws_api_gateway_resource\u0026quot; \u0026quot;proxy\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id parent_id = aws_api_gateway_rest_api.example.root_resource_id path_part = \u0026quot;{proxy+}\u0026quot; } resource \u0026quot;aws_api_gateway_method\u0026quot; \u0026quot;proxy\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_resource.proxy.id http_method = \u0026quot;ANY\u0026quot; authorization = \u0026quot;NONE\u0026quot; } resource \u0026quot;aws_api_gateway_integration\u0026quot; \u0026quot;lambda\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_method.proxy.resource_id http_method = aws_api_gateway_method.proxy.http_method integration_http_method = \u0026quot;POST\u0026quot; type = \u0026quot;AWS_PROXY\u0026quot; uri = aws_lambda_function.example.invoke_arn } resource \u0026quot;aws_api_gateway_method\u0026quot; \u0026quot;proxy_root\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_rest_api.example.root_resource_id http_method = \u0026quot;ANY\u0026quot; authorization = \u0026quot;NONE\u0026quot; } resource \u0026quot;aws_api_gateway_integration\u0026quot; \u0026quot;lambda_root\u0026quot; { rest_api_id = aws_api_gateway_rest_api.example.id resource_id = aws_api_gateway_method.proxy_root.resource_id http_method = aws_api_gateway_method.proxy_root.http_method integration_http_method = \u0026quot;POST\u0026quot; type = \u0026quot;AWS_PROXY\u0026quot; uri = aws_lambda_function.example.invoke_arn } resource \u0026quot;aws_api_gateway_deployment\u0026quot; \u0026quot;example\u0026quot; { depends_on = [ aws_api_gateway_integration.lambda, aws_api_gateway_integration.lambda_root, ] rest_api_id = aws_api_gateway_rest_api.example.id stage_name = \u0026quot;test\u0026quot; }  get invoke_url  get api-id\nawslocal apigateway get-rest-apis  get stage\n   awslocal apigateway get-stages --rest-api-id  http://localhost:4567/restapis/\u0026lt;api-id\u0026gt;/\u0026lt;stage\u0026gt;/_user_request_/  ","date":1543419498,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543419498,"objectID":"c815a2817487074c09101689133994b5","permalink":"https://wubigo.com/post/lambda-apigateway/","publishdate":"2018-11-28T23:38:18+08:00","relpermalink":"/post/lambda-apigateway/","section":"post","summary":"运行环境 terraform -v Terraform v0.12.16 + provider.aws v2.39.0  创建函数 main.js\n'use strict' exports.handler = function(event, context, callback) { var response = { statusCode: 200, headers: { 'Content-Type': 'text/html; charset=utf-8' }, body: '\u0026lt;p\u0026gt;Hello world!\u0026lt;/p\u0026gt;' } callback(null, response) }  zip ../example.zip main.js  上传 awslocal s3api create-bucket --bucket=terraform-serverless-example awslocal s3 cp example.zip s3://terraform-serverless-example/v1.0.0/example.zip  创建资源 lambda.tf\nresource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;example\u0026quot; { function_name = \u0026quot;ServerlessExample\u0026quot; # The bucket name as created earlier with \u0026quot;aws s3api create-bucket\u0026quot; s3_bucket = \u0026quot;terraform-serverless-example\u0026quot; s3_key = \u0026quot;v1.","tags":["LOCALSTACK","SERVERLESS","TERRAFORM"],"title":"函数＋网关","type":"post"},{"authors":null,"categories":[],"content":" serveless backend Lambda allows to trigger execution of code\nin response to events in AWS, enabling\nserverless backend solutions.\nThe Lambda Function itself includes source code\nand runtime configuration.\n","date":1543023984,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543023984,"objectID":"6c4bb9f0f612ca3a46a8982937de682b","permalink":"https://wubigo.com/post/aws-lambda-notes/","publishdate":"2018-11-24T09:46:24+08:00","relpermalink":"/post/aws-lambda-notes/","section":"post","summary":"serveless backend Lambda allows to trigger execution of code\nin response to events in AWS, enabling\nserverless backend solutions.\nThe Lambda Function itself includes source code\nand runtime configuration.","tags":["SERVERLESS","LOCALSTACK"],"title":"Aws Lambda Notes","type":"post"},{"authors":null,"categories":null,"content":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"80247d4c6c90ebe4d510bb399f0bb85b","permalink":"https://wubigo.com/post/2018-11-24-microk8s/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-24-microk8s/","section":"post","summary":"Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd\n$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don\u0026rsquo;t include kube-system\n$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf","tags":["K8S"],"title":"MicroK8S","type":"post"},{"authors":null,"categories":null,"content":" generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git\nReferences https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"24686d6a0d054aaf9d5cb425834973cf","permalink":"https://wubigo.com/post/2018-11-28-jupyternotebook/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/post/2018-11-28-jupyternotebook/","section":"post","summary":"generate configuration file $jupyter notebook --generate-config Writing default config to: /home/bigo/.jupyter/jupyter_notebook_config.py $ diff jupyter_notebook_config.py jupyter_notebook_config.py.bak c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False  set or reset password $jupyter notebook password Enter password: Verify password: [NotebookPasswordApp] Wrote hashed password to /home/bigo/.jupyter/jupyter_notebook_config.json  then restart notebook server\nSharing notebooks When people talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git","tags":["PYTHON"],"title":"entry into jupyter notebook","type":"post"},{"authors":null,"categories":[],"content":" 函数计算概论 函数计算就是事件驱动架构(EDA），目前函数计算支持的事件类型列表\n函数计算事件列表\n计费模式  请求数\n 执行时间\n 内存分配\n  优劣势分析  真正做到谁开发谁运维(who code it who run it)\n 不需要提前做计算容量规划，服务器配置，负责均衡，扩容\n  代表性产品  DB: Aurora   ","date":1542849452,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542849452,"objectID":"76abd545f9bf167624aa1f53f4138c7d","permalink":"https://wubigo.com/post/computing-in-function-way/","publishdate":"2018-11-22T09:17:32+08:00","relpermalink":"/post/computing-in-function-way/","section":"post","summary":" 函数计算概论 函数计算就是事件驱动架构(EDA），目前函数计算支持的事件类型列表\n函数计算事件列表\n计费模式  请求数\n 执行时间\n 内存分配\n  优劣势分析  真正做到谁开发谁运维(who code it who run it)\n 不需要提前做计算容量规划，服务器配置，负责均衡，扩容\n  代表性产品  DB: Aurora   ","tags":["SERVERLESS"],"title":"函数计算之道","type":"post"},{"authors":null,"categories":null,"content":" The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features. Moreover, kubenet has many limitations. For instance, when running kubenet in AWS Cloud, you are limited to 50 EC2 instances. Route tables are used to configure network traffic between Kubernetes nodes, and are limited to 50 entries per VPC. Moreover, a cluster cannot be set up in a Private VPC, since that network topology uses multiple route tables. Other more advanced features, such as BGP, egress control, and mesh networking, are only available with different CNI providers.\nCNI in kops At last count, kops supports seven different CNI providers besides kubenet. Choosing from seven different network providers is a daunting task.\nHere is our current list of providers that can be installed out of the box, sorted in alphabetical order.\nCalico Canal (Flannel + Calico) flannel kopeio-vxlan kube-router romana Weave Net Any of these CNI providers can be used without kops. All of the CNI providers use a daemonset installation model, where their product deploys a Kubernetes Daemonset. Just use kubectl to install the provider on the master once the K8S API server has started. Please refer to each projects specific documentation\nSupport Matrix a table of different features from each of the CNI providers mentioned:\n| Provider | Network Model| Route Distribution|Network Policies|Mesh | |External Datastore|Encryption|Ingress/Egress Policies| Commercial Support| | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | :\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;: |:\u0026mdash;\u0026mdash;\u0026ndash; | :\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-: |:\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; : | | Calico | Layer 3 | Yes |Yes | Etcd | Yes | Yes | | flannel | Layer 2 vxlan| Mo |No | None | No | No | | Weave | Layer 2 vxlan| N/A |Yes | No | Yes | Yes |\n Calico and Canal include a feature to connect directly to Kubernetes, and not use Etcd. Weave Net can operate in AWS-VPC mode without vxlan, but is limited to 50 nodes in EC2. Weave Net does not have egress rules out of the box.  Table Details\nNetwork Model The Network Model with providers is either encapsulated networking such as VXLAN, or unencapsulated layer 2 networking. Encapsulating network traffic requires compute to process, so theoretically is slower. In my opinion, most use cases will not be impacted by the overhead. More about VXLAN on wikipedia.\nRoute Distribution For layer 3 CNI providers, route distribution is necssary. Route distribution is typically via BGP. Route distribution is nice to have a feature with CNI, if you plan to build clusters split across network segments. It is an exterior gateway protocol designed to exchange routing and reachability information on the internet. BGP can assist with pod to pod networking between clusters.\nNetwork Policies A kubernetes.io blog post about network policies in 1.8 here.\nKubernetes now offers functionality to enforce rules about which pods can communicate with each other using network policies. This feature is has become stable Kubernetes 1.7 and is ready to use with supported networking plugins. The Kubernetes 1.8 release has added better capabilities to this feature.  Mesh Networking This feature allows for “Pod to Pod” networking between Kubernetes clusters. This technology is not Kubernetes federation, but is pure networking between Pods.\nEncyption Encrypting the network control plane, so all TCP and UDP traffic is encrypted.\nIngress / Egress Policies The network policies are both Kubernetes and Non-Kubernetes routing control. For instance, many providers will allow an administrator to block a pod communicating with an EC2 instance meta and data service on 169.254.169.254.\nSummary If you do not need the advanced features that a CNI provider delivers, use kubenet. It is stable, and fast. Otherwise, pick one. If you do need run more than 50 nodes on AWS, or need other advanced features, make a decision quickly (don’t spend days deciding), and test with your cluster. File bugs, and develop a relationship with your network provider. At this point in time, networking is not boring in Kubernetes. It is getting more boring every day! Monitor test and monitor more.\nhttps://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/\n","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542844800,"objectID":"a61dafcd727687faf6d4e6ce3be57298","permalink":"https://wubigo.com/post/2018-11-22-cninetworkproviderforkubernetes/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/post/2018-11-22-cninetworkproviderforkubernetes/","section":"post","summary":"The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.\nWhy Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features.","tags":["K8S","CNI","DOCKER"],"title":"Choosing a CNI Network Provider for Kubernetes","type":"post"},{"authors":null,"categories":[],"content":"https://serverless.com/framework/docs/providers/aws/events/\n","date":1542686207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542686207,"objectID":"5806eb52408bce9bb91e9968ecfa267b","permalink":"https://wubigo.com/post/serverless-supports-events-type-on-aws/","publishdate":"2018-11-20T11:56:47+08:00","relpermalink":"/post/serverless-supports-events-type-on-aws/","section":"post","summary":"https://serverless.com/framework/docs/providers/aws/events/","tags":["SERVERLESS"],"title":"Serverless Supports Events Type on Aws","type":"post"},{"authors":null,"categories":[],"content":" get s3 object creation notification  create queue  awslocal s3 mb s3://localstack awslocal sqs create-queue --queue-name localstack   get queue arn   awslocal sqs get-queue-attributes --queue-url http://localhost:4576/queue/localstack --attribute-names All { \u0026quot;Attributes\u0026quot;: { \u0026quot;ApproximateNumberOfMessagesNotVisible\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ApproximateNumberOfMessagesDelayed\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CreatedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;ApproximateNumberOfMessages\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;ReceiveMessageWaitTimeSeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DelaySeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;VisibilityTimeout\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;LastModifiedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:us-east-1:000000000000:localstack\u0026quot; } }   create s3 notification config  cat notification.json { \u0026quot;QueueConfigurations\u0026quot;: [ { \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:local:000000000000:localstack\u0026quot;, \u0026quot;Events\u0026quot;: [ \u0026quot;s3:ObjectCreated:*\u0026quot; ] } ] }   make notification effect  awslocal s3api put-bucket-notification-configuration --bucket localstack --notification-configuration file://notification.json   upload object to s3  awslocal s3 cp notification.json s3://localstack   get notification  awslocal sqs receive-message --queue-url http://localhost:4576/queue/localstack  ","date":1542617173,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542617173,"objectID":"2c374d9db83e62a59668e6a8550bcc20","permalink":"https://wubigo.com/post/aws-get-sqs-notification-on-s3-object-creation/","publishdate":"2018-11-19T16:46:13+08:00","relpermalink":"/post/aws-get-sqs-notification-on-s3-object-creation/","section":"post","summary":"get s3 object creation notification  create queue  awslocal s3 mb s3://localstack awslocal sqs create-queue --queue-name localstack   get queue arn   awslocal sqs get-queue-attributes --queue-url http://localhost:4576/queue/localstack --attribute-names All { \u0026quot;Attributes\u0026quot;: { \u0026quot;ApproximateNumberOfMessagesNotVisible\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;ApproximateNumberOfMessagesDelayed\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;CreatedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;ApproximateNumberOfMessages\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;ReceiveMessageWaitTimeSeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;DelaySeconds\u0026quot;: \u0026quot;0\u0026quot;, \u0026quot;VisibilityTimeout\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;LastModifiedTimestamp\u0026quot;: \u0026quot;1574152022\u0026quot;, \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:us-east-1:000000000000:localstack\u0026quot; } }   create s3 notification config  cat notification.json { \u0026quot;QueueConfigurations\u0026quot;: [ { \u0026quot;QueueArn\u0026quot;: \u0026quot;arn:aws:sqs:local:000000000000:localstack\u0026quot;, \u0026quot;Events\u0026quot;: [ \u0026quot;s3:ObjectCreated:*\u0026quot; ] } ] }   make notification effect  awslocal s3api put-bucket-notification-configuration --bucket localstack --notification-configuration file://notification.","tags":["LOCALSTACK","SERVERLESS"],"title":"基于local stack的本地S3对象创建通知","type":"post"},{"authors":null,"categories":[],"content":"AWS Lambda By default, all native logs within a Lambda function are stored in the function execution result within Lambda. Additionally, if you would like to review log information immediately after executing a function, invoking the Lambda function with the LogType parameter will retrieve the last 4KB of log data generated by the function. This information is returned in the x-amz-log-results header in the HTTP response.\nWhile these methods are great ways to test and debug issues associated with individual function calls, they do not do much by way of analysis or alerting. Thankfully, the log data that is stored in the Lambda function result is also stored in CloudWatch, Amazon’s log aggregation service. To access the CloudWatch logs for a given function, you will need to know the log group and log stream names, which can be retrieved by adding them to the function call logs and retrieving them in the x-amz-log-results response as mentioned above. As an example, this context can be retrieved and logged in Node.js like so:\nconsole.log(‘logGroupName =’, context.log_group_name); console.log(‘logStreamName =’, context.log_stream_name);\n","date":1542355225,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542355225,"objectID":"6ea29161ba48f748a151f825cf70624a","permalink":"https://wubigo.com/post/aws-log/","publishdate":"2018-11-16T16:00:25+08:00","relpermalink":"/post/aws-log/","section":"post","summary":"AWS Lambda By default, all native logs within a Lambda function are stored in the function execution result within Lambda. Additionally, if you would like to review log information immediately after executing a function, invoking the Lambda function with the LogType parameter will retrieve the last 4KB of log data generated by the function. This information is returned in the x-amz-log-results header in the HTTP response.\nWhile these methods are great ways to test and debug issues associated with individual function calls, they do not do much by way of analysis or alerting.","tags":["LOCALSTACK","SERVERLESS"],"title":"Aws Log","type":"post"},{"authors":null,"categories":[],"content":"serverless install -u https://github.com/serverless/examples/tree/master/aws-node-upload-to-s3-and-postprocess -n aws-node-upload-to-s3-and-postprocess sls deploy -s local awslocal logs describe-log-groups { \u0026quot;logGroups\u0026quot;: [ { \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:1:log-group:/aws/lambda/uload-local-postprocess\u0026quot;, \u0026quot;creationTime\u0026quot;: 1573867924377.624, \u0026quot;metricFilterCount\u0026quot;: 0, \u0026quot;logGroupName\u0026quot;: \u0026quot;/aws/lambda/upload-local-postprocess\u0026quot;, \u0026quot;storedBytes\u0026quot;: 0 } ] } awslocal logs describe-log-streams --log-group-name /aws/lambda/uload-local-postprocess { \u0026quot;logStreams\u0026quot;: [] }  serverless install -u https://github.com/serverless/examples/tree/master/aws-node-s3-file-replicator -n aws-node-s3-file-replicator sls deploy -s local awslocal s3api get-bucket-notification-configuration --bucket bbbb awslocal s3api get-bucket-acl --bucket output-bucket-12345  lambda_function.py\nimport json def my_handler(event, context): print(\u0026quot;Received event: \u0026quot; + json.dumps(event, indent=2)) message = 'Hello {} {}!'.format(event['first_name'], event['last_name']) return { 'message': message }  ","date":1542324319,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542324319,"objectID":"6b4f0b7b3c33dadcc0173d9750ef0976","permalink":"https://wubigo.com/post/aws-serverless-localstack-examples/","publishdate":"2018-11-16T07:25:19+08:00","relpermalink":"/post/aws-serverless-localstack-examples/","section":"post","summary":"serverless install -u https://github.com/serverless/examples/tree/master/aws-node-upload-to-s3-and-postprocess -n aws-node-upload-to-s3-and-postprocess sls deploy -s local awslocal logs describe-log-groups { \u0026quot;logGroups\u0026quot;: [ { \u0026quot;arn\u0026quot;: \u0026quot;arn:aws:logs:us-east-1:1:log-group:/aws/lambda/uload-local-postprocess\u0026quot;, \u0026quot;creationTime\u0026quot;: 1573867924377.624, \u0026quot;metricFilterCount\u0026quot;: 0, \u0026quot;logGroupName\u0026quot;: \u0026quot;/aws/lambda/upload-local-postprocess\u0026quot;, \u0026quot;storedBytes\u0026quot;: 0 } ] } awslocal logs describe-log-streams --log-group-name /aws/lambda/uload-local-postprocess { \u0026quot;logStreams\u0026quot;: [] }  serverless install -u https://github.com/serverless/examples/tree/master/aws-node-s3-file-replicator -n aws-node-s3-file-replicator sls deploy -s local awslocal s3api get-bucket-notification-configuration --bucket bbbb awslocal s3api get-bucket-acl --bucket output-bucket-12345  lambda_function.py\nimport json def my_handler(event, context): print(\u0026quot;Received event: \u0026quot; + json.","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的函数计算开发应用列表","type":"post"},{"authors":null,"categories":[],"content":" install nodejs install serverless npm install -g serverless npm install serverless-localstack   check serverless version  serverless -v Framework Core: 1.57.0 Plugin: 3.2.3 SDK: 2.2.1 Components Core: 1.1.2 Components CLI: 1.4.0  create serverless function  serverless create --template aws-nodejs --path my-service cd my-service  serverless.yml\nfunctions: hello: handler: handler.hello events: - http: path: ping method: get  plugins: - serverless-localstack custom: localstack: debug: true stages: - local - dev host: http://localhost endpoints: S3: http://localhost:4572 DynamoDB: http://localhost:4570 CloudFormation: http://localhost:4581 Elasticsearch: http://localhost:4571 ES: http://localhost:4578 SNS: http://localhost:4575 SQS: http://localhost:4576 Lambda: http://localhost:4574 Kinesis: http://localhost:4568 APIGateway: http://localhost:4567 CloudWatch: http://localhost:4582 CloudWatchLogs: http://localhost:4586 CloudWatchEvents: http://localhost:4587  deploy redeploy if all Functions, Events or Resources\nin serverless.yml changed\nserverless deploy --verbose --stage local Serverless: Using serverless-localstack Serverless: Reconfiguring service apigateway to use http://localhost:4567 Serverless: Reconfiguring service cloudformation to use http://localhost:4581 Serverless: Reconfiguring service cloudwatch to use http://localhost:4582 Serverless: Reconfiguring service lambda to use http://localhost:4574 Serverless: Reconfiguring service dynamodb to use http://localhost:4569 Serverless: Reconfiguring service kinesis to use http://localhost:4568 Serverless: Reconfiguring service route53 to use http://localhost:4580 Serverless: Reconfiguring service firehose to use http://localhost:4573 Serverless: Reconfiguring service stepfunctions to use http://localhost:4585 Serverless: Reconfiguring service es to use http://localhost:4578 Serverless: Reconfiguring service s3 to use http://localhost:4572 Serverless: Reconfiguring service ses to use http://localhost:4579 Serverless: Reconfiguring service sns to use http://localhost:4575 Serverless: Reconfiguring service sqs to use http://localhost:4576 Serverless: Reconfiguring service sts to use http://localhost:4592 Serverless: Reconfiguring service iam to use http://localhost:4593 Serverless: Reconfiguring service ssm to use http://localhost:4583 Serverless: Reconfiguring service rds to use http://localhost:4594 Serverless: Reconfiguring service ec2 to use http://localhost:4597 Serverless: Reconfiguring service elasticache to use http://localhost:4598 Serverless: Reconfiguring service kms to use http://localhost:4599 Serverless: Reconfiguring service secretsmanager to use http://localhost:4584 Serverless: Reconfiguring service logs to use http://localhost:4586 Serverless: Reconfiguring service cloudwatchlogs to use http://localhost:4586 Serverless: Reconfiguring service iot to use http://localhost:4589 Serverless: Reconfiguring service cognito-idp to use http://localhost:4590 Serverless: Reconfiguring service cognito-identity to use http://localhost:4591 Serverless: Reconfiguring service ecs to use http://localhost:4601 Serverless: Reconfiguring service eks to use http://localhost:4602 Serverless: Reconfiguring service xray to use http://localhost:4603 Serverless: Reconfiguring service appsync to use http://localhost:4605 Serverless: Reconfiguring service cloudfront to use http://localhost:4606 Serverless: Reconfiguring service athena to use http://localhost:4607 Serverless: Reconfiguring service S3 to use http://localhost:4572 Serverless: Reconfiguring service DynamoDB to use http://localhost:4570 Serverless: Reconfiguring service CloudFormation to use http://localhost:4581 Serverless: Reconfiguring service Elasticsearch to use http://localhost:4571 Serverless: Reconfiguring service ES to use http://localhost:4578 Serverless: Reconfiguring service SNS to use http://localhost:4575 Serverless: Reconfiguring service SQS to use http://localhost:4576 Serverless: Reconfiguring service Lambda to use http://localhost:4574 Serverless: Reconfiguring service Kinesis to use http://localhost:4568 Serverless: config.options_stage: local Serverless: serverless.service.custom.stage: undefined Serverless: serverless.service.provider.stage: dev Serverless: config.stage: local Serverless: Packaging service... Serverless: Excluding development dependencies... Serverless: Creating Stack... Serverless: Checking Stack create progress... CloudFormation - CREATE_COMPLETE - AWS::CloudFormation::Stack - my-service-local Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service my-service.zip file to S3 (389 B)... Serverless: Validating template... Serverless: Skipping template validation: Unsupported in Localstack Serverless: Updating Stack... Serverless: Checking Stack update progress... CloudFormation - UPDATE_COMPLETE - AWS::CloudFormation::Stack - my-service-local Serverless: Stack update finished... Service Information service: my-service stage: local region: us-east-1 stack: my-service-local resources: 11 api keys: None endpoints: GET - http://localhost:4567/restapis/gnz8rtc0xd/local/_user_request_/ping functions: hello: my-service-local-hello layers: None Stack Outputs ServerlessDeploymentBucketName: my-service-local-ServerlessDeploymentBucket-01EGLIMU6EYB HelloLambdaFunctionQualifiedArn: HelloLambdaVersionssJzcvFmzKtAcczGFSyyYtxtSzfXFRBYUf4ZEfoXes ServiceEndpoint: https://gnz8rtc0xd.execute-api.us-east-1.amazonaws.com/local  debug sls SLS_DEBUG=* sls deploy -s local  test function  with serverless  serverless invoke local -f hello -l   with curl  curl http://localhost:4567/restapis/gnz8rtc0xd/local/_user_request_/ping  deploy function to localstack serverless deploy function -f hello -v --stage local  serverless invoke local -f hello -l  serverless-localstack issue  service name has to be less than 5 character(the deployment bucket name gets truncated at 63 characters)  By default, Serverless creates a bucket with a generated name like\n--serverlessdeploymentbuck-1x6jug5lzfnl7\nto store your service\u0026rsquo;s stack state\nhttps://github.com/MikeSouza/serverless-deployment-bucket\nhttps://github.com/localstack/serverless-localstack/issues/30\ncheck the buckets after deployment\nawslocal s3 ls 2006-02-03 08:45:09 my-service-local-ServerlessDeploymentBucket-JBV56BIWWO4T 2006-02-03 08:45:09 simple-point-local-ServerlessDeploymentBucket-NI6J3QAYAJ55  ","date":1542267577,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542267577,"objectID":"9bdac2020300ea8848f8e9b362ec8dcd","permalink":"https://wubigo.com/post/aws-localstack-serverless/","publishdate":"2018-11-15T15:39:37+08:00","relpermalink":"/post/aws-localstack-serverless/","section":"post","summary":"install nodejs install serverless npm install -g serverless npm install serverless-localstack   check serverless version  serverless -v Framework Core: 1.57.0 Plugin: 3.2.3 SDK: 2.2.1 Components Core: 1.1.2 Components CLI: 1.4.0  create serverless function  serverless create --template aws-nodejs --path my-service cd my-service  serverless.yml\nfunctions: hello: handler: handler.hello events: - http: path: ping method: get  plugins: - serverless-localstack custom: localstack: debug: true stages: - local - dev host: http://localhost endpoints: S3: http://localhost:4572 DynamoDB: http://localhost:4570 CloudFormation: http://localhost:4581 Elasticsearch: http://localhost:4571 ES: http://localhost:4578 SNS: http://localhost:4575 SQS: http://localhost:4576 Lambda: http://localhost:4574 Kinesis: http://localhost:4568 APIGateway: http://localhost:4567 CloudWatch: http://localhost:4582 CloudWatchLogs: http://localhost:4586 CloudWatchEvents: http://localhost:4587  deploy redeploy if all Functions, Events or Resources","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的Serverless框架本地集成","type":"post"},{"authors":null,"categories":null,"content":" Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n  $bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.log 2\u0026gt;\u0026amp;1 $bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg $pip install /tmp/tensorflow_pkg/tensorflow-\u0026lt;blah\u0026gt;.whl $python -c 'import tensorflow as tf; print(tf.__version__)' $pip list | grep tensorflow  network-performance-monitoring https://github.com/tensorflow/tensorflow/issues/23402\n","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"e37caefe6bf0fd93fa4d32a82a6041bf","permalink":"https://wubigo.com/post/2018-11-08-buildtensorflow-1-12/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/post/2018-11-08-buildtensorflow-1-12/","section":"post","summary":"Note:\n Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs Have to build 1.6 or higher from source to run on older CPU\n Bazel 0.19.0 doesn\u0026rsquo;t read tools/bazel.rc anymore WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files: tensorflow-1.12.0/tools/bazel.rc\n  $bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\u0026quot;-D_GLIBCXX_USE_CXX11_ABI=0\u0026quot; --sandbox_debug \u0026gt; build.","tags":["DEEPLEARNING","TENSORFLOW"],"title":"build tensorflow 1.12","type":"post"},{"authors":null,"categories":null,"content":" putting /tmp on tmpfs https://blog.ubuntu.com/2016/01/20/data-driven-analysis-tmp-on-tmpfs\nInterrupt Coalescence ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate: on RX: on TX: on  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user\nsudo systemctl disable/stop irqbalance  network-performance-monitoring https://opensourceforu.com/2016/10/network-performance-monitoring/\nLinux Network (TCP) Performance Tuning with Sysctl https://www.slashroot.in/linux-network-tcp-performance-tuning-sysctl\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"dbee25a2eefe542b5e9c76a69361b6bc","permalink":"https://wubigo.com/post/2018-11-07-linuxperformancetuning/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/post/2018-11-07-linuxperformancetuning/","section":"post","summary":"putting /tmp on tmpfs https://blog.ubuntu.com/2016/01/20/data-driven-analysis-tmp-on-tmpfs\nInterrupt Coalescence ubuntu 16 default\nInterrupt Coalescence (IC)\n$ethtool -c enp0s25 Coalesce parameters for enp0s25: Adaptive RX: off TX: off  Pause frames\n$ethtool -a enp0s25 Pause parameters for enp0s25: Autonegotiate: on RX: on TX: on  network Tuning the network adapter (NIC)\nuse Jumbo frames\nifconfig eth0 mtu 9000  ip result for a healthy system with no packet drops\nip -s link show eth0  stop irqbalance for home user","tags":["LINUX"],"title":"Linux performance","type":"post"},{"authors":null,"categories":[],"content":"Improve docker container detection and resource configuration usage\nhttps://blog.softwaremill.com/docker-support-in-new-java-8-finally-fd595df0ca54\nhttps://www.oracle.com/technetwork/java/javase/8u191-relnotes-5032181.html\n","date":1541381628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541381628,"objectID":"118d36b693ba2f8145a1444efe7899ef","permalink":"https://wubigo.com/post/dockering-java-8/","publishdate":"2018-11-05T09:33:48+08:00","relpermalink":"/post/dockering-java-8/","section":"post","summary":"Improve docker container detection and resource configuration usage\nhttps://blog.softwaremill.com/docker-support-in-new-java-8-finally-fd595df0ca54\nhttps://www.oracle.com/technetwork/java/javase/8u191-relnotes-5032181.html","tags":["DOCKER","JAVA"],"title":"Dockering Java 8","type":"post"},{"authors":null,"categories":[],"content":" awslocal lambda add-permission --function-name ServerlessExample --action lambda:InvokeFunction --statement-id sns-topic --principal apigateway.amazonaws.com --source-arn \u0026quot;arn:aws:execute-api:us-east-1:123456789012:pmte6kdjb6/*/*\u0026quot;  ","date":1540536597,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540536597,"objectID":"98eb1333bdda3a1aad957784a3d16990","permalink":"https://wubigo.com/post/aws-lambda-apigateway/","publishdate":"2018-10-26T14:49:57+08:00","relpermalink":"/post/aws-lambda-apigateway/","section":"post","summary":" awslocal lambda add-permission --function-name ServerlessExample --action lambda:InvokeFunction --statement-id sns-topic --principal apigateway.amazonaws.com --source-arn \u0026quot;arn:aws:execute-api:us-east-1:123456789012:pmte6kdjb6/*/*\u0026quot;  ","tags":[],"title":"Aws Lambda Apigateway","type":"post"},{"authors":null,"categories":[],"content":" Status-Line The first line of a Response message is the Status-Line, consisting of the protocol version followed by a numeric status code and its associated textual phrase, with each element separated by SP characters. No CR or LF is allowed except in the final CRLF sequence.\n Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase CRLF  status code vs status in body https://www.codetinkerer.com/2015/12/04/choosing-an-http-status-code.html\nhttps://httpstatuses.com/\nThe main choice is do you want to treat the HTTP status code as part of your REST API or not.\nBoth ways work fine. I agree that, strictly speaking, one of the ideas of REST is that you should use the HTTP Status code as a part of your API (return 200 or 201 for a successful operation and a 4xx or 5xx depending on various error cases.) However, there are no REST police. You can do what you want. I have seen far more egregious non-REST APIs being called \u0026ldquo;RESTful.\u0026rdquo;\nAt this point (August, 2015) I do recommend that you use the HTTP Status code as part of your API. It is now much easier to see the return code when using frameworks than it was in the past. In particular, it is now easier to see the non-200 return case and the body of non-200 responses than it was in the past.\nThe HTTP Status code is part of your api\nYou will need to carefully pick 4xx codes that fit your error conditions. You can include a rest, xml, or plaintext message as the payload that includes a sub-code and a descriptive comment.\nThe clients will need to use a software framework that enables them to get at the HTTP-level status code. Usually do-able, not always straight-forward.\nThe clients will have to distinguish between HTTP status codes that indicate a communications error and your own status codes that indicate an application-level issue.\nThe HTTP Status code is NOT part of your api\nThe HTTP status code will always be 200 if your app received the request and then responded (both success and error cases)\nALL of your responses should include \u0026ldquo;envelope\u0026rdquo; or \u0026ldquo;header\u0026rdquo; information. Typically something like:\nenvelope_ver: 1.0 status: # use any codes you like. Reserve a code for success. msg: \u0026quot;ok\u0026quot; # A human string that reflects the code. Useful for debugging. data: ... # The data of the response, if any.  This method can be easier for clients since the status for the response is always in the same place (no sub-codes needed), no limits on the codes, no need to fetch the HTTP-level status-code.\nhttps://cloud.google.com/blog/products/api-management/restful-api-design-what-about-errors\n","date":1540523654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540523654,"objectID":"861371b511efeaff945652b8e1154ce0","permalink":"https://wubigo.com/post/http-status-code-for-reset-api/","publishdate":"2018-10-26T11:14:14+08:00","relpermalink":"/post/http-status-code-for-reset-api/","section":"post","summary":"Status-Line The first line of a Response message is the Status-Line, consisting of the protocol version followed by a numeric status code and its associated textual phrase, with each element separated by SP characters. No CR or LF is allowed except in the final CRLF sequence.\n Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase CRLF  status code vs status in body https://www.codetinkerer.com/2015/12/04/choosing-an-http-status-code.html\nhttps://httpstatuses.com/\nThe main choice is do you want to treat the HTTP status code as part of your REST API or not.","tags":["HTTP","REST"],"title":"REST API中如何使用Http状态码","type":"post"},{"authors":null,"categories":[],"content":"根据多年在IAAS/PAAS平台的建议经验，并帮助多个行业例如医疗，电信， 建筑行业客户向互联网IAAS/PAAS平台的迁移，在这里做一些直观的分享， 希望能为中小企业上云提供一些有价值的建议。\n本帖中部分建议只是针对没有自己数据中心的中小企业，如果已经拥有自己的数据中心， 需要根据目前的计算能力选择相应的云服务厂商\n云服务模型\n从云服务模型可以看出，企业上云有以下路径可选。 但导向是一致的，专注业务，IT外包。\n 从自建数据中心向IaaS平台迁移 从IaaS平台向PaaS迁移 在公有云厂家间迁移 从IaaS平台向FaaS迁移 混合云  上云的初级阶段: 购买云主机， 购买云主机的性价比指标和物理主机的指标完全不同， 如何选择最高性价比的云主机，参考\n云主机主要性价比指标\n硬盘转速的指标对云主机不再适用，云主机的性价比指标主要有以下几个：\n 云主机内网/外网平均带宽(最核心指标) 存储(网络硬盘)IO吞吐量 初始化完成后首次资源利用率  其中内网平均带宽是最核心的指标，不仅关乎到MONEY的问题，而且对你的应用架构 起决定性的影响，原来的应用架构在云主机模式将会长期处于网络IO阻塞状态。造成 这种问题的根本原因不是你原来的架构不好，而是公有云厂商的网络性能太差，在2016 年的时候，阿里的平均带宽还不到100M每秒。AWS大概在200M每秒。\n网络带宽是很多企业战略转型决定上云遇到的第一个大坑\n 内网平均带宽也是衡量私有云厂商服务能力的核心指标\n 所以公有云厂商又开始向你兜售混合云概念。混合云不符合公有云厂商的核心利益\n国内外云厂商主要产品基准测试对比图\n上云注意事项\n上云的中级阶段：PaaS\n如何选择PAAS供应商\n上云的终级阶段: 不需要服务器。\n如何选择FAAS供应商\n","date":1540136089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540136089,"objectID":"8e7811d692fc4ea6c448dfb65f9b571d","permalink":"https://wubigo.com/post/pass-over-iaas/","publishdate":"2018-10-21T23:34:49+08:00","relpermalink":"/post/pass-over-iaas/","section":"post","summary":"根据多年在IAAS/PAAS平台的建议经验，并帮助多个行业例如医疗，电信， 建筑行业客户向互联网IAAS/PAAS平台的迁移，在这里做一些直观的分享， 希望能为中小企业上云提供一些有价值的建议。\n本帖中部分建议只是针对没有自己数据中心的中小企业，如果已经拥有自己的数据中心， 需要根据目前的计算能力选择相应的云服务厂商\n云服务模型\n从云服务模型可以看出，企业上云有以下路径可选。 但导向是一致的，专注业务，IT外包。\n 从自建数据中心向IaaS平台迁移 从IaaS平台向PaaS迁移 在公有云厂家间迁移 从IaaS平台向FaaS迁移 混合云  上云的初级阶段: 购买云主机， 购买云主机的性价比指标和物理主机的指标完全不同， 如何选择最高性价比的云主机，参考\n云主机主要性价比指标\n硬盘转速的指标对云主机不再适用，云主机的性价比指标主要有以下几个：\n 云主机内网/外网平均带宽(最核心指标) 存储(网络硬盘)IO吞吐量 初始化完成后首次资源利用率  其中内网平均带宽是最核心的指标，不仅关乎到MONEY的问题，而且对你的应用架构 起决定性的影响，原来的应用架构在云主机模式将会长期处于网络IO阻塞状态。造成 这种问题的根本原因不是你原来的架构不好，而是公有云厂商的网络性能太差，在2016 年的时候，阿里的平均带宽还不到100M每秒。AWS大概在200M每秒。\n网络带宽是很多企业战略转型决定上云遇到的第一个大坑\n 内网平均带宽也是衡量私有云厂商服务能力的核心指标\n 所以公有云厂商又开始向你兜售混合云概念。混合云不符合公有云厂商的核心利益\n国内外云厂商主要产品基准测试对比图\n上云注意事项\n上云的中级阶段：PaaS\n如何选择PAAS供应商\n上云的终级阶段: 不需要服务器。\n如何选择FAAS供应商","tags":["PAAS"],"title":"面向应用的无服务器架构","type":"post"},{"authors":null,"categories":[],"content":"https://github.com/wubigo/localstack-examples\n","date":1540012926,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540012926,"objectID":"d85e8d88869409eda04f55a44953a3e2","permalink":"https://wubigo.com/post/aws-s3-upload-with-localstack/","publishdate":"2018-10-20T13:22:06+08:00","relpermalink":"/post/aws-s3-upload-with-localstack/","section":"post","summary":"https://github.com/wubigo/localstack-examples","tags":["LOCALSTACK","SERVERLESS"],"title":"Aws S3 Upload With Localstack","type":"post"},{"authors":null,"categories":[],"content":"Listen Notes(以下简称LN)是一个iPod试听资料搜索引擎和数据库，\n它使用的技术是非常过时的，没有AI，没有深度学习，\n没有区块链。“如果有谁一定要说我在使用AI，那他一定没有\n使用真正的AI\u0026rdquo;。\n通过阅读本帖，你能完全复制一个Listen Notes和其他相似的网站。\n你不需要雇佣很多工程师。是否还记得，当脸书以5700万美元收购\nInstagram的时候，Instagram总共只有13名员工，包括非工程技术\n人员。Instagram是在2012初成立的，现在是2019，云计算技术更加\n成熟，站在巨人的肩膀上，一个规模较小的工程团队更有可能做出一\n些有意义的产品，即使是像我这样的OPC(一个人独立成立的公司)。\n我发现本帖已经在HN和reddit上被广泛分享，对此，我在这里做些\n澄清：\n 本帖并不是最新的。LN使用的技术栈在不停的演进，经过过去两年的\n全职的开发，技术开始变得有点复杂了。LN在2017年初启动的时候\n只用了3台VPS.这里所说的“过时”是指我只使用了我熟悉的技术来快速\n的开发产品，聚焦到业务侧\n 现在我每天只花20%的时间在工程技术上，其他的时间都在进行业务沟通，\n回复邮件，每天不断总结思考\n 如果因为没有使用你推荐的技术，或者没有回复你提出的问题，而冒犯了\n你的话，请你原谅。我不能做到让每个人都开心满意\n 本帖只是告诉你做互联网产品的一种方法，但并不是唯一方法。而且它可能\n不是最好的方法。它通过使用数据手段帮你了解技术世界。\n  https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b\n","date":1539861014,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539861014,"objectID":"29526ac5ecbf924b51abdb7e45458dc7","permalink":"https://wubigo.com/post/the-boring-technology-behind-a-one-person-internet-company/","publishdate":"2018-10-18T19:10:14+08:00","relpermalink":"/post/the-boring-technology-behind-a-one-person-internet-company/","section":"post","summary":"Listen Notes(以下简称LN)是一个iPod试听资料搜索引擎和数据库，\n它使用的技术是非常过时的，没有AI，没有深度学习，\n没有区块链。“如果有谁一定要说我在使用AI，那他一定没有\n使用真正的AI\u0026rdquo;。\n通过阅读本帖，你能完全复制一个Listen Notes和其他相似的网站。\n你不需要雇佣很多工程师。是否还记得，当脸书以5700万美元收购\nInstagram的时候，Instagram总共只有13名员工，包括非工程技术\n人员。Instagram是在2012初成立的，现在是2019，云计算技术更加\n成熟，站在巨人的肩膀上，一个规模较小的工程团队更有可能做出一\n些有意义的产品，即使是像我这样的OPC(一个人独立成立的公司)。\n我发现本帖已经在HN和reddit上被广泛分享，对此，我在这里做些\n澄清：\n 本帖并不是最新的。LN使用的技术栈在不停的演进，经过过去两年的\n全职的开发，技术开始变得有点复杂了。LN在2017年初启动的时候\n只用了3台VPS.这里所说的“过时”是指我只使用了我熟悉的技术来快速\n的开发产品，聚焦到业务侧\n 现在我每天只花20%的时间在工程技术上，其他的时间都在进行业务沟通，\n回复邮件，每天不断总结思考\n 如果因为没有使用你推荐的技术，或者没有回复你提出的问题，而冒犯了\n你的话，请你原谅。我不能做到让每个人都开心满意\n 本帖只是告诉你做互联网产品的一种方法，但并不是唯一方法。而且它可能\n不是最好的方法。它通过使用数据手段帮你了解技术世界。\n  https://broadcast.listennotes.com/the-boring-technology-behind-listen-notes-56697c2e347b","tags":["STARTUP"],"title":"一个OPC互联网公司使用的老技术","type":"post"},{"authors":null,"categories":[],"content":" Create function index.js\nexports.handler = async function(event, context) { console.log(\u0026quot;ENVIRONMENT VARIABLES\\n\u0026quot; + JSON.stringify(process.env, null, 2)) console.log(\u0026quot;EVENT\\n\u0026quot; + JSON.stringify(event, null, 2)) return context.logStreamName }   打包  zip function.zip index.js  aws lambda create-function --function-name my-function --zip-file fileb://function.zip --handler index.handler --runtime nodejs10.x --role arn:aws:iam::123456789012:role/lambda-cli-role --endpoint-url=http://localhost:4574   aws lambda get-function --function-name my-function --endpoint-url=http://localhost:4574 { \u0026quot;Code\u0026quot;: { \u0026quot;Location\u0026quot;: \u0026quot;http://localhost:4574/2015-03-31/functions/my-function/code\u0026quot; }, \u0026quot;Configuration\u0026quot;: { \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;Version\u0026quot;: \u0026quot;$LATEST\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;3d149vplmMjIEgZuPhQgnFJ+tndL4I9D11GL1qdgT6M=\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;my-function\u0026quot;, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T01:16:43.752+0000\u0026quot;, \u0026quot;RevisionId\u0026quot;: \u0026quot;c79398c9-556b-4ed1-ad72-91332dd1f6e0\u0026quot;, \u0026quot;CodeSize\u0026quot;: 322, \u0026quot;FunctionArn\u0026quot;: \u0026quot;arn:aws:lambda:us-east-1:000000000000:function:my-function\u0026quot;, \u0026quot;Handler\u0026quot;: \u0026quot;index.handler\u0026quot;, \u0026quot;Role\u0026quot;: \u0026quot;arn:aws:iam::123456789012:role/lambda-cli-role\u0026quot;, \u0026quot;Timeout\u0026quot;: 3, \u0026quot;Runtime\u0026quot;: \u0026quot;nodejs10.x\u0026quot;, \u0026quot;Description\u0026quot;: \u0026quot;\u0026quot; } }   验证调用  (venv) $aws lambda list-functions --endpoint-url=http://localhost:4574 (venv) $aws lambda invoke --function-name my-function out --log-type Tail --endpoint-url=http://localhost:4574   清理  aws lambda delete-function --function-name my-function --endpoint-url=http://localhost:4574  ","date":1538185168,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538185168,"objectID":"d45ba8b26ff8f465bbcd411242ef7ba6","permalink":"https://wubigo.com/post/aws-lambda-with-localstack/","publishdate":"2018-09-29T09:39:28+08:00","relpermalink":"/post/aws-lambda-with-localstack/","section":"post","summary":"Create function index.js\nexports.handler = async function(event, context) { console.log(\u0026quot;ENVIRONMENT VARIABLES\\n\u0026quot; + JSON.stringify(process.env, null, 2)) console.log(\u0026quot;EVENT\\n\u0026quot; + JSON.stringify(event, null, 2)) return context.logStreamName }   打包  zip function.zip index.js  aws lambda create-function --function-name my-function --zip-file fileb://function.zip --handler index.handler --runtime nodejs10.x --role arn:aws:iam::123456789012:role/lambda-cli-role --endpoint-url=http://localhost:4574   aws lambda get-function --function-name my-function --endpoint-url=http://localhost:4574 { \u0026quot;Code\u0026quot;: { \u0026quot;Location\u0026quot;: \u0026quot;http://localhost:4574/2015-03-31/functions/my-function/code\u0026quot; }, \u0026quot;Configuration\u0026quot;: { \u0026quot;TracingConfig\u0026quot;: { \u0026quot;Mode\u0026quot;: \u0026quot;PassThrough\u0026quot; }, \u0026quot;Version\u0026quot;: \u0026quot;$LATEST\u0026quot;, \u0026quot;CodeSha256\u0026quot;: \u0026quot;3d149vplmMjIEgZuPhQgnFJ+tndL4I9D11GL1qdgT6M=\u0026quot;, \u0026quot;FunctionName\u0026quot;: \u0026quot;my-function\u0026quot;, \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T01:16:43.","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的本地Lambda开发","type":"post"},{"authors":null,"categories":[],"content":" 在windows，启动卷必须线启用共享驱动\n启用共享驱动 1: Open \u0026quot;Settings\u0026quot; in Docker Desktop -\u0026gt; \u0026quot;Shared Drives\u0026quot; -\u0026gt; \u0026quot;Reset Credentials\u0026quot; -\u0026gt; select drive \u0026quot;D\u0026quot; -\u0026gt; \u0026quot;Apply\u0026quot;  检查测试卷 docker run --rm -v d:/tmp:/data alpine ls /data  ","date":1537953252,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537953252,"objectID":"9baf22530856a63af95966a655adb969","permalink":"https://wubigo.com/post/docker-win10-volume/","publishdate":"2018-09-26T17:14:12+08:00","relpermalink":"/post/docker-win10-volume/","section":"post","summary":" 在windows，启动卷必须线启用共享驱动\n启用共享驱动 1: Open \u0026quot;Settings\u0026quot; in Docker Desktop -\u0026gt; \u0026quot;Shared Drives\u0026quot; -\u0026gt; \u0026quot;Reset Credentials\u0026quot; -\u0026gt; select drive \u0026quot;D\u0026quot; -\u0026gt; \u0026quot;Apply\u0026quot;  检查测试卷 docker run --rm -v d:/tmp:/data alpine ls /data  ","tags":["DOCKER","WIN"],"title":"WIN用户使用Docker卷","type":"post"},{"authors":null,"categories":[],"content":" 安装AWS CLI  (venv) d:\\code\\venv\u0026gt;pip install awscli pip install awscli-local  awslocal = aws \u0026ndash;endpoint-url=http://localhost:\n可以安装到系统环境\n 配置AWS CLI  (venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:  命令行自动完成\n$which aws_completer ~/code/venv/bin/aws_completer   tee ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '~/code/venv/bin/aws_completer' aws EOF   安装AWS SAM CLI  (venv) d:\\code\u0026gt;pip install aws-sam-cli (venv) d:\\code\u0026gt;sam --version SAM CLI, version 0.22.0   启动S3  (venv) d:\\code\u0026gt;localstack\\docker-compose up   创建bucket  (venv) d:\\code\u0026gt;aws configure get region local (venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 mb s3://demo-bucket  upload a file to bucket\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 cp java0.log s3://demo-bucket (venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3 ls s3://demo-bucket  Attach an ACL to the bucket so it is readable:\naws --endpoint-url=http://localhost:4572 s3api put-bucket-acl --bucket demo-bucket --acl public-read   list object acl   aws --endpoint-url=http://localhost:4572 s3api get-object-acl --bucket demo-bucket --key java0.log { \u0026quot;Owner\u0026quot;: { \u0026quot;DisplayName\u0026quot;: \u0026quot;webfile\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Grants\u0026quot;: [ { \u0026quot;Grantee\u0026quot;: { \u0026quot;Type\u0026quot;: \u0026quot;CanonicalUser\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Permission\u0026quot;: \u0026quot;FULL_CONTROL\u0026quot; } ] }   set object url and can be downloaded by public  aws --endpoint-url=http://localhost:4572 s3api put-object-acl --bucket demo-bucket --key java0.log --acl public-read aws --endpoint-url=http://localhost:4572 s3 presign s3://demo-bucket/java0.log http://localhost:4572/demo-bucket/java0.log  display the names of all S3 buckets (across all regions)\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 s3api list-buckets --query \u0026quot;Buckets[].Name\u0026quot; [ \u0026quot;demo-bucket\u0026quot; ] aws --endpoint-url=http://localhost:4572 s3api list-objects --bucket demo-bucket { \u0026quot;Contents\u0026quot;: [ { \u0026quot;LastModified\u0026quot;: \u0026quot;2019-09-29T10:17:02.386Z\u0026quot;, \u0026quot;ETag\u0026quot;: \u0026quot;\\\u0026quot;d41d8cd98f00b204e9800998ecf8427e\\\u0026quot;\u0026quot;, \u0026quot;StorageClass\u0026quot;: \u0026quot;STANDARD\u0026quot;, \u0026quot;Key\u0026quot;: \u0026quot;java0.log\u0026quot;, \u0026quot;Owner\u0026quot;: { \u0026quot;DisplayName\u0026quot;: \u0026quot;webfile\u0026quot;, \u0026quot;ID\u0026quot;: \u0026quot;75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a\u0026quot; }, \u0026quot;Size\u0026quot;: 0 } ] }  or specified region\n(venv) d:\\code\u0026gt;aws --endpoint-url=http://localhost:4572 --region local s3api list-buckets --query \u0026quot;Buckets[].Name\u0026quot; [ \u0026quot;demo-bucket\u0026quot; ]   下载样例程序  (venv) [code]$sam init --runtime python2.7 [+] Initializing project structure... Project generated: ./sam-app Steps you can take next within the project folder =================================================== [*] Invoke Function: sam local invoke HelloWorldFunction --event event.json [*] Start API Gateway locally: sam local start-api Read sam-app/README.md for further instructions   本地调用  echo '{\u0026quot;message\u0026quot;: \u0026quot;Hey, are you there?\u0026quot; }' | sam local invoke \u0026quot;HelloWorldFunction\u0026quot;   编译  (venv) [sam-app]$ cd sam-app \u0026amp;\u0026amp; sam build Build Succeeded Built Artifacts : .aws-sam/build Built Template : .aws-sam/build/template.yaml Commands you can use next ========================= [*] Invoke Function: sam local invoke [*] Package: sam package --s3-bucket \u0026lt;yourbucket\u0026gt;   启动本地API网关  venv) [sam-app]$ sam local start-api 2019-09-27 10:18:10 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit)  $curl http://127.0.0.1:3000/hello {\u0026quot;message\u0026quot;: \u0026quot;hello world\u0026quot;}   启动lambda服务  (venv) [sam-app]$ sam local start-lambda   运行函数计算服务  aws --endpoint-url=http://localhost:4585 stepfunctions list-state-machines --region local { \u0026quot;activities\u0026quot;: [] }  aws stepfunctions --endpoint http://localhost:4585 create-state-machine --definition \u0026quot;{\\ \\\u0026quot;Comment\\\u0026quot;: \\\u0026quot;A Hello World example of the Amazon States Language using an AWS Lambda Local function\\\u0026quot;,\\ \\\u0026quot;StartAt\\\u0026quot;: \\\u0026quot;HelloWorld\\\u0026quot;,\\ \\\u0026quot;States\\\u0026quot;: {\\ \\\u0026quot;HelloWorld\\\u0026quot;: {\\ \\\u0026quot;Type\\\u0026quot;: \\\u0026quot;Task\\\u0026quot;,\\ \\\u0026quot;Resource\\\u0026quot;: \\\u0026quot;arn:aws:lambda:us-east-1:123456789012:function:HelloWorldFunction\\\u0026quot;,\\ \\\u0026quot;End\\\u0026quot;: true\\ }\\ }\\ }\\ }}\u0026quot; --name \u0026quot;HelloWorld\u0026quot; --role-arn \u0026quot;arn:aws:iam::012345678901:role/DummyRole\u0026quot; --region local  aws \u0026ndash;endpoint-url=http://localhost:4585 \u0026ndash;lambda-endpoint http://localhost:3001\n","date":1537944234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537944234,"objectID":"5349d6e9ec9f6d5233ae068c6375bb5f","permalink":"https://wubigo.com/post/aws-step-function-with-local-lambda/","publishdate":"2018-09-26T14:43:54+08:00","relpermalink":"/post/aws-step-function-with-local-lambda/","section":"post","summary":"安装AWS CLI  (venv) d:\\code\\venv\u0026gt;pip install awscli pip install awscli-local  awslocal = aws \u0026ndash;endpoint-url=http://localhost:\n可以安装到系统环境\n 配置AWS CLI  (venv) d:\\code\\venv\u0026gt;aws configure AWS Access Key ID [None]: any-id-is-ok AWS Secret Access Key [None]: fake-key Default region name [local]: local Default output format [None]:  命令行自动完成\n$which aws_completer ~/code/venv/bin/aws_completer   tee ~/.bashrc \u0026lt;\u0026lt;-'EOF' complete -C '~/code/venv/bin/aws_completer' aws EOF   安装AWS SAM CLI  (venv) d:\\code\u0026gt;pip install aws-sam-cli (venv) d:\\code\u0026gt;sam --version SAM CLI, version 0.","tags":["SERVERLESS","LOCALSTACK"],"title":"基于local stack的Step Function本地化开发","type":"post"},{"authors":null,"categories":[],"content":" bind eip gatsby develop -- --host=0.0.0.0  Prettier VS Code plugin JSX The hybrid “HTML-in-JS” is actually a syntax extension\nof JavaScript, for React, called JSX\nIn pure JavaScript, it looks more like this:\nsrc/pages/index.js\nimport React from \u0026quot;react\u0026quot; export default () =\u0026gt; React.createElement(\u0026quot;div\u0026quot;, null, \u0026quot;Hello world!\u0026quot;)  Now you can spot the use of the \u0026lsquo;react\u0026rsquo; import! But wait. You’re writing JSX, not pure HTML and\nJavaScript. How does the browser read that? The short answer: It doesn’t. Gatsby sites come with\ntooling already set up to convert your source code into something that browsers can interpret.\ndefault plugins query MyQuery { allSitePlugin { totalCount edges { node { name browserAPIs pluginFilepath version resolve pluginOptions { name path } } } } }  implement an API API\nTo implement an API, export a function with the name of the API\ngatsby-node.js\nexports.onCreateNode = ({ node }) =\u0026gt; { console.log(node.internal.type) }  This onCreateNode function will be called by Gatsby whenever a new node is created (or updated).\nGraphQL Playground GATSBY_GRAPHQL_IDE=playground gatsby develop View the GraphQL Playground, an in-browser IDE, to explore your site's data and schema ⠀ http://localhost:8000/___graphql  The GATSBY_GRAPHQL_IDE=playground part of this command is optional.\nAdding it enables the GraphQL Playground instead of GraphiQL,\nwhich is an older interface for exploring GraphQL.\nGraphQL query template All context values are made available to a template’s GraphQL queries\nas arguments prefaced with $\n exports.createPages = async function({ actions, graphql }) { const { data } = await graphql(` query { allMarkdownRemark { edges { node { fields { slug } } } } } `) data.allMarkdownRemark.edges.forEach(edge =\u0026gt; { const slug = edge.node.fields.slug actions.createPage({ path: slug, component: require.resolve(`./src/templates/blog-post.js`), context: { slug: slug }, }) }) }  Server-side environment variables gatsby-config.js or gatsby-node.js:\n require(\u0026quot;dotenv\u0026quot;).config({ path: `.env.${process.env.NODE_ENV}`, })  .env.development\nGATSBY_GRAPHQL_IDE=playground  MDX After installing gatsby-plugin-mdx, MDX files located in src/pages will turn into pages.\nPages are rendered at a URL that is constructed from the filesystem path inside src/pages.\nAn MDX file at src/pages/awesome.mdx will result in a page being rendered at mysite.com/awesome\nFAQ  There are multiple modules with names that only differ in casing(WIN)  Potential solutions:\ncd C:\\gatsby\\dir before starting gatsby, specifying uppercase Use powershell instead of cmd (powershell will redirect to the correct dir and set the env var correctly) Use UNIX file paths and let Windows figure it out (cd /gatsby/dir instead of cd c:\\gatsby\\dir), but note this will only help if you always use UNIX paths for shell navigation; if you're already in the bogus dir, cmd will not handle it properly. Gatsby could always enforce/assume uppercase drive letters when checking paths  d:\\code\\hello-world\u0026gt;gatsby -v Gatsby CLI version: 2.8.18 Gatsby version: 2.18.8  ","date":1535936617,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535936617,"objectID":"f5aa97ebbcbdd3ccc2d90cf4956da28e","permalink":"https://wubigo.com/post/gatsby-notes/","publishdate":"2018-09-03T09:03:37+08:00","relpermalink":"/post/gatsby-notes/","section":"post","summary":"bind eip gatsby develop -- --host=0.0.0.0  Prettier VS Code plugin JSX The hybrid “HTML-in-JS” is actually a syntax extension\nof JavaScript, for React, called JSX\nIn pure JavaScript, it looks more like this:\nsrc/pages/index.js\nimport React from \u0026quot;react\u0026quot; export default () =\u0026gt; React.createElement(\u0026quot;div\u0026quot;, null, \u0026quot;Hello world!\u0026quot;)  Now you can spot the use of the \u0026lsquo;react\u0026rsquo; import! But wait. You’re writing JSX, not pure HTML and\nJavaScript. How does the browser read that?","tags":["NODE","JS"],"title":"Gatsby Notes","type":"post"},{"authors":null,"categories":[],"content":" glide To upgrade dependencies, please make the necessary modifications in glide.yaml and run glide update.\n","date":1535495211,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535495211,"objectID":"cce73b1fae9184578dda0b661ffb918e","permalink":"https://wubigo.com/post/lang-go-dep-manage/","publishdate":"2018-08-29T06:26:51+08:00","relpermalink":"/post/lang-go-dep-manage/","section":"post","summary":"glide To upgrade dependencies, please make the necessary modifications in glide.yaml and run glide update.","tags":["LANG","GO"],"title":"Lang Go Dep Manage","type":"post"},{"authors":null,"categories":[],"content":" Add notification configuration to SNS Topic resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic\u0026quot; { name = \u0026quot;s3-event-notification-topic\u0026quot; policy = \u0026lt;\u0026lt;POLICY { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: {\u0026quot;AWS\u0026quot;:\u0026quot;*\u0026quot;}, \u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:*:*:s3-event-notification-topic\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;ArnLike\u0026quot;:{\u0026quot;aws:SourceArn\u0026quot;:\u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot;} } }] } POLICY } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; topic { topic_arn = \u0026quot;${aws_sns_topic.topic.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_suffix = \u0026quot;.log\u0026quot; } }  Add notification configuration to Lambda Function resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;iam_for_lambda\u0026quot; { name = \u0026quot;iam_for_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EOF } resource \u0026quot;aws_lambda_permission\u0026quot; \u0026quot;allow_bucket\u0026quot; { statement_id = \u0026quot;AllowExecutionFromS3Bucket\u0026quot; action = \u0026quot;lambda:InvokeFunction\u0026quot; function_name = \u0026quot;${aws_lambda_function.func.arn}\u0026quot; principal = \u0026quot;s3.amazonaws.com\u0026quot; source_arn = \u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot; } resource \u0026quot;aws_lambda_function\u0026quot; \u0026quot;func\u0026quot; { filename = \u0026quot;your-function.zip\u0026quot; function_name = \u0026quot;example_lambda_name\u0026quot; role = \u0026quot;${aws_iam_role.iam_for_lambda.arn}\u0026quot; handler = \u0026quot;exports.example\u0026quot; runtime = \u0026quot;go1.x\u0026quot; } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; lambda_function { lambda_function_arn = \u0026quot;${aws_lambda_function.func.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_prefix = \u0026quot;AWSLogs/\u0026quot; filter_suffix = \u0026quot;.log\u0026quot; } }  import S3 bucket notification can be imported using the bucket\nterraform import aws_s3_bucket_notification.bucket_notification bucket-name  https://www.terraform.io/docs/providers/aws/r/s3_bucket_notification.html\nlocalstack endpoints https://www.terraform.io/docs/providers/aws/guides/custom-service-endpoints.html#localstack\n","date":1532337464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532337464,"objectID":"601b17556296914a246acdbd34f4e949","permalink":"https://wubigo.com/post/terraform-aws-s3-resource-example/","publishdate":"2018-07-23T17:17:44+08:00","relpermalink":"/post/terraform-aws-s3-resource-example/","section":"post","summary":"Add notification configuration to SNS Topic resource \u0026quot;aws_sns_topic\u0026quot; \u0026quot;topic\u0026quot; { name = \u0026quot;s3-event-notification-topic\u0026quot; policy = \u0026lt;\u0026lt;POLICY { \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[{ \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: {\u0026quot;AWS\u0026quot;:\u0026quot;*\u0026quot;}, \u0026quot;Action\u0026quot;: \u0026quot;SNS:Publish\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:sns:*:*:s3-event-notification-topic\u0026quot;, \u0026quot;Condition\u0026quot;:{ \u0026quot;ArnLike\u0026quot;:{\u0026quot;aws:SourceArn\u0026quot;:\u0026quot;${aws_s3_bucket.bucket.arn}\u0026quot;} } }] } POLICY } resource \u0026quot;aws_s3_bucket\u0026quot; \u0026quot;bucket\u0026quot; { bucket = \u0026quot;your_bucket_name\u0026quot; } resource \u0026quot;aws_s3_bucket_notification\u0026quot; \u0026quot;bucket_notification\u0026quot; { bucket = \u0026quot;${aws_s3_bucket.bucket.id}\u0026quot; topic { topic_arn = \u0026quot;${aws_sns_topic.topic.arn}\u0026quot; events = [\u0026quot;s3:ObjectCreated:*\u0026quot;] filter_suffix = \u0026quot;.log\u0026quot; } }  Add notification configuration to Lambda Function resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;iam_for_lambda\u0026quot; { name = \u0026quot;iam_for_lambda\u0026quot; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.","tags":[],"title":"Terraform Aws S3 Resource Example","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境  可以参考从源代码构件K8S开发环境\n","date":1529982647,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529982647,"objectID":"8586d462dfbc6d894d5fa6a3818c6442","permalink":"https://wubigo.com/post/k8s_cni_calico/","publishdate":"2018-06-26T11:10:47+08:00","relpermalink":"/post/k8s_cni_calico/","section":"post","summary":"准备  搭建测试环境  可以参考从源代码构件K8S开发环境","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Calico实现","type":"post"},{"authors":null,"categories":[],"content":" 避开Tiller使用Helm部署K8S应用\nTiller存在的问题  破坏RBAC访问机制  全局的Tiller拥有cluster-admin角色，所以在安装过程中，服务以cluster-admin 角色可以越权访问资源\n 部署名字不能重复且唯一  部署名字唯一且很多chart中部署名字也添加到服务名中，导致服务名字混乱。\n独立使用helm  获取模板 使用配置修改模板 生产yaml文件  git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 helm template install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set security.enabled=false \\ --set ingress.enabled=false \\ --set gateways.istio-ingressgateway.enabled=false \\ --set gateways.istio-egressgateway.enabled=false \\ --set galley.enabled=false \\ --set sidecarInjectorWebhook.enabled=false \\ --set mixer.enabled=false \\ --set prometheus.enabled=false \\ --set global.proxy.envoyStatsd.enabled=false \\ --set pilot.sidecar=false \u0026gt; $HOME/istio-minimal.yaml kubectl create namespace istio-system kubectl apply -f $HOME/istio-minimal.yaml  ","date":1528117376,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528117376,"objectID":"6c4c0ce07df34850631b924a37f82dac","permalink":"https://wubigo.com/post/helm-without-tiller/","publishdate":"2018-06-04T21:02:56+08:00","relpermalink":"/post/helm-without-tiller/","section":"post","summary":"避开Tiller使用Helm部署K8S应用\nTiller存在的问题  破坏RBAC访问机制  全局的Tiller拥有cluster-admin角色，所以在安装过程中，服务以cluster-admin 角色可以越权访问资源\n 部署名字不能重复且唯一  部署名字唯一且很多chart中部署名字也添加到服务名中，导致服务名字混乱。\n独立使用helm  获取模板 使用配置修改模板 生产yaml文件  git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 helm template install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set security.enabled=false \\ --set ingress.enabled=false \\ --set gateways.istio-ingressgateway.enabled=false \\ --set gateways.istio-egressgateway.enabled=false \\ --set galley.enabled=false \\ --set sidecarInjectorWebhook.enabled=false \\ --set mixer.enabled=false \\ --set prometheus.enabled=false \\ --set global.proxy.envoyStatsd.enabled=false \\ --set pilot.sidecar=false \u0026gt; $HOME/istio-minimal.yaml kubectl create namespace istio-system kubectl apply -f $HOME/istio-minimal.","tags":["K8S","DEVOPS"],"title":"避开Tiller使用Helm部署K8S应用","type":"post"},{"authors":null,"categories":[],"content":" Headless services Without POD selectors This creates a service, but it doesn’t know where to send the traffic. This allows you to manually create an Endpoints object that will receive traffic from this service.\nkind: Endpoints apiVersion: v1 metadata: name: mongo subsets: - addresses: - ip: 10.240.0.4 ports: - port: 2701  CNAME records for ExternalName This service does a simple CNAME redirection at the kernel level, so there is very minimal impact on performance.\nkind: Service apiVersion: v1 metadata: name: mongo spec: type: ExternalName externalName: ds149763.mlab.com  ","date":1525948342,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525948342,"objectID":"1d5ac9fc01ac8850430cc92daee5ba32","permalink":"https://wubigo.com/post/k8s-external-services/","publishdate":"2018-05-10T18:32:22+08:00","relpermalink":"/post/k8s-external-services/","section":"post","summary":"Headless services Without POD selectors This creates a service, but it doesn’t know where to send the traffic. This allows you to manually create an Endpoints object that will receive traffic from this service.\nkind: Endpoints apiVersion: v1 metadata: name: mongo subsets: - addresses: - ip: 10.240.0.4 ports: - port: 2701  CNAME records for ExternalName This service does a simple CNAME redirection at the kernel level, so there is very minimal impact on performance.","tags":["K8S"],"title":"K8s External Services","type":"post"},{"authors":null,"categories":null,"content":"The most important conversation you ever have is the one with yourself\n","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"fec0f9fdbade4d5491c25a75127e566b","permalink":"https://wubigo.com/post/2019-01-01-sevenlevelcommunication/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/post/2019-01-01-sevenlevelcommunication/","section":"post","summary":"The most important conversation you ever have is the one with yourself","tags":["communication"],"title":"7 level communication","type":"post"},{"authors":null,"categories":[],"content":" 无服务器架构应用场景  应用后台\n 数据处理\n 实时分析\n 遗留应用API代理\n 调动服务\n RPA\n  ","date":1525099387,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525099387,"objectID":"92819a379c93d5b8a649074d26652d1a","permalink":"https://wubigo.com/post/serverless-user-case/","publishdate":"2018-04-30T22:43:07+08:00","relpermalink":"/post/serverless-user-case/","section":"post","summary":" 无服务器架构应用场景  应用后台\n 数据处理\n 实时分析\n 遗留应用API代理\n 调动服务\n RPA\n  ","tags":["SERVERLESS"],"title":"无服务器架构应用场景","type":"post"},{"authors":null,"categories":[],"content":" https://stackoverflow.com/questions/44547574/create-api-gateway-in-localstack/48682628\nhttps://github.com/localstack/localstack/issues/632\nAWS SAM is an extension for the AWS CloudFormation template language that lets you define serverless applications at a higher level\nlocalstack default regrion us-east-1\ncreate stack file path has to be in file URL format(file:///home/user/\u0026hellip;)\nfunc.yaml\nAWSTemplateFormatVersion: '2010-09-09' Description: Simple CloudFormation Test Template Resources: S3Bucket: Type: AWS::S3::Bucket Properties: AccessControl: PublicRead BucketName: test-bucket-1  aws cloudformation create-stack --stack-name funstack --template-body file:///data/func.yaml --endpoint-url=http://localhost:4581 --region us-east-1  aws cloudformation describe-stacks --endpoint-url=http://localhost:4581 --region us-east-1  ","date":1524982842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524982842,"objectID":"7030e28d328f13e5d86ff6c13c5258f2","permalink":"https://wubigo.com/post/aws-cloudformation-with-localstack/","publishdate":"2018-04-29T14:20:42+08:00","relpermalink":"/post/aws-cloudformation-with-localstack/","section":"post","summary":" https://stackoverflow.com/questions/44547574/create-api-gateway-in-localstack/48682628\nhttps://github.com/localstack/localstack/issues/632\nAWS SAM is an extension for the AWS CloudFormation template language that lets you define serverless applications at a higher level\nlocalstack default regrion us-east-1\ncreate stack file path has to be in file URL format(file:///home/user/\u0026hellip;)\nfunc.yaml\nAWSTemplateFormatVersion: '2010-09-09' Description: Simple CloudFormation Test Template Resources: S3Bucket: Type: AWS::S3::Bucket Properties: AccessControl: PublicRead BucketName: test-bucket-1  aws cloudformation create-stack --stack-name funstack --template-body file:///data/func.yaml --endpoint-url=http://localhost:4581 --region us-east-1  aws cloudformation describe-stacks --endpoint-url=http://localhost:4581 --region us-east-1  ","tags":["LOCALSTACK","SERVERLESS"],"title":"基于Localstack的本地云服务编排","type":"post"},{"authors":null,"categories":[],"content":" 设计目标  存取（入库和分析）高效\n 节省存储空间\n  评估单台设备基于采集评率的每年存储成本\nhttp://mysql.rjweb.org/doc.php/datawarehouse\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"e8283ab0add8e9a2c33e46784ad3a915","permalink":"https://wubigo.com/post/data-lake/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/data-lake/","section":"post","summary":"设计目标  存取（入库和分析）高效\n 节省存储空间\n  评估单台设备基于采集评率的每年存储成本\nhttp://mysql.rjweb.org/doc.php/datawarehouse","tags":["DATALAKE","MYSQL"],"title":"Data Lake","type":"post"},{"authors":null,"categories":[],"content":" docker proxy run cmd as administrator cmd\u0026gt;cd $GIT_HOME cmd\u0026gt;echo \u0026gt; .bash_profile export HTTP_PROXY=http://127.0.0.1:1080 export HTTPS_PROXY=http://127.0.0.1:1080 export no_proxy=localhost,127.0.0.1,192.168.99.100  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"03f312aefa2bb8217d69618ba303dc99","permalink":"https://wubigo.com/post/docker-windows7-docker-toolbox/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/docker-windows7-docker-toolbox/","section":"post","summary":" docker proxy run cmd as administrator cmd\u0026gt;cd $GIT_HOME cmd\u0026gt;echo \u0026gt; .bash_profile export HTTP_PROXY=http://127.0.0.1:1080 export HTTPS_PROXY=http://127.0.0.1:1080 export no_proxy=localhost,127.0.0.1,192.168.99.100  ","tags":["WIN","DOCKER"],"title":"Docker Windows7 Docker Toolbox","type":"post"},{"authors":null,"categories":[],"content":" execution environment Creates an execution environment that represents the context in which the program is currently executed. If the program is invoked standalone, this method returns a local execution environment. If the program is invoked from within the command line client to be submitted to a cluster, this method returns the execution environment of this cluster.  REST instead of akka in 1.5 changing the client to communicate via REST instead of akka. Thus, the port you usually want to specify is 8081, the same that the WebUI runs on.\nThe exception is caused by the client sending http messages to the port that akka listens on, which it of course cannot properly read.\nflink list -m localhost:8081  StreamExecutionEnvironment.createRemoteEnvironment(\u0026quot;localhost\u0026quot;, 8081)  Flink Kafka Consumer The Flink Kafka Consumer integrates with Flink’s checkpointing mechanism to provide exactly-once processing semantics. To achieve that, Flink does not purely rely on Kafka’s consumer group offset tracking, but tracks and checkpoints these offsets internally as well.\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"f9ff1f7dbc73349140a820d81042ca2a","permalink":"https://wubigo.com/post/flink-cluster/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/flink-cluster/","section":"post","summary":"execution environment Creates an execution environment that represents the context in which the program is currently executed. If the program is invoked standalone, this method returns a local execution environment. If the program is invoked from within the command line client to be submitted to a cluster, this method returns the execution environment of this cluster.  REST instead of akka in 1.5 changing the client to communicate via REST instead of akka.","tags":["STREAM","BIGDATA"],"title":"Flink Cluster","type":"post"},{"authors":null,"categories":[],"content":"https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"30093daa5af1ae89d2c62209092a0a56","permalink":"https://wubigo.com/post/grpc-load-balancing-on-kubernetes-without-tears/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/grpc-load-balancing-on-kubernetes-without-tears/","section":"post","summary":"https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/","tags":["GRPC","K8S"],"title":"GRPC Load Balancing on Kubernetes Without Tears","type":"post"},{"authors":null,"categories":[],"content":"  Allow more ways of creating objects using literals\n Introduce new datatypes together with their operators and expressions.\n  Closure simple abbreviated syntax of closures: after a method call, put code in braces with parameters delimited from the closure body by an arrow.\nlog = '' (1..10).each{ log += it } assert log == '12345678910' log = '' (1..10).each{ counter -\u0026gt; log += counter } assert log == '12345678910'  A second way of declaring a closure is to directly assign it to a variable:\ndef printer = { line -\u0026gt; println line }   a single parameter default name(it)  Similarly, if the closure needs to take only a single parameter to work on, Groovy provides a default name—it—so that you don’t need to declare it specifically\n Closures are Groovy’s way of providing transparent callback targets as first-class citizens.  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"0dece19b024b6a1abe91a5ae15f864c2","permalink":"https://wubigo.com/post/groovy-notes/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/groovy-notes/","section":"post","summary":"Allow more ways of creating objects using literals\n Introduce new datatypes together with their operators and expressions.\n  Closure simple abbreviated syntax of closures: after a method call, put code in braces with parameters delimited from the closure body by an arrow.\nlog = '' (1..10).each{ log += it } assert log == '12345678910' log = '' (1..10).each{ counter -\u0026gt; log += counter } assert log == '12345678910'  A second way of declaring a closure is to directly assign it to a variable:","tags":["JAVA","GROOVY"],"title":"Groovy Notes","type":"post"},{"authors":null,"categories":[],"content":" install client pip install shadowsocks  client.json\n{ \u0026quot;server\u0026quot;:\u0026quot;server-ip\u0026quot;, \u0026quot;server_port\u0026quot;:8000, \u0026quot;local_port\u0026quot;:3050, \u0026quot;password\u0026quot;:\u0026quot;your-password\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  { \u0026quot;server\u0026quot;:\u0026quot;your_server_ip\u0026quot;, #ss服务器IP \u0026quot;server_port\u0026quot;:your_server_port, #端口 \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, #本地ip \u0026quot;local_port\u0026quot;:1080, #本地端口 \u0026quot;password\u0026quot;:\u0026quot;your_server_passwd\u0026quot;,#连接ss密码 \u0026quot;timeout\u0026quot;:300, #等待超时 \u0026quot;method\u0026quot;:\u0026quot;rc4-md5\u0026quot;, #加密方式 \u0026quot;fast_open\u0026quot;: false, # true 或 false。如果你的服务器 Linux 内核在3.7+，可以开启 fast_open 以降低延迟。开启方法： echo 3 \u0026gt; /proc/sys/net/ipv4/tcp_fastopen 开启之后，将 fast_open 的配置设置为 true 即可 \u0026quot;workers\u0026quot;: 1 # 工作线程数 }  sudo apt-get install privoxy  /etc/privoxy/config\nlisten-address 127.0.0.1:8118 forward-socks5 / 127.0.0.1:1080 .  systemctl restart privoxy.service  curl -x 127.0.0.1:1080 www.google.com  ","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"b96cb476e93157ba1009f63631e5e30b","permalink":"https://wubigo.com/post/shadowsocks-ubuntu-client/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/shadowsocks-ubuntu-client/","section":"post","summary":"install client pip install shadowsocks  client.json\n{ \u0026quot;server\u0026quot;:\u0026quot;server-ip\u0026quot;, \u0026quot;server_port\u0026quot;:8000, \u0026quot;local_port\u0026quot;:3050, \u0026quot;password\u0026quot;:\u0026quot;your-password\u0026quot;, \u0026quot;timeout\u0026quot;:600, \u0026quot;method\u0026quot;:\u0026quot;aes-256-cfb\u0026quot; }  { \u0026quot;server\u0026quot;:\u0026quot;your_server_ip\u0026quot;, #ss服务器IP \u0026quot;server_port\u0026quot;:your_server_port, #端口 \u0026quot;local_address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;, #本地ip \u0026quot;local_port\u0026quot;:1080, #本地端口 \u0026quot;password\u0026quot;:\u0026quot;your_server_passwd\u0026quot;,#连接ss密码 \u0026quot;timeout\u0026quot;:300, #等待超时 \u0026quot;method\u0026quot;:\u0026quot;rc4-md5\u0026quot;, #加密方式 \u0026quot;fast_open\u0026quot;: false, # true 或 false。如果你的服务器 Linux 内核在3.7+，可以开启 fast_open 以降低延迟。开启方法： echo 3 \u0026gt; /proc/sys/net/ipv4/tcp_fastopen 开启之后，将 fast_open 的配置设置为 true 即可 \u0026quot;workers\u0026quot;: 1 # 工作线程数 }  sudo apt-get install privoxy  /etc/privoxy/config\nlisten-address 127.0.0.1:8118 forward-socks5 / 127.0.0.1:1080 .  systemctl restart privoxy.","tags":["SHELL","VPN"],"title":"Shadowsocks Ubuntu Client","type":"post"},{"authors":null,"categories":[],"content":" setup  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  enable web client all endpoints are exposed to JMX and WEB clents\nBy default, all endpoints except for shutdown are enabled.\n enable all endpoings\n enable all endpoints accessed by web\n  management: endpoints: enabled-by-default: true web: exposure: include: \u0026quot;*\u0026quot;  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\nWebApplicationType  spring: main: web-application-type: reactive   NONE  The application should not run as a web application and should not start an embedded web server.\n REACTIVE  The application should run as a reactive web application and should start an embedded reactive web server.\n SERVLET  The application should run as a servlet-based web application and should start an embedded servlet web server.\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"fc2e98991beb4d616dfd04cffc8782fd","permalink":"https://wubigo.com/post/spring-actuator/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/spring-actuator/","section":"post","summary":"setup  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  enable web client all endpoints are exposed to JMX and WEB clents\nBy default, all endpoints except for shutdown are enabled.\n enable all endpoings\n enable all endpoints accessed by web\n  management: endpoints: enabled-by-default: true web: exposure: include: \u0026quot;*\u0026quot;  https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\nWebApplicationType  spring: main: web-application-type: reactive   NONE  The application should not run as a web application and should not start an embedded web server.","tags":["SPRING","MICROSERVICE"],"title":"Spring Actuator","type":"post"},{"authors":null,"categories":[],"content":" docker run -it -p 9090:9090 -p 1883:1883 -p 5683:5683/udp -v ~/.mytb-data:/data -v ~/.mytb-logs:/var/log/thingsboard --name mytb --restart always thingsboard/tb:2.3.1  default credentials: Systen Administrator: sysadmin@thingsboard.org / sysadmin Tenant Administrator: tenant@thingsboard.org / tenant Customer User: customer@thingsboard.org / customer  Manage device credentials login as tenant\n","date":1524906487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524906487,"objectID":"be988aba65180299d0691749c2d5a1b9","permalink":"https://wubigo.com/post/thingboard/","publishdate":"2018-04-28T17:08:07+08:00","relpermalink":"/post/thingboard/","section":"post","summary":"docker run -it -p 9090:9090 -p 1883:1883 -p 5683:5683/udp -v ~/.mytb-data:/data -v ~/.mytb-logs:/var/log/thingsboard --name mytb --restart always thingsboard/tb:2.3.1  default credentials: Systen Administrator: sysadmin@thingsboard.org / sysadmin Tenant Administrator: tenant@thingsboard.org / tenant Customer User: customer@thingsboard.org / customer  Manage device credentials login as tenant","tags":["WEB","MONITOR"],"title":"Thingboard","type":"post"},{"authors":null,"categories":[],"content":" CORE  Virtual DOM Component-based UI Focus on the view library—separate concerns for routing, state management Official component library for building mobile apps  D:\\code\\API\\web\u0026gt;npm config list ; cli configs metrics-registry = \u0026quot;https://registry.npmjs.org/\u0026quot; scope = \u0026quot;\u0026quot; user-agent = \u0026quot;npm/6.4.1 node/v10.15.3 win32 x64\u0026quot;  npm config set registry \u0026lt;registry url\u0026gt;   install with another registry  npm install --registry=https://registry.npmjs.org/  install vue-cli npm install -g @vue/cli  vue create my-project  OR\nvue ui  ","date":1524628961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524628961,"objectID":"70dc17d46512b88bb596f6d7b0ffb754","permalink":"https://wubigo.com/post/web-vue/","publishdate":"2018-04-25T12:02:41+08:00","relpermalink":"/post/web-vue/","section":"post","summary":"CORE  Virtual DOM Component-based UI Focus on the view library—separate concerns for routing, state management Official component library for building mobile apps  D:\\code\\API\\web\u0026gt;npm config list ; cli configs metrics-registry = \u0026quot;https://registry.npmjs.org/\u0026quot; scope = \u0026quot;\u0026quot; user-agent = \u0026quot;npm/6.4.1 node/v10.15.3 win32 x64\u0026quot;  npm config set registry \u0026lt;registry url\u0026gt;   install with another registry  npm install --registry=https://registry.npmjs.org/  install vue-cli npm install -g @vue/cli  vue create my-project  OR","tags":["WEB","JS"],"title":"Web Vue","type":"post"},{"authors":null,"categories":[],"content":" install winpcap and windump https://www.winpcap.org\nlist all interfaces windump -D  dump on interface windump -i 1 -n dst host 172.17.17.6  See entire packet payload using tcpdump windump -nnvvXSs 1514 -i 1 -n dst host 172.17.17.6  ","date":1524569089,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524569089,"objectID":"1222025a6861e453bda47d3b930e1ee3","permalink":"https://wubigo.com/post/tcpdump-windows/","publishdate":"2018-04-24T19:24:49+08:00","relpermalink":"/post/tcpdump-windows/","section":"post","summary":" install winpcap and windump https://www.winpcap.org\nlist all interfaces windump -D  dump on interface windump -i 1 -n dst host 172.17.17.6  See entire packet payload using tcpdump windump -nnvvXSs 1514 -i 1 -n dst host 172.17.17.6  ","tags":["TCP"],"title":"Tcpdump Windows","type":"post"},{"authors":null,"categories":[],"content":" spring-cloud-greenwich-release To get started with Maven with a BOM (dependency management only):\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;Greenwich.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  ","date":1523485284,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523485284,"objectID":"a70c3d69fe15d53d310406a5ab1682fd","permalink":"https://wubigo.com/post/lang-java-spring-cloud/","publishdate":"2018-04-12T06:21:24+08:00","relpermalink":"/post/lang-java-spring-cloud/","section":"post","summary":" spring-cloud-greenwich-release To get started with Maven with a BOM (dependency management only):\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;Greenwich.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  ","tags":["JAVA","LANG"],"title":"Lang Java Spring Cloud","type":"post"},{"authors":null,"categories":[],"content":" JVM bind with IPv4 Disable IPv6 address lookups when -Djava.net.preferIPv4Stack=true\n-Djava.net.preferIPv4Stack=true  Spring Boot Actuator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Actuator comes with most endpoints disabled. Thus, the only two available by default are /health and /info.\nmanagement.endpoints.web.exposure.include=*  by default, all Actuator endpoints are now placed under the /actuator path\nmvn dependency:tree [INFO] +- org.springframework.boot:spring-boot-starter-data-jpa:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-aop:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework:spring-aop:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.aspectj:aspectjweaver:jar:1.9.2:compile [INFO] | +- org.springframework.boot:spring-boot-starter-jdbc:jar:2.1.4.RELEASE:compile [INFO] | | +- com.zaxxer:HikariCP:jar:3.2.0:compile [INFO] | | \\- org.springframework:spring-jdbc:jar:5.1.6.RELEASE:compile [INFO] | +- javax.transaction:javax.transaction-api:jar:1.3:compile [INFO] | +- javax.xml.bind:jaxb-api:jar:2.3.1:compile [INFO] | | \\- javax.activation:javax.activation-api:jar:1.2.0:compile [INFO] | +- org.hibernate:hibernate-core:jar:5.3.9.Final:compile [INFO] | | +- org.jboss.logging:jboss-logging:jar:3.3.2.Final:compile [INFO] | | +- javax.persistence:javax.persistence-api:jar:2.2:compile [INFO] | | +- org.javassist:javassist:jar:3.23.1-GA:compile [INFO] | | +- net.bytebuddy:byte-buddy:jar:1.9.12:compile [INFO] | | +- antlr:antlr:jar:2.7.7:compile [INFO] | | +- org.jboss:jandex:jar:2.0.5.Final:compile [INFO] | | +- com.fasterxml:classmate:jar:1.4.0:compile [INFO] | | +- org.dom4j:dom4j:jar:2.1.1:compile [INFO] | | \\- org.hibernate.common:hibernate-commons-annotations:jar:5.0.4.Final:compile [INFO] | +- org.springframework.data:spring-data-jpa:jar:2.1.6.RELEASE:compile [INFO] | | +- org.springframework.data:spring-data-commons:jar:2.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-orm:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-context:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-tx:jar:5.1.6.RELEASE:compile [INFO] | | +- org.springframework:spring-beans:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.slf4j:slf4j-api:jar:1.7.26:compile [INFO] | \\- org.springframework:spring-aspects:jar:5.1.6.RELEASE:compile [INFO] +- org.springframework.boot:spring-boot-starter-thymeleaf:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot-autoconfigure:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework.boot:spring-boot-starter-logging:jar:2.1.4.RELEASE:compile [INFO] | | | +- ch.qos.logback:logback-classic:jar:1.2.3:compile [INFO] | | | | \\- ch.qos.logback:logback-core:jar:1.2.3:compile [INFO] | | | +- org.apache.logging.log4j:log4j-to-slf4j:jar:2.11.2:compile [INFO] | | | | \\- org.apache.logging.log4j:log4j-api:jar:2.11.2:compile [INFO] | | | \\- org.slf4j:jul-to-slf4j:jar:1.7.26:compile [INFO] | | +- javax.annotation:javax.annotation-api:jar:1.3.2:compile [INFO] | | \\- org.yaml:snakeyaml:jar:1.23:runtime [INFO] | +- org.thymeleaf:thymeleaf-spring5:jar:3.0.11.RELEASE:compile [INFO] | | \\- org.thymeleaf:thymeleaf:jar:3.0.11.RELEASE:compile [INFO] | | +- org.attoparser:attoparser:jar:2.0.5.RELEASE:compile [INFO] | | \\- org.unbescape:unbescape:jar:1.1.6.RELEASE:compile [INFO] | \\- org.thymeleaf.extras:thymeleaf-extras-java8time:jar:3.0.4.RELEASE:compile [INFO] +- org.springframework.boot:spring-boot-starter-web:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-json:jar:2.1.4.RELEASE:compile [INFO] | | +- com.fasterxml.jackson.core:jackson-databind:jar:2.9.8:compile [INFO] | | | +- com.fasterxml.jackson.core:jackson-annotations:jar:2.9.0:compile [INFO] | | | \\- com.fasterxml.jackson.core:jackson-core:jar:2.9.8:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.9.8:compile [INFO] | | +- com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.9.8:compile [INFO] | | \\- com.fasterxml.jackson.module:jackson-module-parameter-names:jar:2.9.8:compile [INFO] | +- org.springframework.boot:spring-boot-starter-tomcat:jar:2.1.4.RELEASE:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-core:jar:9.0.17:compile [INFO] | | +- org.apache.tomcat.embed:tomcat-embed-el:jar:9.0.17:compile [INFO] | | \\- org.apache.tomcat.embed:tomcat-embed-websocket:jar:9.0.17:compile [INFO] | +- org.hibernate.validator:hibernate-validator:jar:6.0.16.Final:compile [INFO] | | \\- javax.validation:validation-api:jar:2.0.1.Final:compile [INFO] | +- org.springframework:spring-web:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-webmvc:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-expression:jar:5.1.6.RELEASE:compile [INFO] +- com.h2database:h2:jar:1.4.199:runtime [INFO] \\- org.springframework.boot:spring-boot-starter-test:jar:2.1.4.RELEASE:test [INFO] +- org.springframework.boot:spring-boot-test:jar:2.1.4.RELEASE:test [INFO] +- org.springframework.boot:spring-boot-test-autoconfigure:jar:2.1.4.RELEASE:test [INFO] +- com.jayway.jsonpath:json-path:jar:2.4.0:test [INFO] | \\- net.minidev:json-smart:jar:2.3:test [INFO] | \\- net.minidev:accessors-smart:jar:1.2:test [INFO] | \\- org.ow2.asm:asm:jar:5.0.4:test [INFO] +- junit:junit:jar:4.12:test [INFO] +- org.assertj:assertj-core:jar:3.11.1:test [INFO] +- org.mockito:mockito-core:jar:2.23.4:test [INFO] | +- net.bytebuddy:byte-buddy-agent:jar:1.9.12:test [INFO] | \\- org.objenesis:objenesis:jar:2.6:test [INFO] +- org.hamcrest:hamcrest-core:jar:1.3:test [INFO] +- org.hamcrest:hamcrest-library:jar:1.3:test [INFO] +- org.skyscreamer:jsonassert:jar:1.5.0:test [INFO] | \\- com.vaadin.external.google:android-json:jar:0.0.20131108.vaadin1:test [INFO] +- org.springframework:spring-core:jar:5.1.6.RELEASE:compile [INFO] | \\- org.springframework:spring-jcl:jar:5.1.6.RELEASE:compile [INFO] +- org.springframework:spring-test:jar:5.1.6.RELEASE:test [INFO] \\- org.xmlunit:xmlunit-core:jar:2.6.2:test  ","date":1523407698,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523407698,"objectID":"ea0d844a632be5bb714a76ded1de6b40","permalink":"https://wubigo.com/post/lang-java-spring-boot-v2/","publishdate":"2018-04-11T08:48:18+08:00","relpermalink":"/post/lang-java-spring-boot-v2/","section":"post","summary":"JVM bind with IPv4 Disable IPv6 address lookups when -Djava.net.preferIPv4Stack=true\n-Djava.net.preferIPv4Stack=true  Spring Boot Actuator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Actuator comes with most endpoints disabled. Thus, the only two available by default are /health and /info.\nmanagement.endpoints.web.exposure.include=*  by default, all Actuator endpoints are now placed under the /actuator path\nmvn dependency:tree [INFO] +- org.springframework.boot:spring-boot-starter-data-jpa:jar:2.1.4.RELEASE:compile [INFO] | +- org.springframework.boot:spring-boot-starter-aop:jar:2.1.4.RELEASE:compile [INFO] | | +- org.springframework:spring-aop:jar:5.1.6.RELEASE:compile [INFO] | | \\- org.","tags":["JAVA","LANG"],"title":"Lang Java Spring Boot V2","type":"post"},{"authors":null,"categories":null,"content":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523059200,"objectID":"db686c5b514d2e320e7018c5b058cc03","permalink":"https://wubigo.com/post/2018-04-07-dgraphnotes/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/post/2018-04-07-dgraphnotes/","section":"post","summary":" Start Dgraph cluster dgraph zero  start Dgraph server  dgraph server --memory_mb 2048 --zero localhost:5080 --port_offset 2000 Note:port_offsetValue added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]  How do I configure Go to use a proxy https://stackoverflow.com/questions/10383299/how-do-i-configure-go-to-use-a-proxy\nWeb based graph visualization with D3 and KeyLines https://cambridge-intelligence.com/web-graph-visualization-d3-keylines/\nSETUP CLIENT set http_proxy=192.168.0.119:3128 git config --global http.proxy http://192.168.0.119:3128 go get -u github.com/derekparker/delve/cmd/dlv go get -u -v github.com/dgraph-io/dgo  ","tags":["NOSQL","GRAPH"],"title":"Dgraph note","type":"post"},{"authors":null,"categories":[],"content":" SNAPSHOT // Snapshot is an internally consistent snapshot of xDS resources. // Consistentcy is important for the convergence as different resource types // from the snapshot may be delivered to the proxy in arbitrary order. type Snapshot struct { // Endpoints are items in the EDS response payload. Endpoints Resources // Clusters are items in the CDS response payload. Clusters Resources // Routes are items in the RDS response payload. Routes Resources // Listeners are items in the LDS response payload. Listeners Resources // Secrets are items in the SDS response payload. Secrets Resources }  graceful shutdown mechanism Get shutdown notice /healthcheck/fail Wait X time Shutdown  BUILD go get -d github.com/envoyproxy/go-control-plane cd $GOPATH/src/github.com/envoyproxy/go-control-plane docker build -t bigo/envoy-test:v1 .  TEST XDS in docker docker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug docker run -it --rm -e \u0026quot;XDS=xds\u0026quot; bigo/envoy-test:v1 -debug docker run -it --rm -e \u0026quot;XDS=rest\u0026quot; bigo/envoy-test:v1 -debug  TEST XDS with tls in docker docker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug -tls docker run -it --rm -e \u0026quot;XDS=xds\u0026quot; bigo/envoy-test:v1 -debug -tls docker run -it --rm -e \u0026quot;XDS=rest\u0026quot; bigo/envoy-test:v1 -debug -tls  Test it in code 在code里面启动ADS API服务器\npkg/test/main\nlaunch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Launch\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}\u0026quot;, \u0026quot;env\u0026quot;: {}, \u0026quot;args\u0026quot;: [\u0026quot;-debug\u0026quot;, \u0026quot;--xds=ads\u0026quot;] } ] }  在主机上运行\ndocker run -it --name envoy --entrypoint sh bigo/envoy-test:v1 docker cp envoy:/usr/local/bin/envoy ~/go/bin/ envoy -c /sample/bootstrap-ads.yaml --drain-time-s 1 -l debug [source/server/server.cc:270] admin address: 127.0.0.1:19000  在容器内运行\ndocker run -it --rm --name envoy --entrypoint sh bigo/envoy-test:v1  修改配置 /sample/bootstrap-ads.yaml\n允许远程访问代理服务器管理端口\nadmin: address: socket_address: address: 0.0.0.0  docker cp sample/bootstrap-ads.yaml envoy:/sample/bootstrap-ads.yaml #envoy -c /sample/bootstrap-ads.yaml --drain-time-s 1  查询容器IP http://172.17.0.2:19000/clusters xds_cluster::192.168.1.5:28000::cx_connect_fail::6  如果管理服务器没有启动，会出现连接失败计数。\n通过manager server下发配置\ndocker run -it --rm -e \u0026quot;XDS=ads\u0026quot; bigo/envoy-test:v1 -debug  再次检查ENVOY的配置\nhttp://localhost:19000/config_dump\nBootstrap configuration To use the v2 API, it’s necessary to supply a bootstrap configuration file. This provides static server configuration and configures Envoy to access dynamic configuration if needed. This is supplied on the command-line via the -c flag\n./envoy -c \u0026lt;path to config\u0026gt;.{json,yaml,pb,pb_text}  The Bootstrap message is the root of the configuration. Resources such as a Listener or Cluster may be supplied either statically in static_resources or have an xDS service such as LDS or CDS configured in dynamic_resources.\nEnvoy Initialization  During startup, the cluster manager goes through a multi-phase initialization where it first initializes static/DNS clusters, then predefined EDS clusters. Then it initializes CDS if applicable, waits for one response (or failure), and does the same primary/secondary initialization of CDS provided clusters.\n If clusters use active health checking, Envoy also does a single active health check round.\n Once cluster manager initialization is done, RDS and LDS initialize (if applicable). The server doesn’t start accepting connections until there has been at least one response (or failure) for LDS/RDS requests.\n If LDS itself returns a listener that needs an RDS response, Envoy further waits until an RDS response (or failure) is received. Note that this process takes place on every future listener addition via LDS and is known as listener warming.\n After all of the previous steps have taken place, the listeners start accepting new connections. This flow ensures that during hot restart the new process is fully capable of accepting and processing new connections before the draining of the old process begins.\n  ","date":1522795843,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522795843,"objectID":"a3a9a0e201239a79a4abb589caf60042","permalink":"https://wubigo.com/post/envoy-control-plane-api-server-testing/","publishdate":"2018-04-04T06:50:43+08:00","relpermalink":"/post/envoy-control-plane-api-server-testing/","section":"post","summary":"SNAPSHOT // Snapshot is an internally consistent snapshot of xDS resources. // Consistentcy is important for the convergence as different resource types // from the snapshot may be delivered to the proxy in arbitrary order. type Snapshot struct { // Endpoints are items in the EDS response payload. Endpoints Resources // Clusters are items in the CDS response payload. Clusters Resources // Routes are items in the RDS response payload.","tags":["SERVICEMESH","MICROSERVICE"],"title":"Envoy Control Plane API Server Testing","type":"post"},{"authors":null,"categories":[],"content":"envoy.yaml.tmpl\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { host_rewrite: nginx, cluster: nginx_cluster, timeout: 60s } http_filters: - name: envoy.router clusters: - name: nginx_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.sh\n#!/bin/sh set -e echo \u0026quot;Generating envoy.yaml config file...\u0026quot; cat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME \u0026gt; /etc/envoy/envoy.yaml echo \u0026quot;Starting Envoy...\u0026quot; /usr/local/bin/envoy -c /etc/envoy/envoy.yaml -l debug  Dockerfile\nFROM envoyproxy/envoy:latest COPY /tmpl/envoy.yaml.tmpl /tmpl/envoy.yaml.tmpl COPY docker-entrypoint.sh / RUN chmod 500 /docker-entrypoint.sh RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install gettext -y ENTRYPOINT [\u0026quot;/docker-entrypoint.sh\u0026quot;]  docker build -t bigo-envoy:v1 . docker tag bigo-envoy:v1 egistry.cn-beijing.aliyuncs.com/k4s/bigo-envoy:v1 docker push registry.cn-beijing.aliyuncs.com/k4s/bigo-envoy:v1 docker network create test docker run -dit --name nginx --network test nginx:alpine docker run -it --rm --name envoy --network test -e ENVOY_LB_ALG=LEAST_REQUEST -e SERVICE_NAME=nginx bigo-envoy:v1 ENVOY_IP=$(docker inspect envoy --format='{{.NetworkSettings.Networks.test.IPAddress}}') curl $ENVOY_IP docker logs nginx | grep $ENVOY_IP 172.18.0.3 - [02/Apr/2019:05:59:12 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.47.0\u0026quot; \u0026quot;-\u0026quot;  ","date":1522638965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522638965,"objectID":"4f767043c14d8e90eaf185356e8c0958","permalink":"https://wubigo.com/post/k8s-envoy-example/","publishdate":"2018-04-02T11:16:05+08:00","relpermalink":"/post/k8s-envoy-example/","section":"post","summary":"envoy.yaml.tmpl\nadmin: access_log_path: /tmp/admin_access.log address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connection_manager config: stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { host_rewrite: nginx, cluster: nginx_cluster, timeout: 60s } http_filters: - name: envoy.router clusters: - name: nginx_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.","tags":["K8S"],"title":"K8s Envoy Example","type":"post"},{"authors":null,"categories":[],"content":" To ensure stable network ID , need to define a headless service for stateful applications\nStatefulSets are valuable for applications that require one or more of the following.\n Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates  `headless-nginx.yaml\u0026rsquo;\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \u0026quot;nginx\u0026quot; replicas: 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: nginx:1.14-alpine ports: - containerPort: 80 name: web  the .spec.selector field of a StatefulSet must match the labels of its .spec.template.metadata.labels\nkubectl apply -f headless-nginx.yaml kubectl exec -it web-0 -- nslookup nginx Name: nginx Address 1: 10.2.12.86 web-0.nginx.default.svc.cluster.local  kubectl run -it --image busybox test --restart=Never --rm nslookup web-0.nginx  enable KUBE-DNS LOG kubectl -n kube-system edit configmap coredns  Then add log in the Corefile section\n Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance }  kubectl exec busybox cat /etc/resolv.conf nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5  kubectl get svc -n kube-system -o wide|grep dns kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 41d k8s-app=kube-dns  ","date":1522574432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522574432,"objectID":"679217484bc5bb2099f6ff38a8ebe581","permalink":"https://wubigo.com/post/k8s-service-headless/","publishdate":"2018-04-01T17:20:32+08:00","relpermalink":"/post/k8s-service-headless/","section":"post","summary":"To ensure stable network ID , need to define a headless service for stateful applications\nStatefulSets are valuable for applications that require one or more of the following.\n Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates  `headless-nginx.yaml\u0026rsquo;\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .","tags":["K8S"],"title":"K8s Service Headless","type":"post"},{"authors":null,"categories":[],"content":" 术语  端点 Envoy discovers the cluster members via EDS Management server: A logical server implementing the v2 Envoy APIs Upstream: An upstream host receives connections and requests from Envoy and returns responses xDS: CDS/EDS/HDS/LDS/RLS/RDS/SDS APIs. Configuration Cache: cache Envoy configurations in memory in an attempt to provide fast response to consumer Envoys  The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.\n参数化定制Envoy镜像 clusters: - name: myapp_cluster connect_timeout: 0.25s type: STRICT_DNS dns_lookup_family: V4_ONLY lb_policy: ${ENVOY_LB_ALG} hosts: [{ socket_address: { address: ${SERVICE_NAME}, port_value: 80 }}]  docker-entrypoint.shin 做环境变量替换\n#!/bin/sh set -e echo \u0026quot;Generating envoy.yaml config file...\u0026quot; cat /tmpl/envoy.yaml.tmpl | envsubst \\$ENVOY_LB_ALG,\\$SERVICE_NAME \u0026gt; /etc/envoy.yaml echo \u0026quot;Starting Envoy...\u0026quot; /usr/local/bin/envoy -c /etc/envoy.yaml  Dockerfile\nFROM envoyproxy/envoy:latest COPY envoy.yaml /tmpl/envoy.yaml.tmpl COPY docker-entrypoint.sh / RUN chmod 500 /docker-entrypoint.sh RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install gettext -y ENTRYPOINT [\u0026quot;/docker-entrypoint.sh\u0026quot;]  设置时间 docker history --no-trunc envoyproxy/envoy-dev:48082bcd22fe9165eb73bed6d27857f578df63b5  Dockerfile\nFROM envoyproxy/envoy-dev:48082bcd22fe9165eb73bed6d27857f578df63b5 COPY envoy.yaml /etc/envoy/envoy.yaml RUN apt-get update \u0026amp;\u0026amp; apt-get install -y curl ethtool tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai # CMD [\u0026quot;envoy\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;/etc/envoy/envoy.yaml\u0026quot;, \u0026quot;-l\u0026quot;, \u0026quot;debug\u0026quot;]  docker build -t envoy:v1 .  docker run -d --rm --name envoy -p 9901:9901 -p 10000:10000 envoy:v1 envoy -c /etc/envoy/envoy.yaml -l debug docker exec -it envoy bash #ps fax 1 ? Ssl 0:00 envoy -c /etc/envoy/envoy.yaml -l debug  ENVOY配置 Envoy supports multiple configurations:\n static configuration API-based configuration service-discovery-based configuration     资源类别      listeners 暴露给外部客户的端点   cluster 后台服务集群     集群  Clusters are composed of endpoints – a set of network locations that can serve requests for the cluster. Endpoints can also be defined directly as socket addresses, or read dynamically via the Endpoint Discovery Service\n监听器  监听过滤器（内置）\n envoy.client_ssl_auth envoy.echo envoy.http_connection_manager(代理HTTP请求)  http_connection_manager.v2.HttpFilter  envoy.buffer envoy.cors envoy.fault envoy.gzip envoy.http_dynamo_filter envoy.grpc_http1_bridge envoy.grpc_json_transcoder envoy.grpc_web envoy.health_check envoy.header_to_metadata envoy.ip_tagging envoy.lua envoy.rate_limit envoy.router envoy.squash   envoy.mongo_proxy envoy.ratelimit envoy.redis_proxy envoy.tcp_proxy   route_config: virtual_hosts: domains: -\u0026gt; matched against the http requests Host header  config envoy by following its api api document is automatically generated from protocol buffers\nhttps://www.envoyproxy.io/docs/envoy/v1.8.0/api-v2/api\n以上都是静态资源配置，但是在K8S环境，容器是动态分配的，手动配置无法 保证配置信息同步。于是就需要服务发现功能。ENVOY所需的发现服务包括:\n routes (“what cluster should requests with this HTTP header go to”)[RDS] clusters (“what backends does this service have?”)[CDS] listener (the filters for a port)[LDS] endpoints[EDS]\n v1\n  XDS = [ RDS, CDS, LDS, and EDS]   v2  Health Discovery Service (HDS)\nAggregated Discovery Service (ADS)\nSecret Discovery Service (SDS)\n CDS type\nCluster.DiscoveryType\n STATIC STRICT_DNS LOGICAL_DNS\n EDS ⁣ ORIGINAL_DST    clusters: - name: service_backend type: []  istio-pilot是ENVOY发现服务提供者之一，istio-pilot根据K8S API为envoy提供配置routes和clusters服务\n/envoy/examples/front-proxy$ git diff --word-diff diff --git a/examples/front-proxy/Dockerfile-frontenvoy b/examples/front-proxy/Dockerfile-frontenvoy index 83b5ba806..2e203a204 100644 --- a/examples/front-proxy/Dockerfile-frontenvoy +++ b/examples/front-proxy/Dockerfile-frontenvoy @@ -1,5 +1,5 @@ FROM envoyproxy/envoy-dev:latest RUN apt-get update \u0026amp;\u0026amp; apt-get -q install -y \\ curl {+tzdata+} CMD /usr/local/bin/envoy -c /etc/front-envoy.yaml {+-l debug+} --service-cluster front-proxy diff --git a/examples/front-proxy/Dockerfile-service b/examples/front-proxy/Dockerfile-service index c3f5bafef..987b21814 100644 --- a/examples/front-proxy/Dockerfile-service +++ b/examples/front-proxy/Dockerfile-service @@ -1,6 +1,6 @@ FROM envoyproxy/envoy-alpine-dev:latest RUN apk update \u0026amp;\u0026amp; apk add python3 bash curl {+tzdata+} RUN pip3 install -q Flask==0.11.1 requests==2.18.4 RUN mkdir /code ADD ./service.py /code diff --git a/examples/front-proxy/docker-compose.yml b/examples/front-proxy/docker-compose.yml index 2c121d598..05d7eb844 100644 --- a/examples/front-proxy/docker-compose.yml +++ b/examples/front-proxy/docker-compose.yml @@ -15,6 +15,8 @@ services: ports: - \u0026quot;8000:80\u0026quot; - \u0026quot;8001:8001\u0026quot; {+environment:+} {+ - TZ=Asia/Shanghai+} service1: build: @@ -28,8 +30,10 @@ services: - service1 environment: - SERVICE_NAME=1 {+- TZ=Asia/Shanghai+} expose: - \u0026quot;80\u0026quot; service2: build: @@ -43,6 +47,7 @@ services: - service2 environment: - SERVICE_NAME=2 {+- TZ=Asia/Shanghai+} expose: - \u0026quot;80\u0026quot; diff --git a/examples/front-proxy/start_service.sh b/examples/front-proxy/start_service.sh index cc529bcf2..57176eff3 100644 --- a/examples/front-proxy/start_service.sh +++ b/examples/front-proxy/start_service.sh @@ -1,3 +1,3 @@ #!/bin/sh python3 /code/service.py \u0026amp; envoy -c /etc/service-envoy.yaml {+-l debug+} --service-cluster service${SERVICE_NAME}  https://jvns.ca/blog/2018/10/27/envoy-basics/\nhttps://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a\n","date":1522466210,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522466210,"objectID":"b6ffd8cd9ba29e52ef56662c94e592d1","permalink":"https://wubigo.com/post/envoy-notes/","publishdate":"2018-03-31T11:16:50+08:00","relpermalink":"/post/envoy-notes/","section":"post","summary":"术语  端点 Envoy discovers the cluster members via EDS Management server: A logical server implementing the v2 Envoy APIs Upstream: An upstream host receives connections and requests from Envoy and returns responses xDS: CDS/EDS/HDS/LDS/RLS/RDS/SDS APIs. Configuration Cache: cache Envoy configurations in memory in an attempt to provide fast response to consumer Envoys  The simplest way to use Envoy without providing the control plane in the form of a dynamic API is to add the hardcoded configuration to a static yaml file.","tags":["K8S"],"title":"Envoy NOTES","type":"post"},{"authors":null,"categories":[],"content":" 禁止启动时候恢复页面提示 Version 73.0.3683.86 (Official Build) (64-bit)\n 方法1（可靠）  首先关闭chrome，然后修改下面的设置，修改完后重启\n.config/google-chrome/Default/Preferences\nsed -i '/exit_type:Crashed/exit_type:Normal/`  windows用户请参考下面的视频\nChrome Didn\u0026rsquo;t Shut Down Correctly Error Solved Windows 7\n 方法2（不可靠）  Type in address bar (Crtl+L).\nchrome://flags/#infinite-session-restore  Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\nenter password to unlock your keyring  方法1（可靠）\n set password-store to basic\n  dpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.local/share/applications  修改.local/share/applications/google-chrome.desktop\nExec=/usr/bin/google-chrome-stable --password-store=basic %U   方法2（不可靠）\n seahorse\n  seahorse  选择login，右键删除\nHow to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\n","date":1522448437,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522448437,"objectID":"baacc8852cddb889f5f8b0f25941eca6","permalink":"https://wubigo.com/post/linux-chrome/","publishdate":"2018-03-31T06:20:37+08:00","relpermalink":"/post/linux-chrome/","section":"post","summary":"禁止启动时候恢复页面提示 Version 73.0.3683.86 (Official Build) (64-bit)\n 方法1（可靠）  首先关闭chrome，然后修改下面的设置，修改完后重启\n.config/google-chrome/Default/Preferences\nsed -i '/exit_type:Crashed/exit_type:Normal/`  windows用户请参考下面的视频\nChrome Didn\u0026rsquo;t Shut Down Correctly Error Solved Windows 7\n 方法2（不可靠）  Type in address bar (Crtl+L).\nchrome://flags/#infinite-session-restore  Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\nenter password to unlock your keyring  方法1（可靠）\n set password-store to basic\n  dpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.","tags":["WEB","LINUX"],"title":"Linux Chrome","type":"post"},{"authors":null,"categories":[],"content":"helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=false --set sidecarInjectorWebhook.enabled=false [debug] Created tunnel using local port: '44471' [debug] SERVER: \u0026quot;127.0.0.1:44471\u0026quot; [debug] Original chart version: \u0026quot;\u0026quot; [debug] CHART PATH: /home/bigo/istio/install/kubernetes/helm/istio NAME: istio REVISION: 1 RELEASED: Sat Mar 30 06:30:03 2019 CHART: istio-1.0.6 USER-SUPPLIED VALUES: galley: enabled: false gateways: istio-egressgateway: enabled: false istio-ingressgateway: enabled: false global: proxy: envoyStatsd: enabled: false ingress: enabled: false mixer: enabled: false pilot: sidecar: false prometheus: enabled: false security: enabled: false sidecarInjectorWebhook: enabled: false COMPUTED VALUES: certmanager: enabled: false hub: quay.io/jetstack resources: {} tag: v0.3.1 galley: enabled: false image: galley replicaCount: 1 gateways: enabled: true global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily istio-egressgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-egressgateway istio: egressgateway ports: - name: http2 port: 80 - name: https port: 443 replicaCount: 1 secretVolumes: - mountPath: /etc/istio/egressgateway-certs name: egressgateway-certs secretName: istio-egressgateway-certs - mountPath: /etc/istio/egressgateway-ca-certs name: egressgateway-ca-certs secretName: istio-egressgateway-ca-certs serviceAnnotations: {} type: ClusterIP istio-ilbgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-ilbgateway istio: ilbgateway loadBalancerIP: \u0026quot;\u0026quot; ports: - name: grpc-pilot-mtls port: 15011 - name: grpc-pilot port: 15010 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: tcp-dns port: 853 replicaCount: 1 resources: requests: cpu: 800m memory: 512Mi secretVolumes: - mountPath: /etc/istio/ilbgateway-certs name: ilbgateway-certs secretName: istio-ilbgateway-certs - mountPath: /etc/istio/ilbgateway-ca-certs name: ilbgateway-ca-certs secretName: istio-ilbgateway-ca-certs serviceAnnotations: cloud.google.com/load-balancer-type: internal type: LoadBalancer istio-ingressgateway: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: false labels: app: istio-ingressgateway istio: ingressgateway loadBalancerIP: \u0026quot;\u0026quot; ports: - name: http2 nodePort: 31380 port: 80 targetPort: 80 - name: https nodePort: 31390 port: 443 - name: tcp nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: tcp-dns-tls port: 853 targetPort: 853 - name: http2-prometheus port: 15030 targetPort: 15030 - name: http2-grafana port: 15031 targetPort: 15031 replicaCount: 1 resources: {} secretVolumes: - mountPath: /etc/istio/ingressgateway-certs name: ingressgateway-certs secretName: istio-ingressgateway-certs - mountPath: /etc/istio/ingressgateway-ca-certs name: ingressgateway-ca-certs secretName: istio-ingressgateway-ca-certs serviceAnnotations: {} type: LoadBalancer global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily grafana: accessMode: ReadWriteMany enabled: false image: repository: grafana/grafana tag: 5.2.3 persist: false replicaCount: 1 security: enabled: false passphraseKey: passphrase secretName: grafana usernameKey: username service: annotations: {} externalPort: 3000 internalPort: 3000 name: http type: ClusterIP storageClassName: \u0026quot;\u0026quot; ingress: autoscaleMax: 5 autoscaleMin: 1 enabled: false replicaCount: 1 service: annotations: {} loadBalancerIP: \u0026quot;\u0026quot; ports: - name: http nodePort: 32000 port: 80 - name: https port: 443 selector: istio: ingress type: LoadBalancer kiali: dashboard: passphraseKey: passphrase secretName: kiali usernameKey: username enabled: false hub: docker.io/kiali ingress: annotations: null enabled: false tls: null replicaCount: 1 tag: v0.12 mixer: autoscaleMax: 5 autoscaleMin: 1 enabled: false env: GODEBUG: gctrace=2 image: mixer istio-policy: autoscaleEnabled: true autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 istio-telemetry: autoscaleEnabled: true autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 prometheusStatsdExporter: hub: docker.io/prom tag: v0.6.0 replicaCount: 1 pilot: autoscaleMax: 5 autoscaleMin: 1 cpu: targetAverageUtilization: 80 enabled: true env: GODEBUG: gctrace=2 PILOT_PUSH_THROTTLE_COUNT: 100 global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily image: pilot replicaCount: 1 resources: requests: cpu: 500m memory: 2048Mi sidecar: false traceSampling: 1 prometheus: enabled: false hub: docker.io/prom replicaCount: 1 service: annotations: {} nodePort: enabled: false port: 32090 tag: v2.3.1 security: enabled: false image: citadel replicaCount: 1 selfSigned: true servicegraph: enabled: false image: servicegraph ingress: annotations: null enabled: false hosts: - servicegraph.local tls: null prometheusAddr: http://prometheus:9090 replicaCount: 1 service: annotations: {} externalPort: 8088 internalPort: 8088 name: http type: ClusterIP sidecarInjectorWebhook: enableNamespacesByDefault: false enabled: false image: sidecar_injector replicaCount: 1 telemetry-gateway: gatewayName: ingressgateway global: arch: amd64: 2 ppc64le: 2 s390x: 2 configValidation: true controlPlaneSecurityEnabled: false crds: true defaultResources: requests: cpu: 10m disablePolicyChecks: false enableTracing: true hub: gcr.io/istio-release hyperkube: hub: quay.io/coreos tag: v1.7.6_coreos.0 imagePullPolicy: IfNotPresent imagePullSecrets: null k8sIngressHttps: false k8sIngressSelector: ingress meshExpansion: false meshExpansionILB: false mtls: enabled: false oneNamespace: false policyCheckFailOpen: false priorityClassName: \u0026quot;\u0026quot; proxy: accessLogFile: /dev/stdout autoInject: enabled concurrency: 0 discoveryDomain: \u0026quot;\u0026quot; enableCoreDump: false envoyStatsd: enabled: false host: null port: null excludeIPRanges: \u0026quot;\u0026quot; excludeInboundPorts: \u0026quot;\u0026quot; image: proxyv2 includeIPRanges: '*' includeInboundPorts: '*' privileged: false proxyDomain: \u0026quot;\u0026quot; readinessFailureThreshold: 30 readinessInitialDelaySeconds: 1 readinessPeriodSeconds: 2 resources: requests: cpu: 10m stats: prometheusPort: 15090 statusPort: 0 proxy_init: image: proxy_init tag: release-1.0-latest-daily grafanaEnabled: false prometheusEnabled: false tracing: enabled: false ingress: annotations: null enabled: false hosts: - tracing.local tls: null jaeger: hub: docker.io/jaegertracing ingress: annotations: null enabled: false hosts: - jaeger.local tls: null memory: max_traces: 50000 tag: 1.5 ui: port: 16686 provider: jaeger replicaCount: 1 service: annotations: {} externalPort: 9411 internalPort: 9411 name: http type: ClusterIP HOOKS: --- # virtualservices.networking.istio.io # # these CRDs only make sense when pilot is enabled # apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: virtualservices.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: VirtualService listKind: VirtualServiceList plural: virtualservices singular: virtualservice categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # destinationrules.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: destinationrules.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: DestinationRule listKind: DestinationRuleList plural: destinationrules singular: destinationrule categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # serviceentries.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: serviceentries.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: ServiceEntry listKind: ServiceEntryList plural: serviceentries singular: serviceentry categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # gateways.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: gateways.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install \u0026quot;helm.sh/hook-weight\u0026quot;: \u0026quot;-5\u0026quot; labels: app: istio-pilot spec: group: networking.istio.io names: kind: Gateway plural: gateways singular: gateway categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 --- # envoyfilters.networking.istio.io apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: envoyfilters.networking.istio.io annotations: \u0026quot;helm.sh/hook\u0026quot;: crd-install labels: app: istio-pilot spec: group: networking.istio.io names: kind: EnvoyFilter plural: envoyfilters singular: envoyfilter categories: - istio-io - networking-istio-io scope: Namespaced version: v1alpha3 MANIFEST: --- # Source: istio/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: istio namespace: istio-system labels: app: istio chart: istio-1.0.6 release: istio heritage: Tiller data: mesh: |- # Set the following variable to true to disable policy checks by the Mixer. # Note that metrics will still be reported to the Mixer. disablePolicyChecks: false # Set enableTracing to false to disable request tracing. enableTracing: true # Set accessLogFile to empty string to disable access log. accessLogFile: \u0026quot;/dev/stdout\u0026quot; # # Deprecated: mixer is using EDS # Unix Domain Socket through which envoy communicates with NodeAgent SDS to get # key/cert for mTLS. Use secret-mount files instead of SDS if set to empty. sdsUdsPath: \u0026quot;\u0026quot; # How frequently should Envoy fetch key/cert from NodeAgent. sdsRefreshDelay: 15s # defaultConfig: # # TCP connection timeout between Envoy \u0026amp; the application, and between Envoys. connectTimeout: 10s # ### ADVANCED SETTINGS ############# # Where should envoy's configuration be stored in the istio-proxy container configPath: \u0026quot;/etc/istio/proxy\u0026quot; binaryPath: \u0026quot;/usr/local/bin/envoy\u0026quot; # The pseudo service name used for Envoy. serviceCluster: istio-proxy # These settings that determine how long an old Envoy # process should be kept alive after an occasional reload. drainDuration: 45s parentShutdownDuration: 1m0s # # The mode used to redirect inbound connections to Envoy. This setting # has no effect on outbound traffic: iptables REDIRECT is always used for # outbound connections. # If \u0026quot;REDIRECT\u0026quot;, use iptables REDIRECT to NAT and redirect to Envoy. # The \u0026quot;REDIRECT\u0026quot; mode loses source addresses during redirection. # If \u0026quot;TPROXY\u0026quot;, use iptables TPROXY to redirect to Envoy. # The \u0026quot;TPROXY\u0026quot; mode preserves both the source and destination IP # addresses and ports, so that they can be used for advanced filtering # and manipulation. # The \u0026quot;TPROXY\u0026quot; mode also configures the sidecar to run with the # CAP_NET_ADMIN capability, which is required to use TPROXY. #interceptionMode: REDIRECT # # Port where Envoy listens (on local host) for admin commands # You can exec into the istio-proxy container in a pod and # curl the admin port (curl http://localhost:15000/) to obtain # diagnostic information from Envoy. See # https://lyft.github.io/envoy/docs/operations/admin.html # for more details proxyAdminPort: 15000 # # Set concurrency to a specific number to control the number of Proxy worker threads. # If set to 0 (default), then start worker thread for each CPU thread/core. concurrency: 0 # # Zipkin trace collector zipkinAddress: zipkin.istio-system:9411 # # Mutual TLS authentication between sidecars and istio control plane. controlPlaneAuthPolicy: NONE # # Address where istio Pilot service is running discoveryAddress: istio-pilot.istio-system:15007 --- # Source: istio/templates/sidecar-injector-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: istio-sidecar-injector namespace: istio-system labels: app: istio chart: istio-1.0.6 release: istio heritage: Tiller istio: sidecar-injector data: config: |- policy: enabled template: |- initContainers: - name: istio-init image: \u0026quot;gcr.io/istio-release/proxy_init:release-1.0-latest-daily\u0026quot; args: - \u0026quot;-p\u0026quot; - [[ .MeshConfig.ProxyListenPort ]] - \u0026quot;-u\u0026quot; - 1337 - \u0026quot;-m\u0026quot; - [[ annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode ]] - \u0026quot;-i\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeOutboundIPRanges` \u0026quot;*\u0026quot; ]]\u0026quot; - \u0026quot;-x\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/excludeOutboundIPRanges` \u0026quot;\u0026quot; ]]\u0026quot; - \u0026quot;-b\u0026quot; - \u0026quot;[[ annotation .ObjectMeta `traffic.sidecar.istio.io/includeInboundPorts` (includeInboundPorts .Spec.Containers) ]]\u0026quot; - \u0026quot;-d\u0026quot; - \u0026quot;[[ excludeInboundPort (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) (annotation .ObjectMeta `traffic.sidecar.istio.io/excludeInboundPorts` \u0026quot;\u0026quot; ) ]]\u0026quot; imagePullPolicy: IfNotPresent securityContext: capabilities: add: - NET_ADMIN privileged: true restartPolicy: Always containers: - name: istio-proxy image: [[ annotation .ObjectMeta `sidecar.istio.io/proxyImage` \u0026quot;gcr.io/istio-release/proxyv2:release-1.0-latest-daily\u0026quot; ]] ports: - containerPort: 15090 protocol: TCP name: http-envoy-prom args: - proxy - sidecar - --configPath - [[ .ProxyConfig.ConfigPath ]] - --binaryPath - [[ .ProxyConfig.BinaryPath ]] - --serviceCluster [[ if ne \u0026quot;\u0026quot; (index .ObjectMeta.Labels \u0026quot;app\u0026quot;) -]] - [[ index .ObjectMeta.Labels \u0026quot;app\u0026quot; ]] [[ else -]] - \u0026quot;istio-proxy\u0026quot; [[ end -]] - --drainDuration - [[ formatDuration .ProxyConfig.DrainDuration ]] - --parentShutdownDuration - [[ formatDuration .ProxyConfig.ParentShutdownDuration ]] - --discoveryAddress - [[ annotation .ObjectMeta `sidecar.istio.io/discoveryAddress` .ProxyConfig.DiscoveryAddress ]] - --discoveryRefreshDelay - [[ formatDuration .ProxyConfig.DiscoveryRefreshDelay ]] - --zipkinAddress - [[ .ProxyConfig.ZipkinAddress ]] - --connectTimeout - [[ formatDuration .ProxyConfig.ConnectTimeout ]] - --proxyAdminPort - [[ .ProxyConfig.ProxyAdminPort ]] [[ if gt .ProxyConfig.Concurrency 0 -]] - --concurrency - [[ .ProxyConfig.Concurrency ]] [[ end -]] - --controlPlaneAuthPolicy - [[ annotation .ObjectMeta `sidecar.istio.io/controlPlaneAuthPolicy` .ProxyConfig.ControlPlaneAuthPolicy ]] [[- if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \u0026quot;0\u0026quot;) ]] - --statusPort - [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] - --applicationPorts - \u0026quot;[[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/applicationPorts` (applicationPorts .Spec.Containers) ]]\u0026quot; [[- end ]] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: [[ or (index .ObjectMeta.Annotations \u0026quot;sidecar.istio.io/interceptionMode\u0026quot;) .ProxyConfig.InterceptionMode.String ]] [[ if .ObjectMeta.Annotations ]] - name: ISTIO_METAJSON_ANNOTATIONS value: | [[ toJson .ObjectMeta.Annotations ]] [[ end ]] [[ if .ObjectMeta.Labels ]] - name: ISTIO_METAJSON_LABELS value: | [[ toJson .ObjectMeta.Labels ]] [[ end ]] imagePullPolicy: IfNotPresent [[ if (ne (annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ) \u0026quot;0\u0026quot;) ]] readinessProbe: httpGet: path: /healthz/ready port: [[ annotation .ObjectMeta `status.sidecar.istio.io/port` 0 ]] initialDelaySeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/initialDelaySeconds` 1 ]] periodSeconds: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/periodSeconds` 2 ]] failureThreshold: [[ annotation .ObjectMeta `readiness.status.sidecar.istio.io/failureThreshold` 30 ]] [[ end -]]securityContext: readOnlyRootFilesystem: true [[ if eq (annotation .ObjectMeta `sidecar.istio.io/interceptionMode` .ProxyConfig.InterceptionMode) \u0026quot;TPROXY\u0026quot; -]] capabilities: add: - NET_ADMIN runAsGroup: 1337 [[ else -]] runAsUser: 1337 [[ end -]] restartPolicy: Always resources: [[ if (isset .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU`) -]] requests: cpu: \u0026quot;[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyCPU` ]]\u0026quot; memory: \u0026quot;[[ index .ObjectMeta.Annotations `sidecar.istio.io/proxyMemory` ]]\u0026quot; [[ else -]] requests: cpu: 10m [[ end -]] volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true [[ if eq .Spec.ServiceAccountName \u0026quot;\u0026quot; -]] secretName: istio.default [[ else -]] secretName: [[ printf \u0026quot;istio.%s\u0026quot; .Spec.ServiceAccountName ]] [[ end -]] --- # Source: istio/charts/pilot/templates/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: istio-pilot-service-account namespace: istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio --- # Source: istio/charts/pilot/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: istio-pilot-istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio rules: - apiGroups: [\u0026quot;config.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;rbac.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: [\u0026quot;networking.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;authentication.istio.io\u0026quot;] resources: [\u0026quot;*\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;apiextensions.k8s.io\u0026quot;] resources: [\u0026quot;customresourcedefinitions\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;] resources: [\u0026quot;thirdpartyresources\u0026quot;, \u0026quot;thirdpartyresources.extensions\u0026quot;, \u0026quot;ingresses\u0026quot;, \u0026quot;ingresses/status\u0026quot;] verbs: [\u0026quot;*\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;, \u0026quot;pods\u0026quot;, \u0026quot;services\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;namespaces\u0026quot;, \u0026quot;nodes\u0026quot;, \u0026quot;secrets\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] --- # Source: istio/charts/pilot/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: istio-pilot-istio-system labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: istio-pilot-istio-system subjects: - kind: ServiceAccount name: istio-pilot-service-account namespace: istio-system --- # Source: istio/charts/pilot/templates/service.yaml apiVersion: v1 kind: Service metadata: name: istio-pilot namespace: istio-system labels: app: istio-pilot chart: pilot-1.0.6 release: istio heritage: Tiller spec: ports: - port: 15010 name: grpc-xds # direct - port: 15011 name: https-xds # mTLS - port: 8080 name: http-legacy-discovery # direct - port: 9093 name: http-monitoring selector: istio: pilot --- # Source: istio/charts/pilot/templates/deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: istio-pilot namespace: istio-system # TODO: default template doesn't have this, which one is right ? labels: app: istio-pilot chart: pilot-1.0.6 release: istio heritage: Tiller istio: pilot annotations: checksum/config-volume: f8da08b6b8c170dde721efd680270b2901e750d4aa186ebb6c22bef5b78a43f9 spec: replicas: 1 template: metadata: labels: istio: pilot app: pilot annotations: sidecar.istio.io/inject: \u0026quot;false\u0026quot; scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; spec: serviceAccountName: istio-pilot-service-account containers: - name: discovery image: \u0026quot;gcr.io/istio-release/pilot:release-1.0-latest-daily\u0026quot; imagePullPolicy: IfNotPresent args: - \u0026quot;discovery\u0026quot; - --secureGrpcAddr - \u0026quot;:15011\u0026quot; ports: - containerPort: 8080 - containerPort: 15010 - containerPort: 15011 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 30 timeoutSeconds: 5 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: PILOT_CACHE_SQUASH value: \u0026quot;5\u0026quot; - name: GODEBUG value: \u0026quot;gctrace=2\u0026quot; - name: PILOT_PUSH_THROTTLE_COUNT value: \u0026quot;100\u0026quot; - name: PILOT_TRACE_SAMPLING value: \u0026quot;1\u0026quot; resources: requests: cpu: 500m memory: 2048Mi volumeMounts: - name: config-volume mountPath: /etc/istio/config - name: istio-certs mountPath: /etc/certs readOnly: true volumes: - name: config-volume configMap: name: istio - name: istio-certs secret: secretName: istio.istio-pilot-service-account optional: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - ppc64le - s390x preferredDuringSchedulingIgnoredDuringExecution: - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - ppc64le - weight: 2 preference: matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - s390x --- # Source: istio/templates/crds.yaml # # these CRDs only make sense when security is enabled # # # --- # Source: istio/charts/pilot/templates/gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-autogenerated-k8s-ingress namespace: istio-system spec: selector: istio: ingress servers: - port: number: 80 protocol: HTTP2 name: http hosts: - \u0026quot;*\u0026quot; --- # Source: istio/charts/pilot/templates/autoscale.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: istio-pilot namespace: istio-system spec: maxReplicas: 5 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1beta1 kind: Deployment name: istio-pilot metrics: - type: Resource resource: name: cpu targetAverageUtilization: 80 LAST DEPLOYED: Sat Mar 30 06:30:03 2019 NAMESPACE: istio-system STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/ConfigMap NAME DATA AGE istio 1 4s istio-sidecar-injector 1 4s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-pilot ClusterIP 10.106.159.203 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 3s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE istio-pilot 1 1 1 0 3s ==\u0026gt; v1alpha3/Gateway NAME AGE istio-autogenerated-k8s-ingress 2s ==\u0026gt; v2beta1/HorizontalPodAutoscaler NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-pilot Deployment/istio-pilot \u0026lt;unknown\u0026gt;/80% 1 5 0 2s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE istio-pilot-754ccc994f-t7wg2 0/1 ContainerCreating 0 3s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE istio-pilot-service-account 1 5s ==\u0026gt; v1beta1/ClusterRole NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE istio-pilot-istio-system 4s  ","date":1522362783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522362783,"objectID":"83441f2c09ede7e7eb3f86ac3332b0a3","permalink":"https://wubigo.com/post/helm-chart-istio/","publishdate":"2018-03-30T06:33:03+08:00","relpermalink":"/post/helm-chart-istio/","section":"post","summary":"helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=false --set sidecarInjectorWebhook.enabled=false [debug] Created tunnel using local port: '44471' [debug] SERVER: \u0026quot;127.0.0.1:44471\u0026quot; [debug] Original chart version: \u0026quot;\u0026quot; [debug] CHART PATH: /home/bigo/istio/install/kubernetes/helm/istio NAME: istio REVISION: 1 RELEASED: Sat Mar 30 06:30:03 2019 CHART: istio-1.0.6 USER-SUPPLIED VALUES: galley: enabled: false gateways: istio-egressgateway: enabled: false istio-ingressgateway: enabled: false global: proxy: envoyStatsd: enabled: false ingress: enabled: false mixer: enabled: false pilot: sidecar: false prometheus: enabled: false security: enabled: false sidecarInjectorWebhook: enabled: false COMPUTED VALUES: certmanager: enabled: false hub: quay.","tags":[],"title":"Helm Chart Istio","type":"post"},{"authors":null,"categories":[],"content":" for developers building applications to run in Kubernetes clusters, and for DevOps staff troubleshooting Kubernetes applications. Features include:\n View your clusters in an explorer tree view, and drill into workloads, services, pods and nodes. Browse Helm repos and install charts into your Kubernetes cluster. Intellisense for Kubernetes resources and Helm charts and templates. Edit Kubernetes resource manifests and apply them to your cluster. Build and run containers in your cluster from Dockerfiles in your project. View diffs of a resource\u0026rsquo;s current state against the resource manifest in your Git repo Easily check out the Git commit corresponding to a deployed application. Run commands or start a shell within your application\u0026rsquo;s pods. Get or follow logs and events from your clusters. Forward local ports to your application\u0026rsquo;s pods. Create Helm charts using scaffolding and snippets. Bootstrap applications using Draft, and rapidly deploy and debug them to speed up the development loop.  Dependencies docker if you plan to use the extension to build applications rather than only browse.\n kubectl docker helm draft  ","date":1521846156,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521846156,"objectID":"358c4a7d223f3c18c026e34b138e2225","permalink":"https://wubigo.com/post/k8s-ide-tool/","publishdate":"2018-03-24T07:02:36+08:00","relpermalink":"/post/k8s-ide-tool/","section":"post","summary":"for developers building applications to run in Kubernetes clusters, and for DevOps staff troubleshooting Kubernetes applications. Features include:\n View your clusters in an explorer tree view, and drill into workloads, services, pods and nodes. Browse Helm repos and install charts into your Kubernetes cluster. Intellisense for Kubernetes resources and Helm charts and templates. Edit Kubernetes resource manifests and apply them to your cluster. Build and run containers in your cluster from Dockerfiles in your project.","tags":["K8S","IDE","APP"],"title":"K8s IDE Tool: code extension","type":"post"},{"authors":null,"categories":[],"content":" go doc https://golang.google.cn\nproxy  从 Github 的代码库 clone  go get -u github.com/golang/text mv $GOPATH/src/github.com/golang/text $GOPATH/src/golang.org/x/text go get -u github.com/golang/crypto mv $GOPATH/src/github.com/golang/crypto $GOPATH/src/golang.org/x/crypto   设置 GOPROXY 环境变量配置代理  例如：GOPROXY=https://goproxy.io\nhttps://github.com/northbright/Notes/blob/master/Golang/china/get-golang-packages-on-golang-org-in-china.md\nhttps://gocn.vip/article/1678\n","date":1521700441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521700441,"objectID":"663c892073eaaac75f362b8fd2fa17b7","permalink":"https://wubigo.com/post/lang-go-firewall/","publishdate":"2018-03-22T14:34:01+08:00","relpermalink":"/post/lang-go-firewall/","section":"post","summary":"go doc https://golang.google.cn\nproxy  从 Github 的代码库 clone  go get -u github.com/golang/text mv $GOPATH/src/github.com/golang/text $GOPATH/src/golang.org/x/text go get -u github.com/golang/crypto mv $GOPATH/src/github.com/golang/crypto $GOPATH/src/golang.org/x/crypto   设置 GOPROXY 环境变量配置代理  例如：GOPROXY=https://goproxy.io\nhttps://github.com/northbright/Notes/blob/master/Golang/china/get-golang-packages-on-golang-org-in-china.md\nhttps://gocn.vip/article/1678","tags":["LANG","GO"],"title":"Go穿越Firewall","type":"post"},{"authors":null,"categories":[],"content":" v3.11.0-\u0026gt;k8s 1.11\nopenshift all-in-one curl https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar zxf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz cd openshift export PATH=\u0026quot;$(pwd)\u0026quot;:$PATH sudo ./openshift start master  oc setup export KUBECONFIG=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig export CURL_CA_BUNDLE=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/ca.crt sudo chmod +r \u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig openshift complition bash \u0026gt; /usr/share/bash-completion/completions/openshift.complition.sh  master and node configuration after installation /etc/origin/master/master-config.yaml\nidentityProviders: - name: my_allow_provider challenge: true login: true provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider corsAllowedOrigins  Identity Providers The OpenShift master includes a built-in OAuth server the Deny All identity provider is used by default, which denies access for all user names and passwords. To allow access, you must choose a different identity provider and configure the master configuration file appropriately (located at /etc/openshift/master/master-config.yaml by default).\noc login -u system:admin  ","date":1521542857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521542857,"objectID":"f826095e87e398e9c2bf254e188605a6","permalink":"https://wubigo.com/post/k8s-openshift/","publishdate":"2018-03-20T18:47:37+08:00","relpermalink":"/post/k8s-openshift/","section":"post","summary":"v3.11.0-\u0026gt;k8s 1.11\nopenshift all-in-one curl https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar zxf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz cd openshift export PATH=\u0026quot;$(pwd)\u0026quot;:$PATH sudo ./openshift start master  oc setup export KUBECONFIG=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig export CURL_CA_BUNDLE=\u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/ca.crt sudo chmod +r \u0026quot;$(pwd)\u0026quot;/openshift.local.config/master/admin.kubeconfig openshift complition bash \u0026gt; /usr/share/bash-completion/completions/openshift.complition.sh  master and node configuration after installation /etc/origin/master/master-config.yaml\nidentityProviders: - name: my_allow_provider challenge: true login: true provider: apiVersion: v1 kind: AllowAllPasswordIdentityProvider corsAllowedOrigins  Identity Providers The OpenShift master includes a built-in OAuth server the Deny All identity provider is used by default, which denies access for all user names and passwords.","tags":["K8S","PAAS"],"title":"K8s Openshift","type":"post"},{"authors":null,"categories":null,"content":" JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:\n Java 6 JCE\n Java 7 JCE\n Java 8 JCE\n  Connecting to SSL Server from eclipse Append the following to use keystore in eclipse tomcat server\n-Djavax.net.ssl.trustStore=\u0026quot;C:\\Program Files\\Java\\jdk1.8.0_121\\jre\\lib\\ security\u0026quot;  check certificate name by alias then remove from keystore files $keytool -list -v -keystore cacerts | grep 'Alias name:' $sudo keytool -delete -alias wubigo -keystore cacerts  ","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"148bab8c32d2df718a63a191be71e3f6","permalink":"https://wubigo.com/post/2018-03-07-connectingtosslservices/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-07-connectingtosslservices/","section":"post","summary":"JDK Version java -version openjdk version \u0026quot;1.8.0_151\u0026quot; OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12) OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)  Verify that the target server is configured to serve SSL https://www.ssllabs.com/ssltest/\nConnecting to SSL services https://confluence.atlassian.com/kb/unable-to-connect-to-ssl-services-due-to-pkix-path-building-failed-779355358.html\nIf you are getting an exception due to \u0026ldquo;Illegal key size\u0026rdquo; and you are using Sun’s JDK, you need to install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files. See the following links for more information:","tags":null,"title":"Connecting to SSL services","type":"post"},{"authors":null,"categories":["LIFE"],"content":" 导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。\n许多一流的笔杆子都不是好学生——温斯顿·邱吉尔就是一例。\n有关这个问题的解释是，笔头好的人一般不靠听和读来学习，而靠写来学习，这已成了一种规律。学校不让他们以这种方式学习，所以他们的成绩总是很糟糕。\n实际上，学习大概有六七种不同的方式。\n像邱吉尔这样的人靠写来学习，还有些人以详尽的笔记来学习。有些人在实干中学习，另一些人通过听自己讲话学习。\n我属于读者型还是听者型？我如何学习？这是你首先要问自己的问题。\n但光这些问题显然不够。要想做好自我管理，你还需要问这样的问题：我能与别人合作得好吗？还是喜欢单枪匹马？如果你确实有与别人进行合作的能力，你还得问问这个问题：我在怎样的关系下与他人共事？\n有些人最适合当部属。\n 二战时期美国的大英雄乔治·巴顿将军是一个很好的例子。 巴顿是美军的一名高级将领。然而，当有人提议他担任独立指挥官时， 美国陆军参谋长、可能也是美国历史上最成功的伯乐， 乔治·马歇尔将军说： 「巴顿是美国陆军造就的最优秀的部下，但是，他会成为最差劲的司令官。」  一些人作为团队成员工作最出色。另一些人单独工作最出色。一些人当教练和导师特别有天赋，另一些人却没能力做导师。\n另一个关键的问题是，我如何才能取得成果——是作为决策者还是作为顾问？许多人做顾问时的表现会很出色，但是不能够承担决策的负担和压力。与此相反，也有许多人需要顾问来迫使他们思考，随后他们才能做出决定，接着迅速、自信和大胆地执行决定。\n顺便说一下，一个组织的二号人物在提升到一号职位时常常失败，也正是因为这个原因。最高职位需要一个决策者，而一个强势的决策者常常把其信赖的人放在二号位置，当他的顾问。\n其他有助于认识自我的重要问题包括：\n 我是在压力下表现出色，还是适应一种按部就班、可预测的工作环境？ 我是在一个大公司还是在一个小公司中工作表现最佳？  我不止一次地看到有些人在大公司中十分成功，换到小公司中则很不顺利。\n反过来也是如此。\n下面这个结论值得我们反复强调：不要试图改变自我，因为这样你不大可能成功。但是，你应该努力改进你的工作方式。另外，不要从事你干不了或干不好的工作。\n我们的价值观是什么 要能够自我管理，你最后不得不问的问题是：我的价值观是什么？这不是一个有关伦理道德的问题。道德准则对每一个人都一样。要对一个人的道德进行测试，方法很简单，我把它称为「镜子测试」。\n 20 世纪初，德国驻英国大使是当时在伦敦所有大国中最受尊重的一位外交官。 显然，他命中注定会承担重任，即使不当本国的总理，至少也要当外交部长。 然而，在 1906 年，他突然辞职，不愿主持外交使团为英国国王爱德华七世举行的晚宴。 这位国王是一个臭名昭著的色鬼，并且明确表示他想出席什么样的晚宴。 据有关报道，这位德国大使曾说： 「我不想早晨刮脸时在镜子里看到一个皮条客。」  这就是镜子测试。\n我们所尊从的伦理道德要求你问自己：我每天早晨在镜子里想看到一个什么样的人？在一个组织或一种情形下合乎道德的行为，在另一个组织或另一种情形下也是合乎道德的。但是，道德只是价值体系的一部分——尤其对于一个组织的价值体系来说。\n如果一个组织的价值体系不为自己所接受或者与自己的价值观不相容，人们就会备感沮丧，工作效力低下。\n一个人的工作方式和他的长处很少发生冲突，相反，两者能产生互补。但是，一个人的价值观有时会与他的长处发生冲突。\n我们属于何处 少数人很早就知道他们属于何处。\n比如，数学家、音乐家和厨师，通常在四五岁的时候就知道自己会成为数学家、音乐家和厨师了。物理学家通常在十几岁甚至更早的时候就决定了自己的工作生涯。\n但是，大多数人，尤其是很有天赋的人，至少要过了二十五六岁才知道他们将身属何处。\n然而，到这个时候，他们应该知道上面所谈的三个问题的答案：\n- 我的长处是什么？ - 我的工作方式是怎样的？ - 我的价值观是什么？  随后，他们就能够并且应该决定自己该向何处投入精力。或者，他们应该能够决定自己不属于何处。\n已经知道自己在大公司里干不好的人，应该学会拒绝在一个大公司中任职。已经知道自己不适合担任决策者的人，应该学会拒绝做决策工作。\n成功的事业不是预先规划的，而是在人们知道了自己的长处、工作方式和价值观后，准备把握机遇时水到渠成的。知道自己属于何处，可使一个勤奋、有能力但原本表现平平的普通人，变成出类拔萃的工作者。\n我该做什么贡献 综观人类的发展史，绝大多数人永远都不需要提出这样一个问题：我该做出什么贡献？因为他们该做出什么贡献是由别人告知的，他们的任务或是由工作本身决定的（例如农民或工匠的任务），或是由主人决定的（例如佣人的任务）。\n对于知识工作者来说，他们不得不提出一个以前从来没有提出过的问题：我的贡献应该是什么？\n要回答这个问题，他们必须考虑三个不同的因素：\n- 当前形势的要求是什么？ - 鉴于我的长处、我的工作方式以及我的价值观，我怎样才能对需要完成的任务做出最大贡献？ - 最后，必须取得什么结果才能产生重要影响？  一般来说，一项计划的时间跨度如果超过了 18 个月，就很难做到明确和具体。\n因此，在多数情况下我们应该提出的问题是：\n- 我在哪些方面能取得将在今后一年半内见效的结果？ - 如何取得这样的结果？  回答这个问题时必须对几个方面进行权衡。\n首先，这些结果应该是比较难实现的，要有「张力」 （stretching）。但这些结果也应该是能力所及的。\n其次，这些结果应该富有意义，要能够产生一定影响。\n最后，结果应该明显可见，如果可能的话，还应当能够衡量。确定了要实现的结果之后，接着就可以制订行动方针：做什么，从何处着手，如何开始，目标是什么，在多长时间内完成。\n对人际关系负责 除了少数伟大的艺术家、科学家和运动员，很少有人是靠自己单枪匹马而取得成果的。不管是组织成员还是个体职业者，大多数人都要与别人进行合作，并且是有效的合作。要实现自我管理，你需要对自己的人际关系负起责任。这包括两部分内容。\n首先要接受别人是和你一样的个体这个事实。\n他们有自己的长处，自己的做事方式和价值观。因此，要想卓有成效，你就必须知道共事者的长处、工作方式和价值观。\n这个道理听起来让人很容易明白，但是没有几个人真正会去注意。\n一个习惯于写报告的人就是个典型的例子——他在第一份工作时就培养起写报告的习惯，因为他的老板是一个读者型的人，而即使下一个老板是个听者型，此人也会继续写着那肯定没有任何结果的报告。这位老板因此肯定会认为这个员工愚蠢、无能、懒惰，肯定干不好工作。但是，如果这个员工事先研究过新老板的情况，并分析过这位老板的工作方式，这种情况本来可以避免。\n老板既不是组织结构图上的一个头衔，也不是一个「职能」。他们是有个性的人，他们有权以自己最得心应手的方式来工作。与他们共事的人有责任观察他们，了解他们的工作方式，并做出相应的自我调整，去适应老板最有效的工作方式。\n事实上，这就是「管理」上司的秘诀\n这种方法适用于所有与你共事的人。至于工作方式，人各有别。提高效力的第一个秘诀是了解跟你合作和你要依赖的人，以利用他们的长处、工作方式和价值观。工作关系应当既以工作为基础，也以人为基础。\n人际关系责任的第二部分内容是沟通责任。\n在我或是其他人开始给一个组织做咨询时，我们听到的第一件事都与个性冲突有关。其中大部分冲突都是因为：人们不知道别人在做什么，他们又是采取怎样的工作方式，专注于做出什么样的贡献以及期望得到怎样的结果。而这些人不了解情况的原因是，他们没有去问，结果也就不得而知。\n即使一些人懂得负起人际关系责任的重要性，他们和同事的交流也往往不够。他们总是有所顾虑，怕别人把自己看成是一个冒昧、愚蠢、爱打听的人。他们错了。\n因为我们看到，每当有人找到他的同事说「这是我所擅长的工作。这是我的做事方式。这是我的价值观。这是我计划做出的贡献和应当取得的成果」，这个人总会得到如此回答：「这太有帮助了，可你为什么不早点告诉我？」\n如果一个人继续问道：「那么，关于你的长处、你的工作方式、你的价值观以及你计划做出的贡献，我需要知道什么？」他也会得到类似的答复——据我的经验，无一例外。\n事实上，知识工作者应该向与他们共事的每一个人，不管是下属、上司、同事还是团队成员，都发出这样的疑问。\n组织已不再建立在强权的基础上，而是建立在信任的基础上。人与人之间相互信任，不一定意味着他们彼此喜欢对方，而是意味着彼此了解。因此，人们绝对有必要对自己的人际关系负责。\n这是一种义务。不管一个人是公司的一名成员，还是公司的顾问、供应商或经销商，他都需要对他的所有共事者负起这种责任。所谓共事者，是指在工作上他所依赖的同事以及依赖他的同事。\n管理后半生 我们听到了许多有关经理人中年危机的谈论，「厌倦」这个词在其中频频出现。\n45 岁时，多数经理人的职业生涯达到了顶峰。但是他们学不到新东西，也没有什么新贡献，从工作中得不到挑战，因而也谈不上满足感。在他们面前，还有 20 到 25 年的职业道路要走。这就是为什么经理人在进行自我管理后，越来越多地开始发展第二职业的原因。\n发展第二职业有三种方式：\n 第一种是完全投身于新工作。  这常常只需要从一种组织转到另一种组织。\n例如，一家大公司某事业部的会计师成为一家中型医院的财务总监。\n但是也有越来越多的人转入完全不同的职业。还有许多人在第一份职业中取得的成功有限，于是改行从事第二职业。这样的人有很多技能，他们也知道该如何工作。\n为后半生做准备的第二种方式是， - 发展一个平行的职业。\n许多人的第一职业十分成功，他们还会继续从事原有工作。除此之外，他们会开创一项平行的工作，通常是在非营利机构。\n 最后一种方法是社会创业。  社会创业者通常是在第一职业中非常成功的人士。他们都热爱自己的工作，但是这种工作对他们已经不再有挑战性。\n他们虽然继续做着原来的工作，但在这份工作上花的时间越来越少。他们同时开创了另一项事业，通常是非营利性活动。\n管理好自己后半生的人可能总是少数。多数人可能数着年头一年一年过去，直至退休。但正是这些少数人，这些把漫长的工作寿命看做是自己和社会之机会的人，才会成为领袖和模范。\n管理好后半生有一个先决条件：你必须早在你进入后半生之前就开始行动。当 30 年前人们首次认识到工作寿命正在迅速延长时，许多观察家（包括我自己）认为，退休人员会越来越多地成为非营利机构的志愿者。可是，这种情况并没有发生。一个人如果不在 40 岁之前就开始做志愿者，那他 60 岁之后也不会去做志愿者。\n同样，我认识的所有社会创业者，都是早在他们原有的事业达到顶峰之前就开始从事他们的第二事业。\n发展第二兴趣还有一个原因：任何人都不能指望在生活或工作中很长时间都不遭遇严重挫折。在这样的时刻，第二兴趣——不仅仅是业余爱好——可能发挥重要作用。\n在一个崇尚成功的社会里，拥有各种选择变得越来越重要。在知识社会里，我们期望每一个人都能取得成功。这显然是不可能的。\n对许多人来说，能避免失败就行。可是有成功的地方，就会有失败。因此，有一个能够让人们做出贡献、发挥影响力或成为「大人物」的领域，这不仅对个人十分重要，对个人的家庭也同样重要。\n这意味着人们需要找到一个能够有机会成为领袖、受到尊重、取得成功的第二领域——可能是第二份职业。\n自我管理中面临的挑战看上去比较明显。但自我管理需要我们做出以前从未做过的事情。自我管理需要每一个知识工作者在思想和行动上都要成为自己的首席执行官。\n更进一步来看，这样的转变——从一切听从别人吩咐的体力劳动者到不得不自我管理的知识工作者——也使得社会结构发生了深刻变化。\n历史上每一个社会，甚至是个人主义倾向最强的社会，都认为两件事情理所当然（即使只是下意识的）：\n- 第一，组织比员工更长寿； - 第二，大多数人从不挪地方。  如今，情况恰恰相反。知识工作者的寿命超过了组织寿命，而且他们来去自如。\n于是，人们对自我管理的需要在人类事务中掀起了一场革命。\n","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"83ad231316907510eb063abd4a52482f","permalink":"https://wubigo.com/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/post/2018-03-17-%E8%87%AA%E6%88%91%E7%AE%A1%E7%90%86/","section":"post","summary":"导语：本文最早刊登于《哈佛商业评论》，是其历史最佳文章之一。 作者彼得·德鲁克是现代管理学之父，其著作影响了数代追求创新以及 最佳管理实践的学者和企业家们，各类商业管理课程也都深受彼得·德鲁克思想的影响。 本文后被收录在德鲁克的著作《21 世纪的管理挑战》中。非常值得一读再读。 作者 | 彼得·德鲁克 来源 | 《21世纪的管理挑战》第 6 章 自我管理（节选） 字数 | 7150（阅读约 10 分钟）  我们生活的时代充满着前所未有的机会：如果你有雄心，又不乏智慧，那么不管你从何处起步，你都可以沿着自己所选择的道路登上事业的顶峰。\n不过，有了机会，也就有了责任。今天的公司并不怎么管员工的职业发展；实际上，知识工作者必须成为自己的首席执行官。你应该在公司中开辟自己的天地，知道何时改变发展道路，并在可能长达 50 年的职业生涯中不断努力、干出实绩。\n要做好这些事情，你首先要对自己有深刻的认识——不仅清楚自己的优点和缺点，也知道自己是怎样学习新知识和与别人共事的，并且还明白自己的价值观是什么、自己又能在哪些方面做出最大贡献。\n因为只有当所有工作都从自己的长处着眼，你才能真正做到卓尔不群。\n历史上的伟人——拿破仑、达芬奇、莫扎特——都很善于自我管理。这在很大程度上也是他们成为伟人的原因。不过，他们属于不可多得的奇才，不但有着不同于常人的天资，而且天生就会管理自己，因而才取得了不同于常人的成就。\n而我们当中的大多数人，甚至包括那些还算有点天赋的人，都不得不通过学习来掌握自我管理的技巧。\n我们必须学会自我发展，必须知道把自己放在什么样的位置上，才能做出最大的贡献，而且还必须在长达 50 年的职业生涯中保持着高度的警觉和投入。\n我们的长处是什么 多数人都以为他们知道自己擅长什么。其实不然，更多的情况是，人们只知道自己不擅长什么——即便是在这一点上，人们也往往认识不清。\n然而，一个人要有所作为，只能靠发挥自己的长处，而如果从事自己不太擅长的工作是无法取得成就的，更不用说那些自己根本干不了的事情了。\n我们需要知己所长，才能知己所属。\n要发现自己的长处，唯一途径就是回馈分析法（feedback analysis）。每当做出重要决定或采取重要行动时，你都可以事先记录下自己对结果的预期。9 到 12 个月后，再将实际结果与自己的预期比较。\n我本人采用这种方法已有 15 到 20 年了，而每次使用都有意外的收获。\n我们只要持之以恒地运用这个简单的方法，就能在较短的时间内，发现自己的长处。在采用这种方法之后，你就能知道，自己正在做（或没有做）的哪些事情会让你的长处无法发挥出来。\n同时，你也将看到自己在哪些方面能力不是特别强。最后，你还将了解到自己在哪些方面完全不擅长，做不出成绩来。\n根据回馈分析的启示，你需要在几方面采取行动。\n首先最重要的是，专注于你的长处，把自己放到那些能发挥长处的地方。\n其次，加强你的长处。回馈分析会迅速地显示，你在哪些方面需要改善自己的技能或学习新技能。它还将显示你在知识上的差距——这些差距通常都可以弥补。\n第三，发现任何由于恃才傲物而造成的偏见和无知，并且加以克服。有太多的人，尤其是那些术业有专攻的人，往往对其他领域的知识不屑一顾，或者认为聪明的头脑就可取代知识。\n比如，很多一流的工程师遇上与人相关的事就束手无策，他们还以此为荣——因为他们觉得，对条理清晰的工程师头脑来说，人太混乱无序了。与此形成鲜明对照的是，人力资源方面的专业人员常常以他们连基本的会计知识或数量分析都一无所知而自傲。\n不过，人们要是对这样的无知还沾沾自喜的话，那无异于自取灭亡。其实，要让自己的长处得到充分发挥，你就应该努力学习新技能、汲取新知识。\n另外一点也同样重要——纠正你的不良习惯。所谓不良习惯，是指那些会影响你的工作成效和工作表现的事情。这样的习惯能很快地在回馈中反映出来。\n与此同时，回馈还会反映出哪些问题是由缺乏礼貌造成的。礼貌是一个组织的润滑剂。\n礼貌，其实也很简单。无非是说声「请」和「谢谢」，记住别人的名字，或问候对方家人这样的小事。许多聪明人，尤其是聪明的年轻人，没有意识到这一点。\n如果回馈分析表明某个人只要一遇到需要别人合作的事就屡屡失败，那么很可能就意味着这个人的举止不大得体——也就是缺乏礼貌。\n把预期和实际结果进行比较，也会发现自己不能做什么。我们每个人都有许多一窍不通、毫无天分的领域，在这些领域我们甚至连平庸的水平都达不到。人们，尤其是知识工作者，就不应该试图去完成这些领域的工作和任务。他们应该尽量少把精力浪费在那些不能胜任的领域上，因为从无能到平庸要比从一流到卓越需要人们付出多得多的努力。\n我们的工作方式是怎样的 很少有人知道自己平时是怎样把事情给做成的。\n实际上，我们当中的大多数人甚至不知道，不同人有着不同的工作方式和表现。\n许多人不是以他们习惯的方式工作，这当然就容易造成无所作为。\n对于知识工作者来说，「我的工作方式是怎样的？」可能比「我的长处是什么？」这个问题更加重要。\n同一个人的长处一样，一个人的工作方式也是独一无二的，这由人的个性决定。\n通常，几个常见的个性特征就决定了一个人的工作方式。\n我属于读者型，还是听者型？\n首先，你要搞清楚的是，你是读者型（习惯阅读信息）还是听者型（习惯听取信息）的人。绝大多数人甚至都不知道还有读者型和听者型之说，而且很少有人既是读者型又是听者型。知道自己属于哪种类型的人更少。\n没有几个听者型的人可以通过努力变成合格的读者型——不管是主动还是被动的努力，反之亦然。不了解你的工作方式不可能发挥才干或取得成就。\n我们如何学习 要了解一个人的工作方式，需要弄清的第二点是，他是如何学习的。","tags":null,"title":"自我管理","type":"post"},{"authors":null,"categories":[],"content":" 安装Golang Dep go get -v github.com/tools/godep  安装client-go go get k8s.io/client-go/kubernetes cd $GOPATH/src/k8s.io/client-go git checkout v10.0.0 godep restore ./...  集群外开发 集群内开发 ","date":1520081150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520081150,"objectID":"1c706e685ada4b7edf079cff0d994081","permalink":"https://wubigo.com/post/k8s-sdk-setup/","publishdate":"2018-03-03T20:45:50+08:00","relpermalink":"/post/k8s-sdk-setup/","section":"post","summary":" 安装Golang Dep go get -v github.com/tools/godep  安装client-go go get k8s.io/client-go/kubernetes cd $GOPATH/src/k8s.io/client-go git checkout v10.0.0 godep restore ./...  集群外开发 集群内开发 ","tags":["K8S","SDK"],"title":"K8S SDK Setup","type":"post"},{"authors":null,"categories":null,"content":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).\nA file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS to one or more clients on-premises.\n","date":1519948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519948800,"objectID":"916e22deac7af087ed6cf28ed5f5184c","permalink":"https://wubigo.com/post/2018-03-02-filegatewayforhybridarchitectures/","publishdate":"2018-03-02T00:00:00Z","relpermalink":"/post/2018-03-02-filegatewayforhybridarchitectures/","section":"post","summary":"Organizations are looking for ways to reduce their physical data center footprints, particularly for secondary workloads such as backups, files, or on-demand workloads. However, bridging data between private data centers and the public cloud comes with a unique set of challenges. Traditional data center services rely on low-latency network attached storage (NAS) and storage area network (SAN) protocols to access storage locally. Cloud-native applications are generally optimized for API access to data in scalable and durable cloud object storage, such as Amazon Simple Storage Service (Amazon S3).","tags":null,"title":"File Gateway for Hybrid Architectures; Overview and Best Practices","type":"post"},{"authors":null,"categories":[],"content":"Amazon EC2 networking doesn\u0026rsquo;t allow to use private ips in the containers\nthrough bridges or macvlan. Dedicating a network interface to a\ncontainer makes it directly unreachable from the host.\ndocker network create -d macvlan --subnet 172.30.80.0/20 --gateway 172.30.80.1 -o parent=eth0 pub_net docker run -d --network pub_net --ip 172.30.80.10 busybox  ","date":1519808176,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519808176,"objectID":"eb0e881c68daf1c90bbe5862b12b14d7","permalink":"https://wubigo.com/post/aws-ec2-macvaln/","publishdate":"2018-02-28T16:56:16+08:00","relpermalink":"/post/aws-ec2-macvaln/","section":"post","summary":"Amazon EC2 networking doesn\u0026rsquo;t allow to use private ips in the containers\nthrough bridges or macvlan. Dedicating a network interface to a\ncontainer makes it directly unreachable from the host.\ndocker network create -d macvlan --subnet 172.30.80.0/20 --gateway 172.30.80.1 -o parent=eth0 pub_net docker run -d --network pub_net --ip 172.30.80.10 busybox  ","tags":["AWS","SDN"],"title":"Aws EC2 MACVLAN","type":"post"},{"authors":null,"categories":[],"content":"Understanding Real-World Concurrency Bugs in Go\nhttps://songlh.github.io/paper/go-study.pdf\n","date":1519802547,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519802547,"objectID":"3f8e5055c2c166beb90b140c5da6f08b","permalink":"https://wubigo.com/post/effective-coding-go/","publishdate":"2018-02-28T15:22:27+08:00","relpermalink":"/post/effective-coding-go/","section":"post","summary":"Understanding Real-World Concurrency Bugs in Go\nhttps://songlh.github.io/paper/go-study.pdf","tags":[],"title":"Effective Coding Go","type":"post"},{"authors":null,"categories":[],"content":"高效编程\n JAVA PYTHON GO  ","date":1519802289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519802289,"objectID":"8e036e1d3ca18101de945a11ae33f294","permalink":"https://wubigo.com/post/effective-coding/","publishdate":"2018-02-28T15:18:09+08:00","relpermalink":"/post/effective-coding/","section":"post","summary":"高效编程\n JAVA PYTHON GO  ","tags":["LANG"],"title":"Effective Coding","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境  可以参考从源代码构件K8S开发环境\n","date":1519614668,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519614668,"objectID":"5eb1c4f6bb98d786e4e2e3fa3b56c449","permalink":"https://wubigo.com/post/k8s_cni_kube-router/","publishdate":"2018-02-26T11:11:08+08:00","relpermalink":"/post/k8s_cni_kube-router/","section":"post","summary":"准备  搭建测试环境  可以参考从源代码构件K8S开发环境","tags":["K8S","CNI","NETWORK"],"title":"K8s CNI之Kube Router实现","type":"post"},{"authors":null,"categories":[],"content":" 准备  初始化  draft init ... Installing default plugins... Preparing to install into /home/bigo/.draft/plugins/draft-pack-repo draft-pack-repo installed into /home/bigo/.draft/plugins/draft-pack-repo/draft-pack-repo Installed plugin: pack-repo Installation of default plugins complete Installing default pack repositories... Installing pack repo from https://github.com/Azure/draft Installed pack repository github.com/Azure/draft Installation of default pack repositories complete $DRAFT_HOME has been configured at /home/bigo/.draft. ...   设置docker镜像寄存器  draft config set registry registry.cn-beijing.aliyuncs.com/k4s  or\n skip the push process entirely using the \u0026ndash;skip-image-push flag\n 应用设置 cd code/go/ ls app.go draft create ls app.go charts Dockerfile draft.toml  发布应用 draft up Draft Up Started: 'go-web': 01D6QETAPPM7ZYAM7G733ZVMY7 go-web: Building Docker Image: SUCCESS ⚓ (0.9999s) go-web: Pushing Docker Image: SUCCESS ⚓ (139.6931s) go-web: Releasing Application: SUCCESS ⚓ (4.3545s) Inspect the logs with `draft logs 01D6QETAPPM7ZYAM7G733ZVMY7`  检查  检查日志\ndraft logs 01D6QETAPPM7ZYAM7G733ZVMY7  检查软件列表\n  helm ls | grep go-web ... NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE go-web Sun Mar 24 17:01:54 2018 DEPLOYED go-web-v0.1.0 default ...   检查PODS  kubectl get pods | grep go-web NAME READY STATUS RESTARTS AGE go-web-f94bd78d5-qcmq9 1/1 Running 0 5m38s  访问应用 draft connect ... Connect to go-web:8080 on **localhost:34261** [go-web]: * Environment: production [go-web]: * Debug mode: off [go-web]: * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit) ...  curl localhost:34261  应用迭代  修改go-web/web.go\n 发布\n draft up  测试\n  draft connect  删除应用 draft delete helm ls |grep go-web  ","date":1519428653,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519428653,"objectID":"2a6ebeb404c63a77ea5474c0032ee69d","permalink":"https://wubigo.com/post/k8s-development-streamline/","publishdate":"2018-02-24T07:30:53+08:00","relpermalink":"/post/k8s-development-streamline/","section":"post","summary":"准备  初始化  draft init ... Installing default plugins... Preparing to install into /home/bigo/.draft/plugins/draft-pack-repo draft-pack-repo installed into /home/bigo/.draft/plugins/draft-pack-repo/draft-pack-repo Installed plugin: pack-repo Installation of default plugins complete Installing default pack repositories... Installing pack repo from https://github.com/Azure/draft Installed pack repository github.com/Azure/draft Installation of default pack repositories complete $DRAFT_HOME has been configured at /home/bigo/.draft. ...   设置docker镜像寄存器  draft config set registry registry.cn-beijing.aliyuncs.com/k4s  or\n skip the push process entirely using the \u0026ndash;skip-image-push flag","tags":["K8S","APP"],"title":"K8s Development Streamline with draft","type":"post"},{"authors":null,"categories":[],"content":" PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet\nIn local clusters, the default StorageClass uses the hostPath provisioner. hostPath volumes are only suitable for development and testing. With hostPath volumes, the data lives in /tmp on the node the Pod is scheduled onto and does not move between nodes\nProvisioning There are two ways PVs may be provisioned: statically or dynamically.\n Static  A cluster administrator creates a number of PVs. They carry the details of the real storage which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\n Dynamic  When none of the static PVs the administrator created matches a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class in order for dynamic provisioning to occur. Claims that request the class \u0026ldquo;\u0026rdquo; effectively disable dynamic provisioning for themselves.\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the \u0026ndash;enable-admission-plugins flag of the API server component. For more information on API server command line flags, please check kube-apiserver documentation.\n","date":1519426553,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519426553,"objectID":"ef1314c59426d86855db66979c79537e","permalink":"https://wubigo.com/post/k8s-csi/","publishdate":"2018-02-24T06:55:53+08:00","relpermalink":"/post/k8s-csi/","section":"post","summary":"PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet","tags":["STORAGE","K8S","CSI"],"title":"K8S CSI","type":"post"},{"authors":null,"categories":[],"content":" 如果xml文件带有名字空间，XPATH支持\n还不够完善。下面介绍两种可以工作的方式\nnamespace for XML documents http.get(\u0026quot;https://wubigo.com/en/sitemap.xml\u0026quot;, function(res) {  useNamespaces const select = xpath.useNamespaces({\u0026quot;ns0\u0026quot;: \u0026quot;http://www.sitemaps.org/schemas/sitemap/0.9\u0026quot;}); const nodes = select(\u0026quot;//ns0:loc\u0026quot;, doc); nodes.forEach((value) =\u0026gt; console.log(\u0026quot;ns0:\u0026quot;+value));  Implementing a Default Namespace Resolver const nsResolver = function nsResolver(prefix) { const ns = { 'ns0' : 'http://www.sitemaps.org/schemas/sitemap/0.9', 'mathml': 'http://www.w3.org/1998/Math/MathML' }; return ns[prefix] || null; }; nsResolver.lookupNamespaceURI = nsResolver; var result = xpath.evaluate( \u0026quot;//ns0:loc\u0026quot;, // xpathExpression doc, // contextNode nsResolver, // namespaceResolver xpath.XPathResult.ANY_TYPE, // resultType null // result ) node = result.iterateNext(); while (node) { console.log(\u0026quot;url=\u0026quot;+node.toString()); node = result.iterateNext(); }  ","date":1519199105,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519199105,"objectID":"17aa1acfdcce8cd2eda632d0b1ac97b2","permalink":"https://wubigo.com/post/nodejs-xpath/","publishdate":"2018-02-21T15:45:05+08:00","relpermalink":"/post/nodejs-xpath/","section":"post","summary":"如果xml文件带有名字空间，XPATH支持\n还不够完善。下面介绍两种可以工作的方式\nnamespace for XML documents http.get(\u0026quot;https://wubigo.com/en/sitemap.xml\u0026quot;, function(res) {  useNamespaces const select = xpath.useNamespaces({\u0026quot;ns0\u0026quot;: \u0026quot;http://www.sitemaps.org/schemas/sitemap/0.9\u0026quot;}); const nodes = select(\u0026quot;//ns0:loc\u0026quot;, doc); nodes.forEach((value) =\u0026gt; console.log(\u0026quot;ns0:\u0026quot;+value));  Implementing a Default Namespace Resolver const nsResolver = function nsResolver(prefix) { const ns = { 'ns0' : 'http://www.sitemaps.org/schemas/sitemap/0.9', 'mathml': 'http://www.w3.org/1998/Math/MathML' }; return ns[prefix] || null; }; nsResolver.lookupNamespaceURI = nsResolver; var result = xpath.evaluate( \u0026quot;//ns0:loc\u0026quot;, // xpathExpression doc, // contextNode nsResolver, // namespaceResolver xpath.XPathResult.ANY_TYPE, // resultType null // result ) node = result.","tags":["NODE","XML"],"title":"Nodejs Xpath名字空间","type":"post"},{"authors":null,"categories":[],"content":" 问题 长链接\n通过连接池和数据库保持长链接\n Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application\u0026rsquo;s needs. It enables you to run your database in the cloud without managing any database instances. It\u0026rsquo;s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n ","date":1518545819,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518545819,"objectID":"1315c078a311ead68e79c4086d6cf665","permalink":"https://wubigo.com/post/serverless-database/","publishdate":"2018-02-14T02:16:59+08:00","relpermalink":"/post/serverless-database/","section":"post","summary":" 问题 长链接\n通过连接池和数据库保持长链接\n Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application\u0026rsquo;s needs. It enables you to run your database in the cloud without managing any database instances. It\u0026rsquo;s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads.\n ","tags":["SERVERLESS","SQL","EDA"],"title":"数据库无服务器架构","type":"post"},{"authors":null,"categories":null,"content":" Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/\n","date":1517529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517529600,"objectID":"b2e1d541771bcdee60a291eb52a62110","permalink":"https://wubigo.com/post/2018-02-02-dynamicroutingbetweencapsules/","publishdate":"2018-02-02T00:00:00Z","relpermalink":"/post/2018-02-02-dynamicroutingbetweencapsules/","section":"post","summary":"Understanding Dynamic Routing between Capsules https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/","tags":null,"title":"Understanding Dynamic Routing between Capsules","type":"post"},{"authors":null,"categories":[],"content":"Serverless Architectures\n\nISBN-10: 1617293822  ","date":1517283791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517283791,"objectID":"4dc6cf4733ff196921ee3cd9a3b32eeb","permalink":"https://wubigo.com/post/serverless-architectures/","publishdate":"2018-01-30T11:43:11+08:00","relpermalink":"/post/serverless-architectures/","section":"post","summary":"Serverless Architectures\n\nISBN-10: 1617293822  ","tags":["SERVERLESS"],"title":"书籍推荐:无服务器架构及应用案例详解","type":"post"},{"authors":null,"categories":null,"content":" Set namespace preference kubectl config set-context $(kubectl config current-context) --namespace=\u0026lt;bigo\u0026gt;  watch pod kubectl get pods pod1 --watch  Check Performance kubectl top node kubectl top pod  copy file between pod and local kubectl cp ~/f1 \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ ~/  enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out bigo.crt -days 500 kubectl config set-credentials bigo --client-certificate=./bigo.crt --client-key=./bigo.key kubectl config set-context bigo-context --cluster=kubernetes --namespace=bigo-NS --user=bigo kubectl config get-contexts ... CURRENT NAME CLUSTER AUTHINFO NAMESPACE bigo-context kubernetes bigo bigo * kubernetes-admin@kubernetes kubernetes kubernetes-admin ...  binding role to user cat rolebinding-bigo-access.yaml kind: RoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: access-manager-binding namespace: bigo-NS subjects: - kind: User name: bigo apiGroup: \u0026quot;\u0026quot; roleRef: kind: Role name: access-role apiGroup: \u0026quot;\u0026quot; kubectl create -f rolebinding-bigo-access.yaml  USER, GROUP, ROLE , ROLEBIND, RBAC  list all users ``` kubectl config view \u0026hellip; users: name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED \u0026hellip; ```  Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or ``` cat tiller-clusterrolebinding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects:\n kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026ldquo;\u0026rdquo;\n  docker pull registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\nkubectl create -f tiller-clusterrolebinding.yaml\nUpdate the existing tiller-deploy deployment with the Service Account helm init \u0026ndash;service-account tiller \u0026ndash;upgrade\n  helm install \u0026ndash;name prometheus stable/prometheus\nhelm install \u0026ndash;name prometheus1 stable/prometheus \u0026ndash;set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false,\n alertmanager: enabled: false name: p-alertmanager server: name: prometheus880  PVC using local PV  create PVC  cat storage-class-hdd.yaml apiVersion: storage.K8S.io/v1 kind: StorageClass metadata: name: local-hdd provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer   kubectl apply -f storage-class-hdd.yaml\n  create local PV  cat local_volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: local-hdd spec: capacity: storage: 8Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-hdd local: path: /mnt/pv/ nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - bigo-vm4   kubectl apply -f local_volume.yaml\n PersistentVolume nodeAffinity is required when using local volumes. It enables the Kubernetes scheduler to correctly schedule Pods using local volumes to the correct node.\nPersistentVolume volumeMode can now be set to “Block” (instead of the default value “Filesystem”) to expose the local volume as a raw block device. The volumeMode field requires BlockVolume Alpha feature gate to be enabled.\nWhen using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. See the example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity\nPort Forwarding a local port to a port on K8S kubectl port-forward \u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward pods/\u0026lt;podname\u0026gt; 9090:9090 or kubectl port-forward deployment/prometheus 9090:9090 or kubectl port-forward svc/prometheus 9090:9090 or kubectl port-forward rs/prometheus 9090:9090  ","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"0d9d01a1ee982b0db9c49cd2b2fd4f52","permalink":"https://wubigo.com/post/k8s-kubectl-cheatsheet/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/post/k8s-kubectl-cheatsheet/","section":"post","summary":"Set namespace preference kubectl config set-context $(kubectl config current-context) --namespace=\u0026lt;bigo\u0026gt;  watch pod kubectl get pods pod1 --watch  Check Performance kubectl top node kubectl top pod  copy file between pod and local kubectl cp ~/f1 \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ kubectl cp \u0026lt;namespace\u0026gt;/\u0026lt;pod-name\u0026gt;:/tmp/ ~/  enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj \u0026quot;/CN=wubigo/O=bigo LLC\u0026quot; sudo openssl x509 -req -in bigo.","tags":["K8S"],"title":"kubectl cheat sheet","type":"post"},{"authors":null,"categories":null,"content":" Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with. At the end of the day, it’s better to have a few tests running chromium-only, than no tests at all. * Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There’s no need for evil “sleep(1000)” calls in puppeteer scripts. * Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution. * Puppeteer shines when it comes to debugging: flip the “headless” bit to false, add “slowMo”, and you’ll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.\n","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"10b6c1db1cfd215584023df7ed5469c7","permalink":"https://wubigo.com/post/2018-01-07-webtestingautomation/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/post/2018-01-07-webtestingautomation/","section":"post","summary":"Puppeteer vs Selenium/WebDriver  Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers. Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.  That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn’t be your only testing solution, it does have a few good points compared to WebDriver: * Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with.","tags":null,"title":"web testing automation","type":"post"},{"authors":null,"categories":null,"content":" Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/\n","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"70d65632982e837a935dc764b934fd16","permalink":"https://wubigo.com/post/2018-01-02-deathofmicroservicemadness/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/post/2018-01-02-deathofmicroservicemadness/","section":"post","summary":"Death of Microservice Madness http://www.dwmkerr.com/the-death-of-microservice-madness-in-2018/","tags":null,"title":"Death of Microservice Madness","type":"post"},{"authors":null,"categories":null,"content":" Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.aerogrammestudio.com/2013/03/07/pixars-22-rules-of-storytelling/\nKeybase is now supported by the Stellar Development Foundation https://keybase.io/blog/keybase-stellar\nMachine Learning Crash Course https://developers.google.com/machine-learning/crash-course/\nThe Makefile I use with JavaScript projects http://www.olioapps.com/blog/the-lost-art-of-the-makefile/\nHow GDPR Will Change The Way You Develop https://www.smashingmagazine.com/2018/02/gdpr-for-web-developers/\nUber and Waymo Reach Settlement https://www.uber.com/newsroom/uber-waymo-settlement/\nPostmortem of Service Outage at 3.4M Concurrent Users https://www.epicgames.com/fortnite/en-US/news/postmortem-of-service-outage-at-3-4m-ccu\nPerspective: Streaming pivot visualization via WebAssembly https://github.com/jpmorganchase/perspective\nTinc VPN: Secure Private Network Between Hosts https://www.tinc-vpn.org/\nA reimplementation of Winamp 2.9 in HTML5 and Javascript https://github.com/captbaritone/winamp2-js\nWhat I Learned Burning $14k on YouTube Ads for Candy Japan https://www.candyjapan.com/behind-the-scenes/what-i-learned-advertising-on-youtube\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d136c19d4861d8c757ecf23594c7153f","permalink":"https://wubigo.com/post/2018-01-01-hacknewsfavorites2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/post/2018-01-01-hacknewsfavorites2018/","section":"post","summary":"Hijack of Amazon’s domain service used to reroute web traffic for two hours https://doublepulsar.com/hijack-of-amazons-internet-domain-service-used-to-reroute-web-traffic-for-two-hours-unnoticed-3a6f0dda6a6f\nSocioeconomic group classification based on user features http://pimg-faiw.uspto.gov/fdd/83/2018/28/003/0.pdf\nThe long, tortuous and fascinating process of creating a Chinese font (qz.com) https://qz.com/522079/the-long-incredibly-tortuous-and-fascinating-process-of-creating-a-chinese-font/\nAnnouncing 1.1.1.1: the fastest, privacy-first consumer DNS service https://blog.cloudflare.com/announcing-1111/\nIntroducing Cloud Text-to-Speech powered by DeepMind WaveNet technology https://cloudplatform.googleblog.com/2018/03/introducing-Cloud-Text-to-Speech-powered-by-Deepmind-WaveNet-technology.html\nAnnouncing gRPC Support in NGINX https://www.nginx.com/blog/nginx-1-13-10-grpc/\nACME v2 and Wildcard Certificate Support is Live https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live\nPIXAR’S 22 RULES OF STORYTELLING https://www.","tags":null,"title":"Hacknews favorites 2018","type":"post"},{"authors":null,"categories":null,"content":" Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1514592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514592000,"objectID":"d3ca72d17b15012cf75563e378e0a9b8","permalink":"https://wubigo.com/post/2017-12-30-booksireadthisyear/","publishdate":"2017-12-30T00:00:00Z","relpermalink":"/post/2017-12-30-booksireadthisyear/","section":"post","summary":"Gone with the Wind https://www.amazon.com/Gone-Wind-Margaret-Mitchell\nDeep Learning with Python https://www.manning.com/books/deep-learning-with-python\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017","tags":null,"title":"Books I read this year","type":"post"},{"authors":null,"categories":[],"content":" 数据中心网络虚拟化——NVo3技术端到端隧道 NVo3（Network Virtualization over Layer 3），是IETF 2014年十月份提出的数据中心虚拟化技术框架。\nNVo3基于IP/MPLS作为传输网，在其上通过隧道连接的方式，构建大规模的二层租户网络。NVo3的技术模型如下所示，\nPE设备称为NVE（Network Virtualization Element），VN Context作为Tag标识租户网络，P设备即为普通的IP/MPLS路由器。\nNVo3在设计之初，VxLAN与SDN的联合部署已经成为了数据中心的大趋势，因此NVo3的模型中专门画出了\nNVA(Network Virtualization Authority）作为NVE设备的控制器负责隧道建立、地址学习等控制逻辑\nVxLAN（Virtual eXtensible LAN，RFC 7348） Vmware和Cisco联合提出的一种二层技术，突破了VLAN ID只有4k的限制，允许通过现有的IP网络进行隧道的传输。\n别看VxLAN名字听起来和VLAN挺像，但是两者技术上可没什么必然联系。VxLAN是一种MACinUDP的隧道.\nNvGRE NvGRE（Network virtualization GRE，RFC draft）是微软搞出来的数据中心虚拟化技术，是一种MACinGRE隧道。它对传统的GRE报头进行了改造，增加了24位的VSID字段标识租户，而FlowID可用来做ECMP。由于去掉了GRE报头中的Checksum字段，因此NvGRE不支持校验和检验。NvGRE封装以太网帧，外层的报头可以为IPv4也可以为IPv6\n","date":1514521868,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514521868,"objectID":"63a943ffb96ff1b643bd83c49deafac7","permalink":"https://wubigo.com/post/nfv-notes/","publishdate":"2017-12-29T12:31:08+08:00","relpermalink":"/post/nfv-notes/","section":"post","summary":"数据中心网络虚拟化——NVo3技术端到端隧道 NVo3（Network Virtualization over Layer 3），是IETF 2014年十月份提出的数据中心虚拟化技术框架。\nNVo3基于IP/MPLS作为传输网，在其上通过隧道连接的方式，构建大规模的二层租户网络。NVo3的技术模型如下所示，\nPE设备称为NVE（Network Virtualization Element），VN Context作为Tag标识租户网络，P设备即为普通的IP/MPLS路由器。\nNVo3在设计之初，VxLAN与SDN的联合部署已经成为了数据中心的大趋势，因此NVo3的模型中专门画出了\nNVA(Network Virtualization Authority）作为NVE设备的控制器负责隧道建立、地址学习等控制逻辑\nVxLAN（Virtual eXtensible LAN，RFC 7348） Vmware和Cisco联合提出的一种二层技术，突破了VLAN ID只有4k的限制，允许通过现有的IP网络进行隧道的传输。\n别看VxLAN名字听起来和VLAN挺像，但是两者技术上可没什么必然联系。VxLAN是一种MACinUDP的隧道.\nNvGRE NvGRE（Network virtualization GRE，RFC draft）是微软搞出来的数据中心虚拟化技术，是一种MACinGRE隧道。它对传统的GRE报头进行了改造，增加了24位的VSID字段标识租户，而FlowID可用来做ECMP。由于去掉了GRE报头中的Checksum字段，因此NvGRE不支持校验和检验。NvGRE封装以太网帧，外层的报头可以为IPv4也可以为IPv6","tags":["NFV"],"title":"NFV Notes","type":"post"},{"authors":null,"categories":[],"content":" 创建BUCKET 使用两种方式之一创建BUCKET\n terraform  git clone https://github.com/wubigo/iaas cd s3 terraform apply   awscli  aws s3 website s3://s.wubigo.com/ --index-document index.html --error-document 404.html aws s3api put-bucket-policy --bucket s.wubigo.com --policy file://policy.json  确认配置 aws s3api get-bucket-website --bucket s.wubigo.com { \u0026quot;IndexDocument\u0026quot;: { \u0026quot;Suffix\u0026quot;: \u0026quot;index.html\u0026quot; }, \u0026quot;ErrorDocument\u0026quot;: { \u0026quot;Key\u0026quot;: \u0026quot;404.html\u0026quot; } }  配置DNS C记录 查看S3 Website Endpoints: s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com\nCNAME Record s s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com  上传站点内容 aws s3 cp wubigo.github.io s3://s.wubigo.com/ --recursive  ","date":1514279608,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514279608,"objectID":"69945c5ad2b5a745dae7c4fac91e6de0","permalink":"https://wubigo.com/post/aws-s3-web-hosting/","publishdate":"2017-12-26T17:13:28+08:00","relpermalink":"/post/aws-s3-web-hosting/","section":"post","summary":" 创建BUCKET 使用两种方式之一创建BUCKET\n terraform  git clone https://github.com/wubigo/iaas cd s3 terraform apply   awscli  aws s3 website s3://s.wubigo.com/ --index-document index.html --error-document 404.html aws s3api put-bucket-policy --bucket s.wubigo.com --policy file://policy.json  确认配置 aws s3api get-bucket-website --bucket s.wubigo.com { \u0026quot;IndexDocument\u0026quot;: { \u0026quot;Suffix\u0026quot;: \u0026quot;index.html\u0026quot; }, \u0026quot;ErrorDocument\u0026quot;: { \u0026quot;Key\u0026quot;: \u0026quot;404.html\u0026quot; } }  配置DNS C记录 查看S3 Website Endpoints: s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com\nCNAME Record s s.wubigo.com.s3-website-ap-northeast-1.amazonaws.com  上传站点内容 aws s3 cp wubigo.github.io s3://s.wubigo.com/ --recursive  ","tags":["AWS","S3"],"title":"Aws S3 Web Hosting","type":"post"},{"authors":null,"categories":[],"content":" go version go version go version go1.13.5 windows/amd64  vs proxy 根据code提示自动安装插件\n手工安装插件 go代理配置 set http_proxy=http://127.0.0.1:4910  git代理配置 git config --global http.proxy https://127.0.0.1:4910 git config --global http.sslverify \u0026quot;false\u0026quot;  手工安装插件 go get -u -v github.com/go-delve/delve/cmd/dlv go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/acroca/go-symbols go get -u -v github.com/mdempsky/gocode go get -u -v github.com/rogpeppe/godef go get -u -v golang.org/x/tools/cmd/godoc go get -u -v github.com/zmb3/gogetdoc go get -u -v golang.org/x/lint/golint go get -u -v github.com/fatih/gomodifytags go get -u -v golang.org/x/tools/cmd/gorename go get -u -v sourcegraph.com/sqs/goreturns go get -u -v golang.org/x/tools/cmd/goimports go get -u -v github.com/cweill/gotests/... go get -u -v golang.org/x/tools/cmd/guru go get -u -v github.com/josharian/impl go get -u -v github.com/haya14busa/goplay/cmd/goplay go get -u -v github.com/uudashr/gopkgs/cmd/gopkgs go get -u -v github.com/davidrjenni/reftools/cmd/fillstruct  FAQ  vscode go build __debug_bin: Access is denied\ncreate go\\.vscode\\launch.json\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Launch\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;go\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}\u0026quot;, \u0026quot;env\u0026quot;: {}, \u0026quot;args\u0026quot;: [] } ] }  fork/exec d:\\cache\\go-build618440214\\b001\\exe\\sitemap.exe: Access is denied.\n简单的程序运行没有问题\nD:\\code\\\u0026gt;set go GOBIN=D:\\code\\go\\bin GOPATH=d:\\code\\go GOTMPDIR=d:\\cache  VSCODE没有权限访问GOTMPDIR\ngo run sitemap.go   ","date":1513927771,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513927771,"objectID":"ef29b4be90018c5707a2df3e0673e5e3","permalink":"https://wubigo.com/post/go-vscode/","publishdate":"2017-12-22T15:29:31+08:00","relpermalink":"/post/go-vscode/","section":"post","summary":"go version go version go version go1.13.5 windows/amd64  vs proxy 根据code提示自动安装插件\n手工安装插件 go代理配置 set http_proxy=http://127.0.0.1:4910  git代理配置 git config --global http.proxy https://127.0.0.1:4910 git config --global http.sslverify \u0026quot;false\u0026quot;  手工安装插件 go get -u -v github.com/go-delve/delve/cmd/dlv go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/ramya-rao-a/go-outline go get -u -v github.com/acroca/go-symbols go get -u -v github.com/mdempsky/gocode go get -u -v github.com/rogpeppe/godef go get -u -v golang.org/x/tools/cmd/godoc go get -u -v github.","tags":["GO"],"title":"Go Vscode环境配置","type":"post"},{"authors":null,"categories":[],"content":" 与回调函数的区别  不用写错误条件if (err) return callback(err) Promise能被作为对象返回并被后期调用\n 回调\n  function successCallback(result) { console.log(\u0026quot;Audio file ready at URL: \u0026quot; + result); } function failureCallback(error) { console.error(\u0026quot;Error generating audio file: \u0026quot; + error); } createAudioFileAsync(audioSettings, successCallback, failureCallback);   promise  const promise = createAudioFileAsync(audioSettings); promise.then(successCallback, failureCallback);  or\ncreateAudioFileAsync(audioSettings).then(successCallback, failureCallback);  状态 Promise有三种状态\n pending: Initial Case where promise instantiated. fulfilled: Success Case which means promise resolved. rejected: Failure Case which means promise rejected.  方法 Promise有六个方法：\n Promise.all([promise1, promise2, …]); Promise.race([promise1, promise2, …]); Promise.reject(value); Promise.resolve(value); Promise.catch(onRejection); Promise.then(onFulFillment, onRejection);\nAPI参考\n  执行函数  Executor Functions are Parameter for Promise Constructor which holds Resolve and Reject Callbacks. It is executed immediately by the Promise implementation which provides the resolve and reject functions. It’s Triggered before the Promise constructor even returns the created object. The Resolve and Reject functions are bound to the promise to fulfill or reject. It’s expected to initiate some asynchronous work and then, once that completes, call either the resolve or reject.  可以被覆盖的方法  Promise.prototype.catch(); Promise.prototype.then();  错误处理 Promise的两种错误处理方式：\n then方法的第二个参数onRejection回调 catch  'use strict'; // First Approach yourPromise.catch(function (error) { // Your Error Callback }); // Second Approach yourPromise.then(undefined, function (error) { // Your Error Callback });  错误检测：根据catch或onRejection定义的顺序被触发\n'use strict'; Promise.catch(onRejected); Promise.then(onFulfilled, onRejected);  ","date":1513812931,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513812931,"objectID":"7ed2dfd38139da06e83cbaa38a217551","permalink":"https://wubigo.com/post/nodejs-promise/","publishdate":"2017-12-21T07:35:31+08:00","relpermalink":"/post/nodejs-promise/","section":"post","summary":"与回调函数的区别  不用写错误条件if (err) return callback(err) Promise能被作为对象返回并被后期调用\n 回调\n  function successCallback(result) { console.log(\u0026quot;Audio file ready at URL: \u0026quot; + result); } function failureCallback(error) { console.error(\u0026quot;Error generating audio file: \u0026quot; + error); } createAudioFileAsync(audioSettings, successCallback, failureCallback);   promise  const promise = createAudioFileAsync(audioSettings); promise.then(successCallback, failureCallback);  or\ncreateAudioFileAsync(audioSettings).then(successCallback, failureCallback);  状态 Promise有三种状态\n pending: Initial Case where promise instantiated. fulfilled: Success Case which means promise resolved. rejected: Failure Case which means promise rejected.","tags":["NODE"],"title":"Nodejs异步通信之Promise","type":"post"},{"authors":null,"categories":[],"content":" 对象创建有如下几种方式\n使用{} let animal = {} animal.name = 'Leo' animal.energy = 10 animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length }  构造函数 function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length } return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10)  缺点  对象方法重复占用内存空间  方法共享 const animalMethods = { eat(amount) { console.log(`${this.name} is eating.`) this.energy += amount }, sleep(length) { console.log(`${this.name} is sleeping.`) this.energy += length }, play(length) { console.log(`${this.name} is playing.`) this.energy -= length } } function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = animalMethods.eat animal.sleep = animalMethods.sleep animal.play = animalMethods.play return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10)  Object.create Object.create allows you to create an object which will delegate to another\nobject on failed lookups. Put differently, Object.create allows you to create\nan object and whenever there’s a failed property lookup on that object, it can\nconsult another object to see if that other object has the property\nconst parent = { name: 'Stacey', age: 35, heritage: 'Irish' } const child = Object.create(parent) child.name = 'Ryan' child.age = 7 console.log(child.name) // Ryan console.log(child.age) // 7 console.log(child.heritage) // Irish  Object.create并方法共享 const animalMethods = { eat(amount) { console.log(`${this.name} is eating.`) this.energy += amount }, sleep(length) { console.log(`${this.name} is sleeping.`) this.energy += length }, play(length) { console.log(`${this.name} is playing.`) this.energy -= length } } function Animal (name, energy) { let animal = Object.create(animalMethods) animal.name = name animal.energy = energy return animal } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10) leo.eat(10) snoop.play(5)  缺点  要单独管理方法对象  prototype every function in JavaScript has a prototype property that references an object\nfunction doThing () {} console.log(doThing.prototype) // {}  function Animal (name, energy) { let animal = Object.create(Animal.prototype) animal.name = name animal.energy = energy return animal } Animal.prototype.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } Animal.prototype.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } Animal.prototype.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length } const leo = Animal('Leo', 7) const snoop = Animal('Snoop', 10) leo.eat(10) snoop.play(5)  ","date":1513811654,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513811654,"objectID":"360929cb29fcc235c4bfabb4cec26ed5","permalink":"https://wubigo.com/post/nodejs-prototype/","publishdate":"2017-12-21T07:14:14+08:00","relpermalink":"/post/nodejs-prototype/","section":"post","summary":"对象创建有如下几种方式\n使用{} let animal = {} animal.name = 'Leo' animal.energy = 10 animal.eat = function (amount) { console.log(`${this.name} is eating.`) this.energy += amount } animal.sleep = function (length) { console.log(`${this.name} is sleeping.`) this.energy += length } animal.play = function (length) { console.log(`${this.name} is playing.`) this.energy -= length }  构造函数 function Animal (name, energy) { let animal = {} animal.name = name animal.energy = energy animal.eat = function (amount) { console.","tags":["NODE"],"title":"Nodejs对象创建方式","type":"post"},{"authors":null,"categories":[],"content":" Architecture domain Since Stephen Spewak\u0026rsquo;s Enterprise Architecture Planning (EAP) in 1993, and perhaps before then, it has been normal to divide enterprises architecture into four architecture domains.\n Business architecture, Data architecture, Applications architecture, Technology architecture.  Layers of the enterprise architecture\n","date":1512948310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512948310,"objectID":"e7390efc988b5d7ef3ac2ea70e2fdcc9","permalink":"https://wubigo.com/post/enterprise-architecture-framework/","publishdate":"2017-12-11T07:25:10+08:00","relpermalink":"/post/enterprise-architecture-framework/","section":"post","summary":"Architecture domain Since Stephen Spewak\u0026rsquo;s Enterprise Architecture Planning (EAP) in 1993, and perhaps before then, it has been normal to divide enterprises architecture into four architecture domains.\n Business architecture, Data architecture, Applications architecture, Technology architecture.  Layers of the enterprise architecture","tags":["EA","ToGAF"],"title":"Enterprise Architecture Framework","type":"post"},{"authors":null,"categories":[],"content":" set registry npm config set registry=http://registry.npm.taobao.org npm config ls -l userconfig = \u0026quot;C:\\\\Users\\\\Administrator\\\\.npmrc\u0026quot;  declare variables ES6 comes with two more options to declare your variables: const and let. In JavaScript ES6, you will\nrarely find var anymore.\nA variable declared with const cannot be re-assigned or re-declared. It cannot get mutated (changed,\nmodified)\nImmutability is embraced in React and its ecosystem. That’s why const should be your default\nchoice when you define a variable.\nES6 Arrow Functions // function expression function () { ... } // arrow function expression () =\u0026gt; { ... }  You can remove the parentheses when the function\ngets only one argument, but have to keep them when\nit gets multiple arguments.\n// allowed item =\u0026gt; { ... } // allowed (item) =\u0026gt; { ... } // not allowed item, key =\u0026gt; { ... } // allowed (item, key) =\u0026gt; { ... }  Additionally, you can remove the block body, meaning the curly braces, of the ES6 arrow function. In a concise body an implicit return is attached. Thus you can remove the return statement. That will happen more often in the book, so be sure to understand the difference between a block body and a concise body when using arrow functions\nES6 Object Initializer you are allowed to use computed property names in JavaScript ES6\n// ES6 const key = 'name'; const user = { \\[key]: 'Robin', };  Bindings class methods don’t\nautomatically bind this to the class instance\nThat’s a main source of bugs when using React, because if you want to access\nthis.state in your class method, it cannot be retrieved because this is undefined. So in order to\nmake this accessible in your class methods, you have to bind the class methods to this.\nclass methods can be autobound automatically without\nbinding them explicitly by using JavaScript ES6 arrow functions\nclass A { constructor() { this.foo = this.foo.bind(this) } foo() { console.log('foo from A') } } class A { foo = () =\u0026gt; { console.log('foo from A') } }  The official React documentation sticks to the class method bindings in the constructor\nEXPORT default statement  to export and import a single functionality to highlight the main functionality of the exported API of a module to have a fallback import functionality  const robin = { firstname: 'robin', lastname: 'wieruch', }; export default robin;  Furthermore, the import name can differ from the exported default name\nAsynchronous code execution Because most of the JavaScript runtimes are single-threaded, many longer operations, such as network requests, are executed asynchronously. Asynchronous code execution is handled by two known concepts: callbacks and promises.\n promises\nA promise represents an eventual result of an asynchronous operation Promises are just pretty wrappers around callbacks. In real-world situations, you wrap a promise around a certain action or operation. A promise can have two possible outcomes: it can be resolved (fulfilled) or rejected (unfulfilled).   enable \u0026ldquo;TypeScript and JavaScript Language Features\u0026rdquo; extension in VS Code Go to extensions and search @builtin typescript to find the extension\n","date":1512261592,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512261592,"objectID":"11eb496d211ce7e363a7a703a1eedd92","permalink":"https://wubigo.com/post/nodejs-notes/","publishdate":"2017-12-03T08:39:52+08:00","relpermalink":"/post/nodejs-notes/","section":"post","summary":"set registry npm config set registry=http://registry.npm.taobao.org npm config ls -l userconfig = \u0026quot;C:\\\\Users\\\\Administrator\\\\.npmrc\u0026quot;  declare variables ES6 comes with two more options to declare your variables: const and let. In JavaScript ES6, you will\nrarely find var anymore.\nA variable declared with const cannot be re-assigned or re-declared. It cannot get mutated (changed,\nmodified)\nImmutability is embraced in React and its ecosystem. That’s why const should be your default","tags":["NODE","JS"],"title":"Nodejs Notes","type":"post"},{"authors":null,"categories":null,"content":" Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90\n","date":1512172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512172800,"objectID":"d6c0dd10ab9b50518ec7bde144b723e2","permalink":"https://wubigo.com/post/2017-12-02-machinelearning101slidedeck/","publishdate":"2017-12-02T00:00:00Z","relpermalink":"/post/2017-12-02-machinelearning101slidedeck/","section":"post","summary":"Machine Learning 101 slidedeck https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?imm_mid=0f9b7e\u0026amp;cmp=em-data-na-na-newsltr_20171213\u0026amp;slide=id.g183f28bdc3_0_90","tags":null,"title":"Machine Learning 101 slidedeck","type":"post"},{"authors":null,"categories":[],"content":" AWS免费类型  首次注册后的12个月免费\n 永久免费\n 试用\n  aws永久免费的服务 计算服务  Lambda\n1百万请求/月\n3百万秒计算时间/月\n Step\n4000/月\n  存储  DynamoDB\n25GB\n S3\n  The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data out each\nmonth\n Glacier\n10GB\n  ","date":1512035489,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512035489,"objectID":"4dd04abc60d46efd1ea9ae12107e3b49","permalink":"https://wubigo.com/post/aws-free-tier-quota/","publishdate":"2017-11-30T17:51:29+08:00","relpermalink":"/post/aws-free-tier-quota/","section":"post","summary":" AWS免费类型  首次注册后的12个月免费\n 永久免费\n 试用\n  aws永久免费的服务 计算服务  Lambda\n1百万请求/月\n3百万秒计算时间/月\n Step\n4000/月\n  存储  DynamoDB\n25GB\n S3\n  The S3 free tier allows users to store 5 GB of data with standard storage, issue\n20,000 GET requests and 2,000 PUT requests, and transfer 15 GB of data out each\nmonth\n Glacier\n10GB\n  ","tags":["AWS","IAAS","CLOUD","LOCALSTACK"],"title":"AWS云服务免费额度列表","type":"post"},{"authors":null,"categories":[],"content":" create rest api resource awslocal apigateway create-rest-api --name 'My First API' --description 'This is my first API'  awslocal apigateway get-rest-apis { \u0026quot;items\u0026quot;: [ { \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;tjc336382o\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;hello_api2\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;This is my first API\u0026quot;, \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;foyylqv018\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;My First API\u0026quot; } ] }  ","date":1511441566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511441566,"objectID":"8cdde9ad919ef6347f9ce66918bc4d52","permalink":"https://wubigo.com/post/aws-apigateway-notes/","publishdate":"2017-11-23T20:52:46+08:00","relpermalink":"/post/aws-apigateway-notes/","section":"post","summary":" create rest api resource awslocal apigateway create-rest-api --name 'My First API' --description 'This is my first API'  awslocal apigateway get-rest-apis { \u0026quot;items\u0026quot;: [ { \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;tjc336382o\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;hello_api2\u0026quot; }, { \u0026quot;description\u0026quot;: \u0026quot;This is my first API\u0026quot;, \u0026quot;createdDate\u0026quot;: 1574513755, \u0026quot;id\u0026quot;: \u0026quot;foyylqv018\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;My First API\u0026quot; } ] }  ","tags":[],"title":"Aws Apigateway Notes","type":"post"},{"authors":null,"categories":[],"content":" 前提条件 配置AWS aws configure list Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************s-ok shared-credentials-file secret_key ****************-key shared-credentials-file region local config-file ~/.aws/config  ~/.aws/config\n[default] output = json region = local  ~/.aws/credentials\n[default] aws_access_key_id = any-id-is-ok aws_secret_access_key = fake-key  启动aws本地服务 localstack start  创建EC2 配置 mkdir ec2 cd ec2 touch ec2.tf  ec2.tf\nprovider \u0026quot;aws\u0026quot; { profile = \u0026quot;default\u0026quot; region = \u0026quot;us-east-1\u0026quot; endpoints { ec2 = \u0026quot;http://localhost:4597\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } } resource \u0026quot;aws_instance\u0026quot; \u0026quot;example\u0026quot; { ami = \u0026quot;ami-2757f631\u0026quot; instance_type = \u0026quot;t2.micro\u0026quot; }  aws localstack provider provider \u0026quot;aws\u0026quot; { access_key = \u0026quot;mock_access_key\u0026quot; region = \u0026quot;us-east-1\u0026quot; secret_key = \u0026quot;mock_secret_key\u0026quot; s3_force_path_style = true skip_credentials_validation = true skip_metadata_api_check = true skip_requesting_account_id = true endpoints { apigateway = \u0026quot;http://localhost:4567\u0026quot; cloudformation = \u0026quot;http://localhost:4581\u0026quot; cloudwatch = \u0026quot;http://localhost:4582\u0026quot; dynamodb = \u0026quot;http://localhost:4569\u0026quot; es = \u0026quot;http://localhost:4578\u0026quot; firehose = \u0026quot;http://localhost:4573\u0026quot; iam = \u0026quot;http://localhost:4593\u0026quot; kinesis = \u0026quot;http://localhost:4568\u0026quot; lambda = \u0026quot;http://localhost:4574\u0026quot; route53 = \u0026quot;http://localhost:4580\u0026quot; redshift = \u0026quot;http://localhost:4577\u0026quot; s3 = \u0026quot;http://localhost:4572\u0026quot; secretsmanager = \u0026quot;http://localhost:4584\u0026quot; ses = \u0026quot;http://localhost:4579\u0026quot; sns = \u0026quot;http://localhost:4575\u0026quot; sqs = \u0026quot;http://localhost:4576\u0026quot; ssm = \u0026quot;http://localhost:4583\u0026quot; stepfunctions = \u0026quot;http://localhost:4585\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } }  初始化  下载SP  terraform init   应用配置  terraform apply  成功执行后terraform.tfstate自动生成，该文件记录被管理资源的ID\n 显示结果  terraform show  检查结果 awslocal ec2 describe-instances  销毁资源 terraform destroy  配置 .terraformrc\nplugin_cache_dir = \u0026quot;$HOME/.terraform.d/plugin-cache\u0026quot; disable_checkpoint = true   Third-party Plugins  These third-party providers must be manually installed,\nsince terraform init cannot automatically download them\n~/.terraform.d/plugin  ","date":1511421061,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511421061,"objectID":"dcedb933db8e4ca23b71439ac8fa78b2","permalink":"https://wubigo.com/post/terraform-notes/","publishdate":"2017-11-23T15:11:01+08:00","relpermalink":"/post/terraform-notes/","section":"post","summary":"前提条件 配置AWS aws configure list Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************s-ok shared-credentials-file secret_key ****************-key shared-credentials-file region local config-file ~/.aws/config  ~/.aws/config\n[default] output = json region = local  ~/.aws/credentials\n[default] aws_access_key_id = any-id-is-ok aws_secret_access_key = fake-key  启动aws本地服务 localstack start  创建EC2 配置 mkdir ec2 cd ec2 touch ec2.tf  ec2.tf\nprovider \u0026quot;aws\u0026quot; { profile = \u0026quot;default\u0026quot; region = \u0026quot;us-east-1\u0026quot; endpoints { ec2 = \u0026quot;http://localhost:4597\u0026quot; sts = \u0026quot;http://localhost:4592\u0026quot; } } resource \u0026quot;aws_instance\u0026quot; \u0026quot;example\u0026quot; { ami = \u0026quot;ami-2757f631\u0026quot; instance_type = \u0026quot;t2.","tags":["LOCALSTACK","TERRAFORM"],"title":"Terraform Notes","type":"post"},{"authors":null,"categories":[],"content":" disable Taskbar thumbnail preview on Windows https://www.windowscentral.com/how-disable-taskbar-thumbnail-preview-windows-10\n","date":1506669961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506669961,"objectID":"df61cfea5c04d0c5a243098d0839a4e4","permalink":"https://wubigo.com/post/win10-customize/","publishdate":"2017-09-29T15:26:01+08:00","relpermalink":"/post/win10-customize/","section":"post","summary":"disable Taskbar thumbnail preview on Windows https://www.windowscentral.com/how-disable-taskbar-thumbnail-preview-windows-10","tags":["WIN"],"title":"Win10 Customize","type":"post"},{"authors":null,"categories":[],"content":" 最近在做智慧工地的项目，“智慧工地”的建设很大一部分信息主要是来自于工程BIM模型。 大部分情况下，BIM模型的精确度决定了“智慧工地”的开展程度。 “智慧工地”建设中，BIM模型的应用主要集中在以下几个方面：\n 工程量的统计：分析各施工流水段各材料的工程量，如混凝土的工程量。\n 施工模拟：施工进度计划与BIM模型相关联，对施工过程进行模拟。将实际工程进度与模拟进度进行对比，可以直观的看出工程是否滞后。\n 可视化交底：通过BIM的可视化特点，对施工方案进行模拟，对施工人员进行3D动画交底，提高了交底的可行性。\n 节点分析： 对复杂节点进行BIM建模，通过模型对复杂节点进行分析。\n 综合管线碰撞检测： 检测预留孔洞、机电、设备管线安装碰撞。\n  从具体实现的角度对BIM和CAD做个比较\n    CAD BIM     工具集 2D(3D通过划线) 3D   连接过程 直接(2D对象直接连接) 间接(3D对象通过参数装配)   视图 高度具体的可视化视图 动态流线型视图，可随意放大，缩小   知识复用  MODEL(部件从MODEL抽取，而且部件的修改同时同步到MODEL)    基于WEB的3D建模工具  https://www.onshape.com/ Lagoa(基于云端的3D建模APP，被Autodesk6千万美元收购) https://www.vectary.com/ https://clara.io/  ","date":1504914766,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504914766,"objectID":"298b657e214758877416cdcc41d63cfb","permalink":"https://wubigo.com/post/bim-vs-cad/","publishdate":"2017-09-09T07:52:46+08:00","relpermalink":"/post/bim-vs-cad/","section":"post","summary":" 最近在做智慧工地的项目，“智慧工地”的建设很大一部分信息主要是来自于工程BIM模型。 大部分情况下，BIM模型的精确度决定了“智慧工地”的开展程度。 “智慧工地”建设中，BIM模型的应用主要集中在以下几个方面：\n 工程量的统计：分析各施工流水段各材料的工程量，如混凝土的工程量。\n 施工模拟：施工进度计划与BIM模型相关联，对施工过程进行模拟。将实际工程进度与模拟进度进行对比，可以直观的看出工程是否滞后。\n 可视化交底：通过BIM的可视化特点，对施工方案进行模拟，对施工人员进行3D动画交底，提高了交底的可行性。\n 节点分析： 对复杂节点进行BIM建模，通过模型对复杂节点进行分析。\n 综合管线碰撞检测： 检测预留孔洞、机电、设备管线安装碰撞。\n  从具体实现的角度对BIM和CAD做个比较\n    CAD BIM     工具集 2D(3D通过划线) 3D   连接过程 直接(2D对象直接连接) 间接(3D对象通过参数装配)   视图 高度具体的可视化视图 动态流线型视图，可随意放大，缩小   知识复用  MODEL(部件从MODEL抽取，而且部件的修改同时同步到MODEL)    基于WEB的3D建模工具  https://www.onshape.com/ Lagoa(基于云端的3D建模APP，被Autodesk6千万美元收购) https://www.vectary.com/ https://clara.io/  ","tags":["3D","AR"],"title":"BIM和CAD的对比","type":"post"},{"authors":null,"categories":["IT"],"content":" On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data. Customization: Colors,labels,thickness of line, title, opacity, grid, figsize, ticks of axis and linestyle  How TensorBoard gets data from TensorFlow The first step in using TensorBoard is acquiring data from your TensorFlow run. For this, you need summary ops. Summary ops are ops, like tf.matmul or tf.nn.relu, which means they take in tensors, produce tensors, and are evaluated from within a TensorFlow graph. However, summary ops have a twist: the Tensors they produce contain serialized protobufs, which are written to disk and sent to TensorBoard. To visualize the summary data in TensorBoard, you should evaluate the summary op, retrieve the result, and then write that result to disk using a summary.FileWriter.\nlearning rate Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region The following formula shows the relationship.\nnew_weight = existing_weight — learning_rate * gradient  Typically learning rates are configured naively at random by the user. At best, the user would leverage on past experiences (or other types of learning material) to gain the intuition on what is the best value to use in setting learning rates.\nAs such, it’s often hard to get it right. The below diagram demonstrates the different scenarios one can fall into when configuring the learning rate\nmost common ways to prevent overfitting in neural networks  Get more training data. Reduce the capacity of the network. Add weight regularization. Add dropout  activation function A function that takes the input signal and generates an output signal, but takes into account some kind of threshold is called an activation function\nCNN feature map state of art AI https://www.stateoftheart.ai/\nhttp://colah.github.io/posts/2014-07-Understanding-Convolutions/\nReferences  [1] https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10 [2] http://wubigo.com/2017/01/numpy-notes/  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"5b3065c2f225a6419d8aeb3abb124d09","permalink":"https://wubigo.com/post/2017-08-01-deep_learning_with_python/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/post/2017-08-01-deep_learning_with_python/","section":"post","summary":"On the other hand, convolution is most typically done with 3x3 windows and no stride (stride 1).\nMATPLOTLIB Matplot is a python library that help us to plot data. The easiest and basic plots are line, scatter and histogram plots.\n Line plot is better when x axis is time. Scatter is better when there is correlation between two variables Histogram is better when we need to see distribution of numerical data.","tags":null,"title":"Deep Learning with Python","type":"post"},{"authors":null,"categories":[],"content":"Some things worth noting in br_add_if:\n Only ethernet like devices can be added to bridge, as bridge is a layer 2 device. Bridges cannot be added to a bridge. New interface is set to promiscuous mode: dev_set_promiscuity(dev, 1)  https://goyalankit.com/blog/linux-bridge\n","date":1501202178,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501202178,"objectID":"4fd405e9b253ff208d5d75a489baf20d","permalink":"https://wubigo.com/post/linux-bridge/","publishdate":"2017-07-28T08:36:18+08:00","relpermalink":"/post/linux-bridge/","section":"post","summary":"Some things worth noting in br_add_if:\n Only ethernet like devices can be added to bridge, as bridge is a layer 2 device. Bridges cannot be added to a bridge. New interface is set to promiscuous mode: dev_set_promiscuity(dev, 1)  https://goyalankit.com/blog/linux-bridge","tags":["SDN","NFV"],"title":"Linux Bridge","type":"post"},{"authors":null,"categories":null,"content":" docker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.61.1 (x86_64-alpine-linux-musl) libcurl/7.61.1 LibreSSL/2.0.0 zlib/1.2.11 libssh2/1.8.0 nghttp2/1.32.0 Release-Date: 2018-09-05  kubectl apply -f cmd-override-pod.yaml kubectl logs command-override Usage: curl [options...] \u0026lt;url\u0026gt; --abstract-unix-socket \u0026lt;path\u0026gt; Connect via abstract Unix domain socket --anyauth Pick any authentication method -a, --append Append to target file when uploading  工具POD apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - name: busybox image: busybox:1.28 command: - sleep - \u0026quot;13600\u0026quot; imagePullPolicy: IfNotPresent restartPolicy: Always  busybox:latest has bug on nslookup\ndocker network create test 32024cd09daca748f8254468f4f00893afc2e1173c378919b1f378ed719f1618 docker run -dit --name nginx --network test nginx:alpine 7feaf1f0b4f3d421603bbb984854b753c7cbc6b581dd0a304d3b8fccf8c6604b $ docker run -it --rm --network test busybox:1.28 nslookup nginx Server: 127.0.0.11 Address 1: 127.0.0.11 Name: nginx Address 1: 172.22.0.2 nginx.test docker stop nginx docker network rm test  kubectl exec -ti busybox -- nslookup kubernetes.default kubectl run -it --image busybox test --restart=Never --rm nslookup kubernetes.default  无选择器服务 使用场景：\n 通过SERVICE连接到外部服务 连接到另一个名字空间或集群 迁移过程中访问遗留系统  步骤\n 创建服务  kind: Service apiVersion: v1 metadata: name: ext-db spec: ports: - protocol: TCP port: 80 targetPort: 3316   手动创建一个端点  kind: Endpoints apiVersion: v1 metadata: name: my-service subsets: - addresses: - ip: 10.8.0.2 ports: - port: 3316  kube-proxy mode kubectl get cm kube-proxy -n kube-system -o yaml \u0026gt; kube-proxy.yaml sed -i s/mode:\u0026quot;\u0026quot;/mode:\u0026quot;ipvs/ kube-proxy.yaml sec -i s/creationTimestamp:*// kube-proxy.yaml sed -i s/resourceVersion: \u0026quot;*\u0026quot;// kube-proxy.yaml kubectl apply -f kube-proxy.yaml sudo ipvsadm -Ln ... IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.96.0.1:443 rr -\u0026gt; 192.168.1.11:6443 Masq 1 1 0 TCP 10.96.0.10:53 rr -\u0026gt; 10.2.0.129:53 Masq 1 0 0 -\u0026gt; 10.2.0.132:53 Masq 1 0 0 TCP 10.99.128.143:44134 rr -\u0026gt; 10.2.12.103:44134 Masq 1 0 0 TCP 10.101.148.51:8080 rr -\u0026gt; 10.2.12.102:8080 Masq 1 0 0 TCP 10.101.148.51:9093 rr -\u0026gt; 10.2.12.102:9093 Masq 1 0 0 TCP 10.101.148.51:15010 rr -\u0026gt; 10.2.12.102:15010 Masq 1 0 0 TCP 10.101.148.51:15011 rr -\u0026gt; 10.2.12.102:15011 Masq 1 0 0 TCP 10.102.2.50:443 rr -\u0026gt; 10.2.0.131:8443 Masq 1 0 0 UDP 10.96.0.10:53 rr -\u0026gt; 10.2.0.129:53 Masq 1 0 0 -\u0026gt; 10.2.0.132:53 Masq 1 0 0 ...  Creating sample user  Create Service Account  dashboard-adminuser.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system   Create ClusterRoleBinding  asumming that cluster-admin exists(provisioned by kubeadmin or kops)\nadminuser-bind-clusteramdin.yaml\napiVersion: rbac.authorization.K8S.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.K8S.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl apply -f dashboard-adminuser.yaml   login with Bearer Token  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')  multi-tenant K8S clusters at network-level:  Namespaces Ingress rules allow/deny and ingress/egress Network Policies Network-aware Zones  Architect a multi-tenant system with kubernetes I don\u0026rsquo;t think there is one document out there really summaries everything. The link below is a bit old but can help outline some of the basics on how they build on K8S. Ultimately the primitives are the same but they abstract namespaces a bit and build it around RBAC. Coupled with a default vxlan (isolated) SDN plugin and their ingress routing, its a compelling multi-tenant solution that provides isolation and quotes at multiple levels.\nOpenshift really just adds some glue (a lot of it being devleoper workflow) on top of Kubernetes. What is nice is that RedHat continues to try and upstream features of origin into K8S where it makes sense.\nhttps://blog.openshift.com/building-kubernetes-bringing-google-scale-container-orchestration-to-the-enterprise/ https://www.reddit.com/r/kubernetes/comments/6qp24h/ask_kubernetes_how_would_you_architect_a/\n","date":1499904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499904000,"objectID":"a16545f8df9259854a34ab4a1e650274","permalink":"https://wubigo.com/post/k8s-notes/","publishdate":"2017-07-13T00:00:00Z","relpermalink":"/post/k8s-notes/","section":"post","summary":"docker Entrypoint vs k8s command     docker k8s     entry ENTRYPOINT command   arguments CMD args    k8s command and args override the default Entrypoint and Cmd\nDockerfile\nFROM alpine:3.8 RUN apk add --no-cache curl ethtool \u0026amp;\u0026amp; rm -rf /var/cache/apk/* CMD [\u0026quot;--version\u0026quot;] ENTRYPOINT [\u0026quot;curl\u0026quot;]  cmd-override-pod.yaml\napiVersion: v1 kind: Pod metadata: name: command-override labels: purpose: override-command spec: containers: - name: command-override-container image: bigo/curl:v1 command: [\u0026quot;curl\u0026quot;] args: [\u0026quot;--help\u0026quot;] restartPolicy: Never  docker run -it bigo/curl:v1 curl 7.","tags":["K8S","PAAS"],"title":"K8S notes","type":"post"},{"authors":null,"categories":null,"content":" proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm\n","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"66349127f34fa3acce2c6902605a99b7","permalink":"https://wubigo.com/post/2017-05-03-atomnote/","publishdate":"2017-05-03T00:00:00Z","relpermalink":"/post/2017-05-03-atomnote/","section":"post","summary":"proxy config for Windows  ~/.atom/.apmrc https-proxy = http://192.168.0.119:3128/ http-proxy = http://192.168.0.119:3128/  Set Up \u0026amp; Use Atom as a Markdown Editor https://www.portent.com/blog/content-strategy/atom-markdown.htm","tags":null,"title":"atom note","type":"post"},{"authors":null,"categories":[],"content":" # side car proxy\n 方法1  Namespace labels\nkubectl label ns servicea istio-injection=enabled  Istio watches over all the deployments and adds the side car container to our pods.This is achieved by leveraging what is called MutatingAdmissionWebhooks, this feature was introduced in Kubernetes 1.9. So before the resources get created, the web hook intercepts the requests, checks if “Istio injection” is enabled for that namespace, and then adds the side car container to the pod\n istioctl command line tool   PILOT = ENVOY CONTROL PLANE API SERVER\nPilot maintains a canonical representation of services in the mesh that is independent of the underlying platform. Platform-specific adapters in Pilot are responsible for populating this canonical model appropriately. For example, the Kubernetes adapter in Pilot implements the necessary controllers to watch the Kubernetes API server for changes to the pod registration information, ingress resources, and third-party resources that store traffic management rules. This data is translated into the canonical representation. An Envoy-specific configuration is then generated based on the canonical representation\nPilot enables service discovery, dynamic updates to load balancing pools and routing tables.\nYou can specify high-level traffic management rules through Pilot’s Rule configuration. These rules are translated into low-level configurations and distributed to Envoy instances\nK8S KUBE-PROXY Kubernetes services take care of maintaining the list of Pod endpoints it can route traffic to. And usually kube-proxy does the load balancing between these pod endpoints. ENVOY client side load balancing do not want kube-proxy to load balance, we want to get the list of Pod endpoints and load balance it ourselves. For this we can use a “headless service”, which will just return the list of endpoints.\n Client-side Load Balancing  Many are familiar with what server-side load balancing is but the lesser known, client-side load balancing, has begun to climb in popularity due to SOA and microservices. Instead of relying on another service to distribute the load, the client itself, is responsible for deciding where to send the traffic also using an algorithm like round-robin. It can either discover the instances, via service discovery, or can be configured with a predefined list. Netflix Ribbon is an example of a client-side load balancer.\n安装  启用代理envoy（pilot.sidecar=true）  helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set sidecarInjectorWebhook.enabled=false --set pilot.sidecar=true   检查POD  istio-pilot包含两个容器： discovery 和 istio-proxy\nkubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-pilot-786dc4c88d-vnsr9 2/2 Running 0 15m   检查代理  kubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c istio-proxy -- bash # cd /etc/istio/proxy/ # ls envoy.yaml envoy_pilot.yaml.tmpl envoy_policy.yaml.tmpl envoy_telemetry.yaml.tmpl # ps fax PID TTY STAT TIME COMMAND 64 pts/2 Ss 0:00 bash 74 pts/2 R+ 0:00 \\_ ps fax 1 ? Ssl 0:00 /usr/local/bin/pilot-agent proxy --serviceCluster istio-pilot --templateFile /etc/istio/proxy/envoy_pilot.yaml.tmpl --controlPlaneAuthPolicy NONE 15 ? Sl 0:14 /usr/local/bin/envoy -c /etc/istio/proxy/envoy.yaml --restart-epoch 0 --drain-time-s 2 --parent-shutdown-time-s 3 --service-cluster istio-pilot --service-node sidecar~10.2.12.70   检查 discovery  kubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash # ls -l /etc/istio/config/ total 0 lrwxrwxrwx 1 root root 11 Mar 30 06:52 mesh -\u0026gt; ..data/mesh # ps fax PID TTY STAT TIME COMMAND 61 pts/0 Ss 0:00 bash 71 pts/0 R+ 0:00 \\_ ps fax 1 ? Ssl 1:55 /usr/local/bin/pilot-discovery discovery   检查日志  PodUID=${kubectl get pod -n istio-system istio-pilot-786dc4c88d-vnsr9 -o=jsonpath='{.metadata.uid}}' scp vm4:/var/log/pods/50f3507c-52b8-11e9-9372-08002775f493/istio-proxy/1.log ~./   检查proxy by adminPort  进入容器查看\nkubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash #curl http://localhost:15000/  或本地代理\nkubectl port-forward -n istio-system istio-pilot-786dc4c88d-vnsr9 15000:15000  pilot地址\nistio-pilot:release-1.0-latest-daily没有把服务端口通过EXPOSE暴露， 通过inspect查找\nkubectl exec -n istio-system istio-pilot-786dc4c88d-ls2z6 -c discovery env | grep \u0026quot;ISTIO_PILOT\u0026quot; ISTIO_PILOT_PORT=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_8080_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_PORT_HTTP_MONITORING=9093 ISTIO_PILOT_PORT_15010_TCP_PROTO=tcp ISTIO_PILOT_PORT_15010_TCP_PORT=15010 ISTIO_PILOT_SERVICE_PORT=15010 ISTIO_PILOT_PORT_15011_TCP=tcp://10.111.94.9:15011 ISTIO_PILOT_PORT_15011_TCP_PROTO=tcp ISTIO_PILOT_PORT_9093_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTP_LEGACY_DISCOVERY=8080 ISTIO_PILOT_PORT_15011_TCP_PORT=15011 ISTIO_PILOT_PORT_8080_TCP=tcp://10.111.94.9:8080 ISTIO_PILOT_PORT_8080_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTPS_XDS=15011 ISTIO_PILOT_PORT_9093_TCP=tcp://10.111.94.9:9093 ISTIO_PILOT_SERVICE_PORT_GRPC_XDS=15010 ISTIO_PILOT_PORT_8080_TCP_PORT=8080 ISTIO_PILOT_PORT_9093_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_HOST=10.111.94.9 ISTIO_PILOT_PORT_15010_TCP=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_15010_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_15011_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_9093_TCP_PORT=9093  docker inspect --format='{{range .Config.Env}}{{println .}}{{end}}' istio-pilot docker inspect --format='{{range .Config.Env}}{{println .}}{{end}}' ab92d1c866ce | grep \u0026quot;ISTIO_PILOT_*\u0026quot; ISTIO_PILOT_PORT=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_8080_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_PORT_HTTP_MONITORING=9093 ISTIO_PILOT_PORT_15010_TCP_PROTO=tcp ISTIO_PILOT_PORT_15010_TCP_PORT=15010 ISTIO_PILOT_SERVICE_PORT=15010 ISTIO_PILOT_PORT_15011_TCP=tcp://10.111.94.9:15011 ISTIO_PILOT_PORT_15011_TCP_PROTO=tcp ISTIO_PILOT_PORT_9093_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTP_LEGACY_DISCOVERY=8080 ISTIO_PILOT_PORT_15011_TCP_PORT=15011 ISTIO_PILOT_PORT_8080_TCP=tcp://10.111.94.9:8080 ISTIO_PILOT_PORT_8080_TCP_PROTO=tcp ISTIO_PILOT_SERVICE_PORT_HTTPS_XDS=15011 ISTIO_PILOT_PORT_9093_TCP=tcp://10.111.94.9:9093 ISTIO_PILOT_SERVICE_PORT_GRPC_XDS=15010 ISTIO_PILOT_PORT_8080_TCP_PORT=8080 ISTIO_PILOT_PORT_9093_TCP_ADDR=10.111.94.9 ISTIO_PILOT_SERVICE_HOST=10.111.94.9 ISTIO_PILOT_PORT_15010_TCP=tcp://10.111.94.9:15010 ISTIO_PILOT_PORT_15010_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_15011_TCP_ADDR=10.111.94.9 ISTIO_PILOT_PORT_9093_TCP_PORT=9093  kubectl exec -it -n istio-system istio-pilot-786dc4c88d-vnsr9 -c discovery -- bash #cat /etc/istio/config/mesh | grep discoveryAddress   pilot-agent      default debug     \u0026ndash;log_output_level default:info default:debug   \u0026ndash;log_stacktrace_level default:none default:debug    Comma-separated minimum per-scope logging level of messages to output, in the form of \u0026lt;scope\u0026gt;:\u0026lt;level\u0026gt;,\u0026lt;scope\u0026gt;:\u0026lt;level\u0026gt;,... where scope can be one of [default, model, rbac] and level can be one of [debug, info, warn, error, fatal, none] (default `default:info`)   调试istio-discovery  kubectl get deployments -n istio-system -o json \u0026gt; istio.k8s.deployment.json  discovery调试信息\u0026ndash;log_output_level\n \u0026quot;args\u0026quot;: [ \u0026quot;discovery\u0026quot;, \u0026quot;--log_output_level\u0026quot;, \u0026quot;default:debug\u0026quot; ]  proxy调试信息(/usr/local/bin/proxy -l debug)\nproxy被pilot-agent启动，所以调试日志还是和discovery一样\n \u0026quot;args\u0026quot;: [ \u0026quot;proxy\u0026quot;, \u0026quot;--serviceCluster\u0026quot;, \u0026quot;istio-pilot\u0026quot;, \u0026quot;--templateFile\u0026quot;, \u0026quot;/etc/istio/proxy/envoy_pilot.yaml.tmpl\u0026quot;, \u0026quot;--controlPlaneAuthPolicy\u0026quot;, \u0026quot;NONE\u0026quot;, \u0026quot;--log_output_level\u0026quot;, \u0026quot;default:debug\u0026quot; ]  kubectl apply -f istio.k8s.deployment.json  kubectl exec -it -n istio-system istio-pilot-84678c759f-qjbf4 -c discovery -- bash root@istio-pilot-84678c759f-qjbf4:/# ps -fax PID TTY STAT TIME COMMAND 28 pts/0 Ss 0:00 bash 39 pts/0 R+ 0:00 \\_ ps -fax 1 ? Ssl 0:28 /usr/local/bin/pilot-discovery discovery --log_output_level default:debug   下载配置  kubectl cp istio-system/istio-pilot-b8d58697f-5nthh:etc/istio/proxy/envoy.yaml ./ -c istio-proxy  PodUID=${kubectl get pod -n istio-system istio-pilot-786dc4c88d-vnsr9 -o=jsonpath='{.metadata.uid}' kubectl cp istio-system/istio-pilot-b8d58697f-5nthh:/etc/istio/proxy/envoy.yaml ./ -c istio-proxy  Adding Kubernetes registry adapter 2019-04-03T06:43:56.839512Z info Primary Cluster name: Kubernetes 2019-04-03T06:43:56.839600Z info Service controller watching namespace \u0026quot;\u0026quot; for service, endpoint, nodes and pods, refresh 60000000000 gc 4 @4.096s 4%: 0.043+22+4.4 ms clock, 0.087+1.2/6.0/13+8.9 ms cpu, 5-\u0026gt;5-\u0026gt;3 MB, 6 MB goal, 2 P 2019-04-03T06:43:56.852472Z debug empty Webhook API endpoint. 2019-04-03T06:43:56.875696Z info ads Starting ADS server with throttle=25 burst=100 2019-04-03T06:43:56.879233Z info Setting up event handlers 2019-04-03T06:43:56.879495Z info Discovery service started at http=[::]:8080 grpc=[::]:15010  ","date":1493535895,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493535895,"objectID":"8af54d26b1dec578465ae13bb710819c","permalink":"https://wubigo.com/post/k8s-istio-discovery-proxy/","publishdate":"2017-04-30T15:04:55+08:00","relpermalink":"/post/k8s-istio-discovery-proxy/","section":"post","summary":"# side car proxy\n 方法1  Namespace labels\nkubectl label ns servicea istio-injection=enabled  Istio watches over all the deployments and adds the side car container to our pods.This is achieved by leveraging what is called MutatingAdmissionWebhooks, this feature was introduced in Kubernetes 1.9. So before the resources get created, the web hook intercepts the requests, checks if “Istio injection” is enabled for that namespace, and then adds the side car container to the pod","tags":["K8S"],"title":"K8s Istio Pilot as envoy control place","type":"post"},{"authors":null,"categories":[],"content":" 特性  与hadoop集成，支持MR数据读取 二级索引 支持长行（最多20亿的列） 动态schema更改 bulk load 其他数据源如hadoop with sstableloader， CSV importing with cqlsh DTCS优化时序数据性能  DB CATEGORY BY CAP  CA  To primarily support consistency and availability means that you’re likely using two-phase commit for distributed transactions. It means that the system will block when a network partition occurs, so it may be that your system is limited to a single data center cluster in an attempt to mitigate this. If your application needs only this level of scale, this is easy to manage and allows you to rely on familiar, simple structures.\n CP  To primarily support consistency and partition tolerance, you may try to advance your architecture by setting up data shards in order to scale. Your data will be consistent, but you still run the risk of some data becoming unavailable if nodes fail.\n AP  To primarily support availability and partition tolerance, your system may return inaccurate data, but the system will always be available, even in the face of network partitioning. DNS is perhaps the most popular example of a system that is massively scalable, highly available, and partition tolerant.\n Cassandra uses a special primary key called a composite key (or compound key) to represent wide rows, also called partitions. The composite key consists of a partition key, plus an optional set of clustering columns. The partition key is used to determine the nodes on which rows are stored and can itself consist of multiple columns. The clustering columns are used to control how data is sorted for storage within a partition. Cassandra also supports an additional construct called a static column, which is for storing data that is not part of the primary key but is shared by every row in a partition  Server-Side Denormalization with Materialized Views Historically, denormalization in Cassandra has required designing and managing multiple tables using techniques we will introduce momentarily. Beginning with the 3.0 release, Cassandra provides a feature known as materialized views which allows us to create multiple denormalized views of data based on a base table design. Cassandra manages materialized views on the server, including the work of keeping the views in sync with the table\nMaterialized views simplify application development: instead of the application having to keep multiple denormalized tables in sync, Cassandra takes on the responsibility of updating views in order to keep them consistent with the base table\nPrimary Keys Are Forever After you create a table, there is no way to modify the primary key, because this controls how data is distributed within the cluster, and even more importantly, how it is stored on disk.\n","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"283f625d2a671c49b6886482df3aff47","permalink":"https://wubigo.com/post/cassandra/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/cassandra/","section":"post","summary":"特性  与hadoop集成，支持MR数据读取 二级索引 支持长行（最多20亿的列） 动态schema更改 bulk load 其他数据源如hadoop with sstableloader， CSV importing with cqlsh DTCS优化时序数据性能  DB CATEGORY BY CAP  CA  To primarily support consistency and availability means that you’re likely using two-phase commit for distributed transactions. It means that the system will block when a network partition occurs, so it may be that your system is limited to a single data center cluster in an attempt to mitigate this.","tags":["SHELL","NOSQL"],"title":"Cassandra","type":"post"},{"authors":null,"categories":[],"content":" \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  gateway with routing @EnableZuulServer is used when you want to build your own routing service and not use any Zuul prebuilt capabilities. An example of this would be if you wanted to use Zuul to integrate with a service discovery engine other than Eureka (for example, Consul). We’ll only use the @EnableZuulServer annotation in this book.\nThe Zuul proxy server is designed by default to work on the Spring products. As such, Zuul will automatically use Eureka to look up services by their service IDs and then use Netflix Ribbon to do client-side load balancing of requests from within Zuul.\n","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"ecac105488c8c9fb88e6f4fa40351b6b","permalink":"https://wubigo.com/post/spring-gateway/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/spring-gateway/","section":"post","summary":"\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  gateway with routing @EnableZuulServer is used when you want to build your own routing service and not use any Zuul prebuilt capabilities. An example of this would be if you wanted to use Zuul to integrate with a service discovery engine other than Eureka (for example, Consul). We’ll only use the @EnableZuulServer annotation in this book.\nThe Zuul proxy server is designed by default to work on the Spring products.","tags":["SPRING","MICROSERVICE"],"title":"Spring Gateway","type":"post"},{"authors":null,"categories":[],"content":" http://localhost:8080/oauth/token\ncurl -u eagleeye:thisissecret -i -H \u0026lsquo;Accept:application/json\u0026rsquo; -d \u0026ldquo;grant_type=password\u0026amp;scope=webclient\u0026amp;username=will\u0026amp;password=pass\u0026rdquo; -H \u0026ldquo;Content-Type: application/x-www-form-urlencoded\u0026rdquo; -X POST http://localhost:8080/oauth/token\naccess protected resource ","date":1493370487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493370487,"objectID":"132484b899dcf0aa9734b6ac65080af0","permalink":"https://wubigo.com/post/spring-oauth2/","publishdate":"2017-04-28T17:08:07+08:00","relpermalink":"/post/spring-oauth2/","section":"post","summary":" http://localhost:8080/oauth/token\ncurl -u eagleeye:thisissecret -i -H \u0026lsquo;Accept:application/json\u0026rsquo; -d \u0026ldquo;grant_type=password\u0026amp;scope=webclient\u0026amp;username=will\u0026amp;password=pass\u0026rdquo; -H \u0026ldquo;Content-Type: application/x-www-form-urlencoded\u0026rdquo; -X POST http://localhost:8080/oauth/token\naccess protected resource ","tags":["SHELL","MYSQL"],"title":"Spring Oauth2","type":"post"},{"authors":null,"categories":[],"content":" 提示： 以下操作是在VirtualBox虚机环境，并做如下配置\n 网络  下拉高级设置，在\u0026rdquo;Adapter Type\u0026rdquo;选择PCnet-FAST III\u0026rdquo;, 而不是默认的e1000 (Intel PRO/1000). 另外\u0026rdquo;Promiscuous Mode\u0026rdquo;必须设置为\u0026rdquo;Allow All\u0026rdquo;. 否则通过网桥连接的容器无法工作, 因为虚拟网卡 会过滤掉掉所有带有不同MAC的数据包。\n 多网卡  每块网卡都要做上述调整\n准备  安装util-linux  sudo apt install util-linux  /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.255.255.0 dns-nameservers 192.168.1.1  ip route default via 192.168.1.1 dev enp0s3 onlink 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev enp0s8 proto kernel scope link src 192.168.1.16  使用NAT  docker host network  assign a second ip to host interface\nexport SIP=192.168.1.117 sudo ip addr add $SIP/24 dev enp0s3  bind container to SIP host network\ndocker run -it --name web -p ${SIP}:80:80 nginx:1.14-alpine sudo iptables -L DOCKER -v -n Chain DOCKER (1 references) pkts bytes target prot opt in out source destination 7 528 ACCEPT tcp -- !docker0 docker0 0.0.0.0/0 172.17.0.2 tcp dpt:80    sudo iptables -t nat -I POSTROUTING -s 172.17.0.2 \\ -j SNAT --to-source 192.168.1.119 sudo iptables -t nat -L -n -v  使用LINUX网桥  查看网卡的ip  ifconfig enp0s8 enp0s3 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16   创建网桥br-enp0s8并把enp0s8的IP分配给网桥，把enp0s8连接到网桥  sudo brctl addbr br-enp0s8 sudo ip link set br-enp0s8 up sudo ip addr add 192.168.1.111/24 dev br-enp0s8; \\ ip addr del 192.168.1.111/24 dev enp0s8; \\ brctl addif br-enp0s8 enp0s8; \\ ip route del default; \\ ip route add default via 192.168.1.1 dev br-enp0s8 ifconfig br-enp0s8 br-enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68 inet addr:192.168.1.16 ifconfig enp0s8 enp0s8 Link encap:Ethernet HWaddr 08:00:27:4e:18:68  br-enp0s8和enp0s8拥有相同的HWaddr(Mac地址)\n 确认网络是否对外连接正常  ip route default via 192.168.1.1 dev br-enp0s8 169.254.0.0/16 dev enp0s3 scope link metric 1000 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 192.168.1.0/24 dev enp0s3 proto kernel scope link src 192.168.1.10 192.168.1.0/24 dev br-enp0s8 proto kernel scope link src 192.168.1.16  curl -IL https://wubigo.com HTTP/1.1 200 OK   启动容器  docker run -it --rm --name web -p 80 nginx:1.14-alpine   创建veth接口对web-int/web-ext:  sudo ip link add web-int type veth peer name web-ext   连接veth一端web-ext到网桥  sudo brctl addif br-enp0s8 web-ext   连接veth的另一端web-int连接到容器的网络名字空间  sudo ip link set netns $(docker-pid web) dev web-int sudo nsenter -t $(docker-pid web) -n ip link set web-int up sudo nsenter -t $(docker-pid web) -n ip addr add 192.168.1.117/24 dev web-int   检查容器已经连接到web-int并且ip地址正确分配  docker exec -it web ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:0.0.0.0 Mask:255.255.0.0 inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:84 errors:0 dropped:0 overruns:0 frame:0 TX packets:21 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:9396 (9.1 KiB) TX bytes:2348 (2.2 KiB) web-int Link encap:Ethernet HWaddr 5A:1D:90:CF:6B:2C inet addr:192.168.1.117 Bcast:0.0.0.0 Mask:255.255.255.0 UP BROADCAST MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)  docker exec -it web ip route default via 172.17.0.1 dev eth0 172.17.0.0/16 dev eth0 scope link src 172.17.0.2 192.168.1.0/24 dev web-int scope link src 192.168.1.117   设置web-int为容器路由默认接口  sudo nsenter -t $(docker-pid web) -n ip route del default sudo nsenter -t $(docker-pid web) -n ip route add default via 192.168.1.1 dev web-int   测试清理  docker rm web sudo ip link set br-enp0s8 down sudo brctl delbr br-enp0s8  ","date":1493075455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493075455,"objectID":"16cf3f49bb97c1d9ad4d0ad448b4417c","permalink":"https://wubigo.com/post/connect-container-to-host-network/","publishdate":"2017-04-25T07:10:55+08:00","relpermalink":"/post/connect-container-to-host-network/","section":"post","summary":"提示： 以下操作是在VirtualBox虚机环境，并做如下配置\n 网络  下拉高级设置，在\u0026rdquo;Adapter Type\u0026rdquo;选择PCnet-FAST III\u0026rdquo;, 而不是默认的e1000 (Intel PRO/1000). 另外\u0026rdquo;Promiscuous Mode\u0026rdquo;必须设置为\u0026rdquo;Allow All\u0026rdquo;. 否则通过网桥连接的容器无法工作, 因为虚拟网卡 会过滤掉掉所有带有不同MAC的数据包。\n 多网卡  每块网卡都要做上述调整\n准备  安装util-linux  sudo apt install util-linux  /etc/network/interface\ncat interfaces # interfaces(5) file used by ifup(8) and ifdown(8) auto lo iface lo inet loopback auto enp0s3 iface enp0s3 inet static address 192.168.1.10 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1 auto enp0s8 iface enp0s8 inet static address 192.168.1.16 netmask 255.","tags":["DOCKER","NETWORK"],"title":"容器多种方式链接宿主网络","type":"post"},{"authors":null,"categories":[],"content":" the architecture of gRPC is layered: The lowest layer is the transport: gRPC uses HTTP/2 as its transport protocol. HTTP/2 provides the same basic semantics as HTTP 1.1 (the version with which nearly all developers are familiar), but aims to be more efficient and more secure. The new features in HTTP/2 that are most obvious at first glance are (1) that it can multiplex many parallel requests over the same network connection and (2) that it allows full-duplex bidirectional communication. We’ll learn more about HTTP/2 and the ways it differs from and improves on HTTP 1.1 later in the book.\nThe next layer is the channel: This is a thin abstraction over the transport. The channel defines calling conventions and implements the mapping of an RPC onto the underlying transport. At this layer, a gRPC call consists of a client-provided service name and method name, optional request metadata (key-value pairs), and zero or more request messages. A call is completed when the server provides optional response header metadata, zero or more response messages, and response trailer metadata. The trailer metadata indicates the final disposition of the call: whether it was a success or a failure. At this layer, there is no knowledge of interface constraints, data types, or message encoding. A message is just a sequence of zero or more bytes. A call may have any number of request and response messages.\nThe last layer is the stub: The stub layer is where interface constraints and data types are defined. Does a method accept exactly one request message or a stream of request messages? What kind of data is in each response message and how is it encoded? The answers to these questions are provided by the stub. The stub marries the IDL-defined interfaces to a channel. The stub code is generated from the IDL. The channel layer provides the ABI that these generated stubs use.\nStreaming When a very large amount of data must be exchanged, this can mean significant memory pressure on both the client process and the server process. And it means that operations must typically impose hard limits on the size of request and response messages, to prevent resource exhaustion. Streaming alleviates this by allowing the request or response to be an arbitrarily long sequence of messages. The cumulative total size of a request or response stream may be incredibly large, but clients and servers do not need to store the entire stream in memory. Instead, they can operate on a subset of data, even as little as just one message at a time.\nNot only does gRPC support streaming, but it also supports full-duplex bidirectional streams. Bidirectional means that the client can use a stream to upload an arbitrary amount of request data and the server can use a stream to send back an arbitrary amount of response data, all in the same RPC. The novel part is the “full-duplex” part. Most request-response protocols, including HTTP 1.1 are “half-duplex.” They support bidirectional communication (HTTP 1.1 even supports bidirectional streaming), but the two directions cannot be used at the same time. A request must first be fully uploaded before the server begins responding; only after the client is done transmitting can the server then reply with its full response. gRPC is built on HTTP/2, which explicitly supports full-duplex streams, which means that the client can upload request data at the same time the server is sending back response data. This is very powerful and eliminates the need for things like web sockets, which is an extension of HTTP 1.1, to allow full-duplex communication over an HTTP 1.1 connection. Thanks to streaming, applications can build very sophisticated conversational protocols on top of gRPC.\n","date":1491170022,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491170022,"objectID":"4e37a31092cf539cc02f79edb1e32e0c","permalink":"https://wubigo.com/post/rpc-grpc/","publishdate":"2017-04-03T05:53:42+08:00","relpermalink":"/post/rpc-grpc/","section":"post","summary":"the architecture of gRPC is layered: The lowest layer is the transport: gRPC uses HTTP/2 as its transport protocol. HTTP/2 provides the same basic semantics as HTTP 1.1 (the version with which nearly all developers are familiar), but aims to be more efficient and more secure. The new features in HTTP/2 that are most obvious at first glance are (1) that it can multiplex many parallel requests over the same network connection and (2) that it allows full-duplex bidirectional communication.","tags":["RPC"],"title":"RPC GRPC","type":"post"},{"authors":null,"categories":[],"content":"使用一个没有被占用的网段设置DOCKER_GATEWAY\nexport DOCKER_GATEWAY=172.28.0.1  URL=https://github.com/istio/istio/releases/download/1.1.1/istio-1.1.1-linux.tar.gz curl -L \u0026quot;$URL\u0026quot; | tar xz cd istio-1.1.1 docker-compose -f install/consul/istio.yaml up -d  Configure kubectl to use mapped local port for the API server:\nkubectl config set-context istio --cluster=istio kubectl config set-cluster istio --server=http://localhost:8080 kubectl config use-context istio  docker-compose -f samples/bookinfo/platform/consul/bookinfo.yaml up -d  kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml kubectl get destinationrules -o yaml  kubectl apply -f samples/bookinfo/platform/consul/virtual-service-all-v1.yaml  docker-compose -f bookinfo.yaml exec details-v1 sh #cat /etc/resolv.conf search service.consul nameserver 127.0.0.11 options ndots:0  docker run -it --rm --network consul_istiomesh busybox:glibc #cat /etc/resolv.conf  destinationrules\napiVersion: v1 items: - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;details\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;details.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v2\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: details namespace: default resourceVersion: \u0026quot;106\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/details uid: 4666f266-5530-11e9-bf95-0242ac1c000d spec: host: details.service.consul subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;productpage.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: productpage namespace: default resourceVersion: \u0026quot;103\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/productpage uid: 465ee98f-5530-11e9-bf95-0242ac1c000d spec: host: productpage.service.consul subsets: - labels: version: v1 name: v1 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;ratings\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;ratings.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: ratings namespace: default resourceVersion: \u0026quot;105\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/ratings uid: 4662363d-5530-11e9-bf95-0242ac1c000d spec: host: ratings.service.consul subsets: - labels: version: v1 name: v1 - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;DestinationRule\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;reviews\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;reviews.service.consul\u0026quot;,\u0026quot;subsets\u0026quot;:[{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v1\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v2\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v2\u0026quot;},{\u0026quot;labels\u0026quot;:{\u0026quot;version\u0026quot;:\u0026quot;v3\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;v3\u0026quot;}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:15:43Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: reviews namespace: default resourceVersion: \u0026quot;104\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/destinationrules/reviews uid: 46605c8c-5530-11e9-bf95-0242ac1c000d spec: host: reviews.service.consul subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 - labels: version: v3 name: v3 kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot;  VirtualService\napiVersion: v1 items: - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;details\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;details.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;details.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: details namespace: default resourceVersion: \u0026quot;110\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/details uid: 95f21f5f-5530-11e9-bf95-0242ac1c000d spec: hosts: - details.service.consul http: - route: - destination: host: details.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;productpage\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;productpage.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;productpage.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: productpage namespace: default resourceVersion: \u0026quot;107\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/productpage uid: 95ea84fc-5530-11e9-bf95-0242ac1c000d spec: hosts: - productpage.service.consul http: - route: - destination: host: productpage.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;ratings\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;ratings.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;ratings.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: ratings namespace: default resourceVersion: \u0026quot;109\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/ratings uid: 95ee32e2-5530-11e9-bf95-0242ac1c000d spec: hosts: - ratings.service.consul http: - route: - destination: host: ratings.service.consul subset: v1 - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026quot;apiVersion\u0026quot;:\u0026quot;networking.istio.io/v1alpha3\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;VirtualService\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;reviews\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;hosts\u0026quot;:[\u0026quot;reviews.service.consul\u0026quot;],\u0026quot;http\u0026quot;:[{\u0026quot;route\u0026quot;:[{\u0026quot;destination\u0026quot;:{\u0026quot;host\u0026quot;:\u0026quot;reviews.service.consul\u0026quot;,\u0026quot;subset\u0026quot;:\u0026quot;v1\u0026quot;}}]}]}} clusterName: \u0026quot;\u0026quot; creationTimestamp: \u0026quot;2019-04-02T10:17:57Z\u0026quot; deletionGracePeriodSeconds: null deletionTimestamp: null name: reviews namespace: default resourceVersion: \u0026quot;108\u0026quot; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/default/virtualservices/reviews uid: 95eb9df1-5530-11e9-bf95-0242ac1c000d spec: hosts: - reviews.service.consul http: - route: - destination: host: reviews.service.consul subset: v1 kind: List metadata: resourceVersion: \u0026quot;\u0026quot; selfLink: \u0026quot;\u0026quot;  ","date":1491118301,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491118301,"objectID":"b4198015380af687e4523385cda01e51","permalink":"https://wubigo.com/post/istio-pilot-docker/","publishdate":"2017-04-02T15:31:41+08:00","relpermalink":"/post/istio-pilot-docker/","section":"post","summary":"使用一个没有被占用的网段设置DOCKER_GATEWAY\nexport DOCKER_GATEWAY=172.28.0.1  URL=https://github.com/istio/istio/releases/download/1.1.1/istio-1.1.1-linux.tar.gz curl -L \u0026quot;$URL\u0026quot; | tar xz cd istio-1.1.1 docker-compose -f install/consul/istio.yaml up -d  Configure kubectl to use mapped local port for the API server:\nkubectl config set-context istio --cluster=istio kubectl config set-cluster istio --server=http://localhost:8080 kubectl config use-context istio  docker-compose -f samples/bookinfo/platform/consul/bookinfo.yaml up -d  kubectl apply -f samples/bookinfo/platform/consul/destination-rule-all.yaml kubectl get destinationrules -o yaml  kubectl apply -f samples/bookinfo/platform/consul/virtual-service-all-v1.yaml  docker-compose -f bookinfo.yaml exec details-v1 sh #cat /etc/resolv.","tags":[],"title":"Istio Pilot Docker","type":"post"},{"authors":null,"categories":null,"content":" Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/\n","date":1491091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491091200,"objectID":"29b330b2e5eaea4b52637c2de1262033","permalink":"https://wubigo.com/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","publishdate":"2017-04-02T00:00:00Z","relpermalink":"/post/2017-04-02-microserviceplatformspring-cloudvskubernetes/","section":"post","summary":"Microservice platform Spring-cloud VS Kubernetes https://developers.redhat.com/blog/2016/12/09/spring-cloud-for-microservices-compared-to-kubernetes/","tags":null,"title":"Microservice platform Spring-cloud VS Kubernetes","type":"post"},{"authors":null,"categories":[],"content":" Before Kubernetes version 1.11, the Kubernetes DNS service was based on kube-dns. Version 1.11 introduced CoreDNS to address some security and stability concerns with kube-dns.\nRegardless of the software handling the actual DNS records, both implementations work in a similar manner:\n A service named kube-dns and one or more pods are created. The kube-dns service listens for service and endpoint events from the Kubernetes API and updates its DNS records as needed. These events are triggered when you create, update or delete Kubernetes services and their associated pods. kubelet sets each new pod\u0026rsquo;s /etc/resolv.conf nameserver option to the cluster IP of the kube-dns service, with appropriate search options to allow for shorter hostnames to be used: Applications running in containers can then resolve hostnames such as example-service.namespace into the correct cluster IP addresses.  Kubernetes DNS Records  SVC    service.namespace.svc.cluster.local\n  POD    10.2.9.4.namespace.pod.cluster.local\n addressing a service in the same namespace\nnslookup other-svc  addressing a service in a different namespace\nnslookup other-svc.other-ns  Pod’s dnsPolicy Note: “Default” is not the default DNS policy. If dnsPolicy is not explicitly specified, then “ClusterFirst” is used.\n“ClusterFirst“: Any DNS query that does not match the configured cluster domain suffix, such as “www.kubernetes.io”, is forwarded to the upstream nameserver inherited from the node. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See related discussion for details on how DNS queries are handled in those cases.\ncustomize pod dns with dnsConfig busybox has bug on nslookup of k8s svc addressing,\nuse alpine instead\nbigo@bigo-HP:~/wubigo.github.io$ kubectl run -it --image curl:v1 curl --restart=Never --rm -- sh If you don't see a command prompt, try pressing enter. / # nslookup kubernetes Name: kubernetes Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local / # nslookup nginx Name: nginx Address 1: 10.2.12.99 web-0.nginx.default.svc.cluster.local  coredns CM kubectl -n kube-system get configmap coredns -o yaml kubectl -n kube-system edit configmap coredns data: Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance } kind: ConfigMap metadata: creationTimestamp: \u0026quot;2019-02-19T06:54:07Z\u0026quot; name: coredns namespace: kube-system resourceVersion: \u0026quot;561721\u0026quot; selfLink: /api/v1/namespaces/kube-system/configmaps/coredns uid: 2732a277-3413-11e9-86cc-08002775f493  ","date":1491060642,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491060642,"objectID":"5a7cba9adb40573c37beaa0bf81d912a","permalink":"https://wubigo.com/post/k8s-dns/","publishdate":"2017-04-01T23:30:42+08:00","relpermalink":"/post/k8s-dns/","section":"post","summary":"Before Kubernetes version 1.11, the Kubernetes DNS service was based on kube-dns. Version 1.11 introduced CoreDNS to address some security and stability concerns with kube-dns.\nRegardless of the software handling the actual DNS records, both implementations work in a similar manner:\n A service named kube-dns and one or more pods are created. The kube-dns service listens for service and endpoint events from the Kubernetes API and updates its DNS records as needed.","tags":["K8S"],"title":"K8s DNS","type":"post"},{"authors":null,"categories":[],"content":" set date FROM alpine:3.8 RUN apk add --no-cache tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai RUN ln -s /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone  docker run -it --rm -e TZ=Asia/Shanghai alpine:3.8 ash  创建/etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  ","date":1490944038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490944038,"objectID":"b639ba3bccaf25e3b32bbf6c60c904ec","permalink":"https://wubigo.com/post/docker-alpine/","publishdate":"2017-03-31T15:07:18+08:00","relpermalink":"/post/docker-alpine/","section":"post","summary":" set date FROM alpine:3.8 RUN apk add --no-cache tzdata \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV TZ Asia/Shanghai RUN ln -s /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone  docker run -it --rm -e TZ=Asia/Shanghai alpine:3.8 ash  创建/etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  ","tags":["DOCKER"],"title":"Docker Alpine","type":"post"},{"authors":null,"categories":[],"content":" PodUID kubectl get pod \u0026lt;PID_NAME\u0026gt; -o=jsonpath='{.metadata.uid}'  POD on disk /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  In a production cluster, logs are usually collected, aggregated, and shipped to a remote store where advanced analysis/search/archiving functions are supported. In kubernetes, the default cluster-addons includes a per-node log collection daemon, fluentd. To facilitate the log collection, kubelet creates symbolic links to all the docker containers logs under /var/log/containers with pod and container metadata embedded in the filename.\n/var/log/containers/__-.log`\n/var/log/containers/\nls -l  ","date":1490830896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490830896,"objectID":"a96770cd1787cbef0328436c23093cc0","permalink":"https://wubigo.com/post/k8s-kubelet/","publishdate":"2017-03-30T07:41:36+08:00","relpermalink":"/post/k8s-kubelet/","section":"post","summary":"PodUID kubectl get pod \u0026lt;PID_NAME\u0026gt; -o=jsonpath='{.metadata.uid}'  POD on disk /var/lib/kubelet/pods/\u0026lt;PodUID\u0026gt;/\n/var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;\nls -l /var/log/pods/\u0026lt;PodUID\u0026gt;/\u0026lt;container_name\u0026gt;/ lrwxrwxrwx 1 root root 165 3月 30 06:52 0.log -\u0026gt; /var/lib/docker/containers/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021/e74eafc4b3f0cfe2e4e0462c93101244414eb3048732f409c29cc54527b4a021-json.log  In a production cluster, logs are usually collected, aggregated, and shipped to a remote store where advanced analysis/search/archiving functions are supported. In kubernetes, the default cluster-addons includes a per-node log collection daemon, fluentd. To facilitate the log collection, kubelet creates symbolic links to all the docker containers logs under /var/log/containers with pod and container metadata embedded in the filename.","tags":["K8S"],"title":"K8s Kubelet","type":"post"},{"authors":null,"categories":[],"content":" 你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.\n 会用好上司的长处，才是你高效能的关键\n（文章来源于：维小维生素摘编）\n","date":1490173687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490173687,"objectID":"ea851af3bade29bf69c3197f95ac6ee4","permalink":"https://wubigo.com/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","publishdate":"2017-03-22T17:08:07+08:00","relpermalink":"/post/2018-03-28-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%BA%BA%E8%84%89/","section":"post","summary":"你的老板，才是你最重要的人脉\n我相信这世界上，90% 的人都有过诅咒上司的一刻。偏执、变态、有病这些词，都是恨不得改成老板微信标签的定语。\n但是说大实话，年纪轻轻就有人促使你学会面对职场残酷的人，那不是一个贵人是什么？\n但是我们该如何面对这个残忍的贵人呢？我掏心掏肺给你一点方向。\n上司不是用来喜欢的 他不过是你的资源 最近老同事 Susan 给我打来了电话，很苦恼：我怎么就摊上了这样的破上司呢？\n不等我回答，她就直接自问自答：老板觉得我有能力，但是认为我对他不够尊重，就因为这样竟然就要封杀我！我要跳槽了！\n我笑了，你的情形再惨，惨得过孙悟空吗？毕竟唐僧动不动就给他念紧箍咒，没两天就叫他收拾包袱走人，他不也一样跟唐僧一起去取了个经回来吗。\n这无非是因为，孙悟空是足够聪明的猴子。\n他知道，唐僧是他老孙的一个重要资源，而不仅仅是他的上司。哪怕师傅再迂腐无能，再是非不分，他还是能调动观音菩萨、如来 佛祖来到身边的人。\n「要换做是我」，我笑着说，「尊重还不容易吗，态度好点就行了，直到他把行业诸多诀窍都教会我，再走。」\nSusan 开始若有所思了。她和大部分年轻人一样，对这个道理都不太懂。\n大部分人觉得：老板是资方，自己是劳方，天生就是对立的两端。毕竟在现实生活中逼你干活、骂你不对、给你 Mission impossible 的人就是他啊。\n但是聪明人却是这么看的：老板是小宇宙，自己也是小宇宙，只要懂得挖，他就是我的合作方，我的资源库。\n话说大名鼎鼎的格力董明珠，也有一个相处得不太顺当的前老板朱江洪。\n1991 年，朱江洪是格力的总经理，董明珠是一名普通的销售。一个技术狂，一个铁娘子，免不了争吵。\n然而争吵归争吵，董明珠还是在朱江洪的提拔下，从经营部长、大区经理、副总、总裁做到到格力集团董事长。两个人合作，把格力带进了千亿俱乐部，甚至突破了两千亿销售大关。\n董明珠就是这样一个足够机智的人。吵架归吵架，矛盾归矛盾，心底里骂过对方多少句「傻X」没关系，互相依赖、相互合作做出成绩来才是关键。\n每一个老板，无论多差劲，好歹都有一丝半点闪光点，否则他坐不到老板的位置上\n这样一个人，就是你人生长河中一个重要的资源库。\n多骂他几句「有病」，形成对立关系，无非是绝了自己获取人脉、信息、协助、建议的后路，这不是很笨吗？\n所有的职场恩怨，都起源于沟通不够 我刚开始工作的公司是个外企，沟通风格相当自由。\n上司和下属之间，有工作谈工作，没工作谈目标，没什么不能直接说的。很多人，哪怕跨了级，情同姐妹、称兄道弟的却比比皆是。\n然而当我到第一个民企工作的时候就感觉很压抑了。\n老板喜欢拍马屁，下属喜欢越位拍马屁，办公室流行着欲言又止的气氛，私下拉小群组帮派却热火朝天。所谓的人事厚黑学、信息不对称无处不在，我也算是开眼界了。\n然而我始终是一个好学宝宝，遇到问题和困难都喜欢探究个为什么。\n我发现，造成这么多职场政治的终极原因，无非就是沟通的问题。\n上司和下属之间沟通不明确，不拿出真心来说清楚要求，说一句留半句，剩下 9 句让你猜，这才是致命的低效源头\n俄国作家契科夫曾经写过小说《公务员之死》。这个倒霉蛋公务员就是不会沟通的典型。\n他不小心打喷嚏的时候把鼻涕喷在前座的将军身上，公务员连忙道歉，将军撇了撇嘴说「没关系，算了」。\n然而，公务员却把将军的「撇嘴」看成很生气，接下来每天都去将军那里道歉，将军烦不胜烦，更加不想理他。结果小公务员就一直提心吊胆，到最后竟然吓死了。\n这就是「说者无心，听者有意」的典型。小学语文课不好好做阅读理解，跑到职场上对老板表情做那么多阅读理解干嘛？\n上司很糟糕不跟你沟通，你不也很糟糕地没去主动找上司沟通吗？ 这世界上沟通是相互的，依赖别人终究被动，依赖自己才是王道\n糟糕上司永远没有最后，跳槽也解决不了 不知道大家发现了没有，身边的朋友，只要在一个公司遇到委屈，很可能就会跳槽跑路。\n他们常常寄望的是，下一个公司遇到一个圣人一样的妈妈老板，天天怕你冷了怕你饿了，更怕你的玻璃心掉一地碎了，像养蛙游戏的老母亲一样日日念叨着你快回来。\n实际上，很可能下一个老板更奇葩，更凶猛，KPI 更难完成。大概这世界上遇到好老板的几率，和媳妇遇到好婆婆的几率差不多低。\n大家熟知的偶像刘德华，在成为天王巨星之前也跟不少导演合作过了。\n80 年代和杜琪峰合作拍《鹿鼎记》，由始到终都被杜老板骂不如男一号梁朝伟。\n90 年代写了第一首歌《情是那么笨》，被知名作词人黄霑毫不留情地指责：「没见过写情这么笨的作词人。」\n当他进军歌坛的时候，还是被歌坛的老大哥谭咏麟一脸不客气地说他不会唱歌，劝他打消进军歌坛的念头。\n你看，哪怕是个有点名声的小鲜肉换各种角色都可能被前辈痛骂，何况你这个籍籍无名的小土豆，怎么可能通过跳槽就彻底换了一个毫不糟心的环境呢？\n糟糕上司哪里都有，如果你能死不要脸地撑到最后，妥妥用能力告诉他「老大，你看走眼了」，那才叫做真的威风\n职场说到底也是成年人的生意场，每个人都是自己的老板。 你的老板，你的同事，你的合作伙伴，都是你的供应商。\n他们身上的精华可以吸取，糟点可以避开，那你的装备才会越来越多，在升职加薪的路上一路打怪冲关。\n这年头流行让别人给自己「赋能」，现代管理学之父彼得·德鲁克比我们早数十年就看到上司的这个「功能」。他说：\n Making the strength of the boss productive is a key to the subordinate’s own effectiveness.","tags":["CAREER","MANAGE"],"title":"最重要的人脉","type":"post"},{"authors":null,"categories":[],"content":"  Modules  A module is a collection of related Go packages that are versioned together as a single unit.\nModules record precise dependency requirements and create reproducible builds.\n go.mod  A module is defined by a tree of Go source files with a go.mod file in the tree\u0026rsquo;s root directory. Module source code may be located outside of GOPATH. There are four directives: module, require, replace, exclude.\nHow to Use Modules  How to Install and Activate Module Support  Install the latest Go 1.11 release.   Once installed, you can then activate module support in one of two ways:\nInvoke the go command in a directory outside of the $GOPATH/src tree, with a valid go.mod file in the current directory or any parent of it and the environment variable GO111MODULE unset (or explicitly set to auto). Invoke the go command with GO111MODULE=on environment variable set.  https://semver.org/\n","date":1490166560,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490166560,"objectID":"0780d10226636c9b3c4e5f80730c3cb2","permalink":"https://wubigo.com/post/lang-go-module/","publishdate":"2017-03-22T15:09:20+08:00","relpermalink":"/post/lang-go-module/","section":"post","summary":"Modules  A module is a collection of related Go packages that are versioned together as a single unit.\nModules record precise dependency requirements and create reproducible builds.\n go.mod  A module is defined by a tree of Go source files with a go.mod file in the tree\u0026rsquo;s root directory. Module source code may be located outside of GOPATH. There are four directives: module, require, replace, exclude.","tags":["LANG","GO"],"title":"Go Module","type":"post"},{"authors":null,"categories":[],"content":" Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or ``` cat tiller-clusterrolebinding.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.K8S.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects:\n kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \u0026ldquo;\u0026rdquo;\n  docker pull registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\nkubectl create -f tiller-clusterrolebinding.yaml\nUpdate the existing tiller-deploy deployment with the Service Account helm init \u0026ndash;service-account tiller \u0026ndash;upgrade ```\n","date":1489828359,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489828359,"objectID":"16ad83a5ce3bc4eddd51e55285a3b095","permalink":"https://wubigo.com/post/k8s-helm-setup/","publishdate":"2017-03-18T17:12:39+08:00","relpermalink":"/post/k8s-helm-setup/","section":"post","summary":"Enable Helm in cluster  Create a Service Account tiller for the Tiller server (in the kube-system namespace). Service Accounts are meant for intra-cluster processes running in Pods.\n Bind the cluster-admin ClusterRole to this Service Account. ClusterRoleBindings to be applicable in all namespaces. Tiller to manage resources in all namespaces.\n Update the existing Tiller deployment (tiller-deploy) to associate its pod with the Service Account tiller.\nkubectl create serviceaccount tiller --namespace kube-system kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;template\u0026quot;:{\u0026quot;spec\u0026quot;:{\u0026quot;serviceAccount\u0026quot;:\u0026quot;tiller\u0026quot;}}}}'  or ``` cat tiller-clusterrolebinding.","tags":["K8S"],"title":"K8s Helm Setup","type":"post"},{"authors":null,"categories":[],"content":" Configuring Nodes to Authenticate to a Private Registry  Note: Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers (credHelpers or credsStore) are not supported.\n Docker stores keys for private registries in the $HOME/.dockercfg or $HOME/.docker/config.json file. If there are files in the search paths list below, kubelet uses it as the credential provider when pulling images.\n {\u0026ndash;root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.json ${HOME}/.docker/config.json /.docker/config.json {\u0026ndash;root-dir:-/var/lib/kubelet}/.dockercfg {cwd of kubelet}/.dockercfg ${HOME}/.dockercfg /.dockercfg  ~/.docker/config.json\n\u0026quot;auths\u0026quot;: { \u0026quot;registry.cn-hangzhou.aliyuncs.com\u0026quot;: { \u0026quot;auth\u0026quot;: \u0026quot;d3ViaWdvOjEyMzEyMwo=\u0026quot; } }, \u0026quot;HttpHeaders\u0026quot;: { \u0026quot;User-Agent\u0026quot;: \u0026quot;Docker-Client/17.03.3-ce (linux)\u0026quot; }   convert the base64-encoded auth data to a readable format  echo \u0026quot;d3ViaWdvOjEyMzEyMwo=\u0026quot; | base64 --decode  nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}') for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done  https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nhttps://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n","date":1489704286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489704286,"objectID":"620e8ba367120ada5a564b558a531ccb","permalink":"https://wubigo.com/post/k8s-private-registry/","publishdate":"2017-03-17T06:44:46+08:00","relpermalink":"/post/k8s-private-registry/","section":"post","summary":"Configuring Nodes to Authenticate to a Private Registry  Note: Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers (credHelpers or credsStore) are not supported.\n Docker stores keys for private registries in the $HOME/.dockercfg or $HOME/.docker/config.json file. If there are files in the search paths list below, kubelet uses it as the credential provider when pulling images.\n {\u0026ndash;root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.","tags":["K8S"],"title":"K8s Private Registry","type":"post"},{"authors":null,"categories":[],"content":" ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  Enables forwarding of the authentication agent connection  client config .ssh/config  ForwardAgent yes   Enable ssh-agent on main device  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null #ps ${SSH_AGENT_PID} doesn't work under cywgin ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ \u0026gt; /dev/null || { start_agent; } else start_agent; fi  http://mah.everybody.org/docs/ssh\n","date":1489656084,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489656084,"objectID":"785f669f0bbe895ddc21b4d02aad8ae6","permalink":"https://wubigo.com/post/linux-ssh/","publishdate":"2017-03-16T17:21:24+08:00","relpermalink":"/post/linux-ssh/","section":"post","summary":"ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  Enables forwarding of the authentication agent connection  client config .ssh/config  ForwardAgent yes   Enable ssh-agent on main device  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then .","tags":["LINUX","SSH"],"title":"Linux SSH","type":"post"},{"authors":null,"categories":[],"content":" setup external ETCD  install docker, kubelet, and kubeadm Configure the kubelet to be a service manager for etcd Create configuration files for kubeadm  /tmp/${HOST0}/kubeadmcfg.yaml\napiVersion: \u0026quot;kubeadm.k8s.io/v1beta1\u0026quot; kind: ClusterConfiguration etcd: local: serverCertSANs: - \u0026quot;192.168.1.10\u0026quot; peerCertSANs: - \u0026quot;192.168.1.10\u0026quot; extraArgs: initial-cluster: infra0=https://192.168.1.10:2380 initial-cluster-state: new name: infra0 listen-peer-urls: https://192.168.1.10:2380 listen-client-urls: https://192.168.1.10:2379 advertise-client-urls: https://192.168.1.10:2379 initial-advertise-peer-urls: https://192.168.1.10:2380   Generate the certificate authority\nsudo kubeadm init phase certs etcd-ca export HOST0=\u0026quot;192.168.1.10\u0026quot; sudo kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml sudo kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml  Create the static pod manifests\n  kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml  /etc/kubernetes/manifests\napiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.1.10:2379 - --initial-advertise-peer-urls=https://192.168.1.10:2380 - --initial-cluster=infra0=https://192.168.1.10:2380 - --initial-cluster-state=new - --listen-client-urls=https://192.168.1.10:2379 - --listen-peer-urls=https://192.168.1.10:2380 - --name=infra0 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --key-file=/etc/kubernetes/pki/etcd/server.key - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: k8s.gcr.io/etcd:3.2.24 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[192.168.1.10]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo failureThreshold: 8 initialDelaySeconds: 15 timeoutSeconds: 15 name: etcd resources: {} volumeMounts: - mountPath: /var/lib/etcd name: etcd-data - mountPath: /etc/kubernetes/pki/etcd name: etcd-certs hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/pki/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd type: DirectoryOrCreate name: etcd-data status: {}   ensure the etcd pod is running  journalctl -xeu kubelet remote_image.go:112] PullImage \u0026quot;k8s.gcr.io/etcd:3.2.24\u0026quot; from image service failed: rpc error: code = Unknown desc = Error response fro 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537292 4221 kuberuntime_image.go:51] Pull image \u0026quot;k8s.gcr.io/etcd:3.2.24\u0026quot; failed: rpc error: code = Unknown desc = Error response from daemon: Get 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537393 4221 kuberuntime_manager.go:749] container start failed: ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get htt 3月 16 14:41:53 bigo-vm2 kubelet[4221]: E0316 14:41:53.537469 4221 pod_workers.go:190] Error syncing pod 30ecbbae123bb7b8baaa2f08cb762164 (\u0026quot;etcd-bigo-vm2_kube-system(30ecbbae123bb7b8baaa2f08cb762164)\u0026quot;)  docker pull mirrorgooglecontainers/etcd:3.2.24 docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24   Check the cluster health  docker run --rm -it \\ --net host \\ -v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:3.2.24 etcdctl \\ --cert-file /etc/kubernetes/pki/etcd/peer.crt \\ --key-file /etc/kubernetes/pki/etcd/peer.key \\ --ca-file /etc/kubernetes/pki/etcd/ca.crt \\ --endpoints https://${HOST0}:2379 cluster-health  Set up the first control plane node  Copy the following files from any node from the etcd cluster  export CONTROL_PLANE=\u0026quot;192.168.1.9\u0026quot; scp /etc/kubernetes/pki/etcd/ca.crt \u0026quot;${CONTROL_PLANE}\u0026quot;: scp /etc/kubernetes/pki/apiserver-etcd-client.crt \u0026quot;${CONTROL_PLANE}\u0026quot;: scp /etc/kubernetes/pki/apiserver-etcd-client.key \u0026quot;${CONTROL_PLANE}\u0026quot;:  kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable apiServer: certSANs: - \u0026quot;192.168.1.9\u0026quot; controlPlaneEndpoint: \u0026quot;192.168.1.9:6443\u0026quot; etcd: external: endpoints: - https://192.168.1.10:2379 caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key networking: podSubnet: \u0026quot;10.2.0.0/16\u0026quot;  kubeadm init --pod-network-cidr 10.2.0.0/16 --config kubeadm-config.yaml -v 4 kubeadm init --config kubeadm-config.yaml -v 4  setup CNI ssh $ETCD curl https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/calico.yaml\u0026gt; calico.yaml # calico etcd setup sed -i -e \u0026quot;s/\\(^etcd_endpoints: \\\u0026quot;http.*$\\)/etcd_endpoints: \\\u0026quot;https:\\/\\/$VM:2379\\\u0026quot;/g\u0026quot; calico.yaml # etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; sed -i -e 's/etcd_ca: \\\u0026quot;\\\u0026quot; \\# \\\u0026quot;\\/calico-secrets/etcd-ca\\\u0026quot;/etcd_ca: \\\u0026quot;\\/calico-secrets\\/etcd-ca\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_cert: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/etcd_cert: \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_key: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/etcd_key: \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/g' calico.yaml CA=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) CERT=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0) KEY=$(sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0) sed -i -e \u0026quot;s/# etcd-ca: null/etcd-ca: $CA/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-cert: null/etcd-cert: $CERT/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-key: null/etcd-key: $KEY/g\u0026quot; calico.yaml  join a node https://blog.scottlowe.org/2018/08/21/bootstrapping-etcd-cluster-with-tls-using-kubeadm/ https://github.com/kelseyhightower/standalone-kubelet-tutorial\n","date":1489641812,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489641812,"objectID":"68d7fe0fb30aa6cb3cc71de26d0b5669","permalink":"https://wubigo.com/post/k8s-ha-setup-with-kubeadm/","publishdate":"2017-03-16T13:23:32+08:00","relpermalink":"/post/k8s-ha-setup-with-kubeadm/","section":"post","summary":"setup external ETCD  install docker, kubelet, and kubeadm Configure the kubelet to be a service manager for etcd Create configuration files for kubeadm  /tmp/${HOST0}/kubeadmcfg.yaml\napiVersion: \u0026quot;kubeadm.k8s.io/v1beta1\u0026quot; kind: ClusterConfiguration etcd: local: serverCertSANs: - \u0026quot;192.168.1.10\u0026quot; peerCertSANs: - \u0026quot;192.168.1.10\u0026quot; extraArgs: initial-cluster: infra0=https://192.168.1.10:2380 initial-cluster-state: new name: infra0 listen-peer-urls: https://192.168.1.10:2380 listen-client-urls: https://192.168.1.10:2379 advertise-client-urls: https://192.168.1.10:2379 initial-advertise-peer-urls: https://192.168.1.10:2380   Generate the certificate authority\nsudo kubeadm init phase certs etcd-ca export HOST0=\u0026quot;192.168.1.10\u0026quot; sudo kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.","tags":["K8S"],"title":"K8s HA Setup With Kubeadm","type":"post"},{"authors":null,"categories":[],"content":"tl;dr\nuget https://osdn.net/projects/systemrescuecd/storage/releases/6.0.2/systemrescuecd-6.0.2.iso sudo mount -t tmpfs tmpfs /takeover/ sudo mount -o loop,ro -t iso9660 ~/systemrescuecd-6.0.2.iso /mnt/cd cp -rf /mnt/cd/* /takeover/ curl -L https://www.busybox.net/downloads/binaries/1.26.2-defconfig-multiarch/busybox-x86_64 \u0026gt; busybox chmod u+x /takeover/busybox git clone https://github.com/marcan/takeover.sh.git gcc takeover.sh/fakeinit.c -o ./fakeinit  ","date":1489242916,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489242916,"objectID":"aa638212ded38327c0c22535695dd9d8","permalink":"https://wubigo.com/post/reinstall-a-running-linux-system-via-ssh/","publishdate":"2017-03-11T22:35:16+08:00","relpermalink":"/post/reinstall-a-running-linux-system-via-ssh/","section":"post","summary":"tl;dr\nuget https://osdn.net/projects/systemrescuecd/storage/releases/6.0.2/systemrescuecd-6.0.2.iso sudo mount -t tmpfs tmpfs /takeover/ sudo mount -o loop,ro -t iso9660 ~/systemrescuecd-6.0.2.iso /mnt/cd cp -rf /mnt/cd/* /takeover/ curl -L https://www.busybox.net/downloads/binaries/1.26.2-defconfig-multiarch/busybox-x86_64 \u0026gt; busybox chmod u+x /takeover/busybox git clone https://github.com/marcan/takeover.sh.git gcc takeover.sh/fakeinit.c -o ./fakeinit  ","tags":["LINUX"],"title":"通过SSH远程修复linux","type":"post"},{"authors":null,"categories":[],"content":" 准备 docker pull istio/proxyv2:1.0.6 docker tag istio/proxyv2:1.0.6 gcr.io/istio-release/proxyv2:release-1.0-latest-daily docker push registry.cn-beijing.aliyuncs.com/co1/istio_proxyv2:1.0.6 docker pull istio/pilot:1.0.6 docker tag istio/pilot:1.0.6 gcr.io/istio-release/pilot:release-1.0-latest-daily docker pull istio/mixer:1.0.6 docker tag istio/mixer:1.0.6 gcr.io/istio-release/mixer:release-1.0-latest-daily docker pull istio/galley:1.0.6 docker tag istio/galley:1.0.6 gcr.io/istio-release/galley:release-1.0-latest-daily docker pull istio/citadel:1.0.6 docker tag istio/citadel:1.0.6 gcr.io/istio-release/citadel:release-1.0-latest-daily docker pull istio/sidecar_injector:1.0.6 docker tag istio/sidecar_injector:1.0.6 gcr.io/istio-release/sidecar_injector:release-1.0-latest-daily git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6  安装 Istio by default uses LoadBalancer service object types. Some platforms do not support LoadBalancer service objects. For platforms lacking LoadBalancer support, install Istio with NodePort support instead with the flags \u0026ndash;set gateways.istio-ingressgateway.type=NodePort \u0026ndash;set gateways.istio-egressgateway.type=NodePort appended to the end of the Helm operation.\nhelm install install/kubernetes/helm/istio --name istio --namespace istio-system --set gateways.istio-ingressgateway.type=NodePort --set gateways.istio-egressgateway.type=NodePort  精简安装 helm install --debug install/kubernetes/helm/istio --name istio --namespace istio-system --set security.enabled=false --set ingress.enabled=false --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=false --set galley.enabled=false --set mixer.enabled=false --set prometheus.enabled=false --set global.proxy.envoyStatsd.enabled=false --set pilot.sidecar=true --set sidecarInjectorWebhook.enabled=false  kubectl label namespace default istio-injection=enabled kubectl describe ns default -n istio-system  RESOURCES: ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE istio-pilot 1 1 1 0 3s ==\u0026gt; v1alpha3/Gateway NAME AGE istio-autogenerated-k8s-ingress 3s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE istio-pilot-754ccc994f-zzkj9 0/1 Pending 0 2s ==\u0026gt; v1/ConfigMap NAME DATA AGE istio 1 5s istio-sidecar-injector 1 4s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE istio-pilot-service-account 1 4s ==\u0026gt; v1beta1/ClusterRole NAME AGE istio-pilot-istio-system 4s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-pilot ClusterIP 10.96.216.216 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,8080/TCP,9093/TCP 4s ==\u0026gt; v2beta1/HorizontalPodAutoscaler NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE istio-pilot Deployment/istio-pilot \u0026lt;unknown\u0026gt;/80% 1 5 0 2s  Ensure the istio-pilot-* Kubernetes pod is deployed and its container is up and running:\nkubectl get pods -n istio-system  MountVolume.SetUp failed for volume \u0026quot;certs\u0026quot; : secret \u0026quot;istio.istio-sidecar-injector-service-account\u0026quot; not found  the missing secret is created by the citadel pod which isn\u0026rsquo;t running due to the the \u0026ndash;set security.enabled=false flag, setting that to true starts citadel and the secret is created and then pilot will start.\n删除 helm del --purge istio kubectl -n istio-system delete job --all kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system kubectl get customresourcedefinitions.apiextensions.k8s.io |grep istio | xargs kubectl delete customresourcedefinitions.apiextensions.k8s.io  运行配置 kubectl get cm -n istio-system istio -o yaml \u0026gt; istio.config awk '{gsub(/\\\\n/,\u0026quot;\\n\u0026quot;)}1' istio.config  or\nkubectl exec -it istio-pilot -c discovery -n istio-system -- bash #cat /etc/istio/config/mesh | grep discoveryAddress kubectl get svc/istio-pilot -n istio-system -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \u0026quot;2018-03-29T11:04:04Z\u0026quot; labels: app: istio-pilot chart: pilot-1.0.6 heritage: Tiller release: istio name: istio-pilot namespace: istio-system resourceVersion: \u0026quot;467151\u0026quot; selfLink: /api/v1/namespaces/istio-system/services/istio-pilot uid: 5de2a2d8-5212-11e9-b518-08002775f493 spec: clusterIP: 10.108.66.176 ports: - name: grpc-xds port: 15010 protocol: TCP targetPort: 15010 - name: https-xds port: 15011 protocol: TCP targetPort: 15011 - name: http-legacy-discovery port: 8080 protocol: TCP targetPort: 8080 - name: http-monitoring port: 9093 protocol: TCP targetPort: 9093 selector: istio: pilot sessionAffinity: None type: ClusterIP status: loadBalancer: {} kubectl port-forward svc/istio-pilot -n istio-system 15010:15010  ","date":1488616718,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488616718,"objectID":"ebf88da19207ac64b970ca310f802061","permalink":"https://wubigo.com/post/k8s-mesh-istio/","publishdate":"2017-03-04T16:38:38+08:00","relpermalink":"/post/k8s-mesh-istio/","section":"post","summary":"准备 docker pull istio/proxyv2:1.0.6 docker tag istio/proxyv2:1.0.6 gcr.io/istio-release/proxyv2:release-1.0-latest-daily docker push registry.cn-beijing.aliyuncs.com/co1/istio_proxyv2:1.0.6 docker pull istio/pilot:1.0.6 docker tag istio/pilot:1.0.6 gcr.io/istio-release/pilot:release-1.0-latest-daily docker pull istio/mixer:1.0.6 docker tag istio/mixer:1.0.6 gcr.io/istio-release/mixer:release-1.0-latest-daily docker pull istio/galley:1.0.6 docker tag istio/galley:1.0.6 gcr.io/istio-release/galley:release-1.0-latest-daily docker pull istio/citadel:1.0.6 docker tag istio/citadel:1.0.6 gcr.io/istio-release/citadel:release-1.0-latest-daily docker pull istio/sidecar_injector:1.0.6 docker tag istio/sidecar_injector:1.0.6 gcr.io/istio-release/sidecar_injector:release-1.0-latest-daily git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6  安装 Istio by default uses LoadBalancer service object types. Some platforms do not support LoadBalancer service objects.","tags":["K8S"],"title":"K8S微服务治理","type":"post"},{"authors":null,"categories":[],"content":" GITHUB两种主要的pull request的开发模式\n分叉拉取模式 任何开发人员可以在项目源仓库(upstream)分叉，然后仓库该分叉(origin)到本地文件系统进行开发 测试，测试完毕提交到分叉origin，并发送pull request到源仓库upstream, 源仓库维护人员评审 更改，并最终决定是否合并该更改到源仓库\n在发送pull request之前，好几个开发人员共同为一个特性协作开发， 互相从对方的仓库拉取代码。 这时，从对方的仓库拉取代码简化重新定义一个remote，该remote把本地的分叉指向对方仓库地址。\n https://github.com/wubigo/wubigo.github.io 单击Fork按钮(右上角)\n GITHUB把该仓库代码复制到自己的github账号，建立分叉仓库\n 打开git命令行客户端，把分叉仓库克隆到本地环境  git clone https://github.com/$USER_NAME/wubigo.github.io.git cd wubigo.github.io git remote add upstream git@github.com:wubigo/wubigo.github.io.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v origin https://github.com/Fuang/wubigo.github.io.git (fetch) origin https://github.com/Fuang/wubigo.github.io.git (push) upstream git@github.com:wubigo/wubigo.github.io.git (fetch) upstream git@github.com:wubigo/wubigo.github.io.git (push)   同步本地代码到upstream  git fetch upstream git checkout master git rebase upstream/master git push   查看各个分支的最新提交ID  git branch -av * master a3b4a2b Update 2016-02-03-k8s local development setup.md remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/jekyll 439d951 Merge branch 'master' of github.com:wubigo/igo.github.io remotes/origin/master a3b4a2b Update 2016-02-03-k8s local development setup.md remotes/upstream/jekyll 439d951 Merge branch 'master' of github.com:wubigo/igo.github.io remotes/upstream/master a38ea93 deploy:github fork\u0026amp;pull model   从分叉创建pull request  原则上你可以在你分叉的任何分支或提交ID上向upstream仓库发出pull request， 但实际开发中，建议自建主题分支。\ngit branch add-project git add . git commit -m \u0026quot;add project\u0026quot; git push   进入github页面  点击add-project分支后面的compare \u0026amp; pull request按钮\n确认base repo是upstream， head repo是分叉\ncompare分支选择自建的add-project\n","date":1488585981,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488585981,"objectID":"64c4e271393e7ccf69cf6202c0f2d498","permalink":"https://wubigo.com/post/git-forkpull-model/","publishdate":"2017-03-04T08:06:21+08:00","relpermalink":"/post/git-forkpull-model/","section":"post","summary":"GITHUB两种主要的pull request的开发模式\n分叉拉取模式 任何开发人员可以在项目源仓库(upstream)分叉，然后仓库该分叉(origin)到本地文件系统进行开发 测试，测试完毕提交到分叉origin，并发送pull request到源仓库upstream, 源仓库维护人员评审 更改，并最终决定是否合并该更改到源仓库\n在发送pull request之前，好几个开发人员共同为一个特性协作开发， 互相从对方的仓库拉取代码。 这时，从对方的仓库拉取代码简化重新定义一个remote，该remote把本地的分叉指向对方仓库地址。\n https://github.com/wubigo/wubigo.github.io 单击Fork按钮(右上角)\n GITHUB把该仓库代码复制到自己的github账号，建立分叉仓库\n 打开git命令行客户端，把分叉仓库克隆到本地环境  git clone https://github.com/$USER_NAME/wubigo.github.io.git cd wubigo.github.io git remote add upstream git@github.com:wubigo/wubigo.github.io.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v origin https://github.com/Fuang/wubigo.github.io.git (fetch) origin https://github.com/Fuang/wubigo.github.io.git (push) upstream git@github.com:wubigo/wubigo.github.io.git (fetch) upstream git@github.com:wubigo/wubigo.github.io.git (push)   同步本地代码到upstream  git fetch upstream git checkout master git rebase upstream/master git push   查看各个分支的最新提交ID  git branch -av * master a3b4a2b Update 2016-02-03-k8s local development setup.","tags":["GIT","SCM"],"title":"分叉拉取模式(fork and pull model)","type":"post"},{"authors":null,"categories":[],"content":" 准备  创建角色和授权  kubectl create clusterrolebinding \u0026quot;cluster-admin-faas\u0026quot; \\ --clusterrole=cluster-admin \\ --user=\u0026quot;cluster-admin-faas\u0026quot;   分别为FAAS核心服务和函数创建名字空间  kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml   创建凭证  # generate a random password PASSWORD=$(head -c 12 /dev/urandom | shasum| cut -d' ' -f1) kubectl -n openfaas create secret generic basic-auth \\ --from-literal=basic-auth-user=admin \\ --from-literal=basic-auth-password=\u0026quot;$PASSWORD\u0026quot;   在本地helm仓库增加openfaas  helm repo add openfaas https://openfaas.github.io/faas-netes/ \u0026quot;openfaas\u0026quot; has been added to your repositories  开始安装 helm repo update \\ \u0026amp;\u0026amp; helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set basic_auth=true \\ --set functionNamespace=openfaas-fn  默认通过NodePorts方式访问openfaas控制台\nkubectl -n openfaas get svc -o wide |grep external gateway-external NodePort 10.105.222.28 \u0026lt;none\u0026gt; 8080:31112/TCP 91m app=gateway export OPENFAAS_URL=http://\u0026lt;pod-node-ip\u0026gt;:31112 curl OPENFAAS_URL USERNAME=$(kubectl get secrets basic-auth -n openfaas -o yaml | grep basic-auth-user) PASSWD=$(kubectl get secrets basic-auth -n openfaas -o yaml | grep basic-auth-password) PASSWD=$(echo '$PASSWD' | base64 --decode)  验证安装结果  通过浏览器访问openfaas  curl http://\u0026lt;pod-node-ip\u0026gt;:31112  输入上面的用户名和密码进入openfaas控制台\n 安装openfaas客户端  curl -sSL https://cli.openfaas.com | sh echo -n $PASSWORD | faas-cli login -g $OPENFAAS_URL -u admin --password-stdin  Removing the OpenFaaS All control plane components can be cleaned up with helm:\nhelm delete --purge openfaas kubectl delete namespace/openfaas kubectl delete namespace/openfaas-fn  ","date":1488501955,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488501955,"objectID":"12eeed65e38554ed2601db2d66499d71","permalink":"https://wubigo.com/post/serverless-computing-why-how/","publishdate":"2017-03-03T08:45:55+08:00","relpermalink":"/post/serverless-computing-why-how/","section":"post","summary":"准备  创建角色和授权  kubectl create clusterrolebinding \u0026quot;cluster-admin-faas\u0026quot; \\ --clusterrole=cluster-admin \\ --user=\u0026quot;cluster-admin-faas\u0026quot;   分别为FAAS核心服务和函数创建名字空间  kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml   创建凭证  # generate a random password PASSWORD=$(head -c 12 /dev/urandom | shasum| cut -d' ' -f1) kubectl -n openfaas create secret generic basic-auth \\ --from-literal=basic-auth-user=admin \\ --from-literal=basic-auth-password=\u0026quot;$PASSWORD\u0026quot;   在本地helm仓库增加openfaas  helm repo add openfaas https://openfaas.github.io/faas-netes/ \u0026quot;openfaas\u0026quot; has been added to your repositories  开始安装 helm repo update \\ \u0026amp;\u0026amp; helm upgrade openfaas --install openfaas/openfaas \\ --namespace openfaas \\ --set basic_auth=true \\ --set functionNamespace=openfaas-fn  默认通过NodePorts方式访问openfaas控制台","tags":["SERVERLESS","FAAS"],"title":"无服务器计算环境OPENFAAS搭建","type":"post"},{"authors":null,"categories":[],"content":"","date":1488447914,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488447914,"objectID":"64d52d939f2a482b199b94cb83c3ade7","permalink":"https://wubigo.com/post/docker-network-host/","publishdate":"2017-03-02T17:45:14+08:00","relpermalink":"/post/docker-network-host/","section":"post","summary":"","tags":[],"title":"Docker Network Host","type":"post"},{"authors":null,"categories":[],"content":"","date":1488446262,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488446262,"objectID":"c11870dfe8c3bf61b1af1185dea9ef23","permalink":"https://wubigo.com/post/docker_network_bridge/","publishdate":"2017-03-02T17:17:42+08:00","relpermalink":"/post/docker_network_bridge/","section":"post","summary":"","tags":[],"title":"Docker_network_bridge","type":"post"},{"authors":null,"categories":[],"content":" docker默认的网络 桥接网络\nDocker网络macvlan 网络macvlan\nDocker宿主网络 宿主网络\nDocker覆盖网络 宿主端口绑定 绑定方式： -p\n绑定形式\n ip:hostPort:containerPort| ip::containerPort | hostPort:containerPort | containerPort\n containerPort必须指定\ndocker run --rm --name web -p 80:80 -v /home/bigo/site:/usr/share/nginx/html:ro -d nginx:1.14-alpine  docker 会为端口绑定的容器自动启动docker-proxy进程\ndocker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.2 -container-port 80  ","date":1488424251,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488424251,"objectID":"7c5ad0c4ff20287ebd8ffa813ae04338","permalink":"https://wubigo.com/post/docker_network/","publishdate":"2017-03-02T11:10:51+08:00","relpermalink":"/post/docker_network/","section":"post","summary":" docker默认的网络 桥接网络\nDocker网络macvlan 网络macvlan\nDocker宿主网络 宿主网络\nDocker覆盖网络 宿主端口绑定 绑定方式： -p\n绑定形式\n ip:hostPort:containerPort| ip::containerPort | hostPort:containerPort | containerPort\n containerPort必须指定\ndocker run --rm --name web -p 80:80 -v /home/bigo/site:/usr/share/nginx/html:ro -d nginx:1.14-alpine  docker 会为端口绑定的容器自动启动docker-proxy进程\ndocker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 80 -container-ip 172.17.0.2 -container-port 80  ","tags":["DOCKER"],"title":"Docker网络","type":"post"},{"authors":null,"categories":null,"content":" Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use. 2. Scroll to the \u0026ldquo;Alternate Domain Names (CNAMEs)\u0026rdquo; field and enter your domain/subdomain you\u0026rsquo;re using 3. Scroll down to SSL Certificate and change the option to \u0026ldquo;Custom SSL Certificate\u0026rdquo;, then select the certificate you just created in the drop-down list. Scroll the rest of the way down and click \u0026ldquo;Create Distribution\u0026rdquo;.\nChange Distribution CNAME ","date":1488412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488412800,"objectID":"2366dc628bf609e0193702a5b12908f5","permalink":"https://wubigo.com/post/2017-03-02-s3withcustomdomainnameviahttps/","publishdate":"2017-03-02T00:00:00Z","relpermalink":"/post/2017-03-02-s3withcustomdomainnameviahttps/","section":"post","summary":"Getting an SSL Certificate and CloudFront Create CloudFront Distribution Navigate to CloudFront in your AWS console and click \u0026ldquo;Create Distribution\u0026rdquo;. Click \u0026ldquo;Get Started\u0026rdquo; under the Web option (not the RTMP). You\u0026rsquo;ll arrive on the Create Distribution page. Here you need to change three things: 1. Click inside the input field for \u0026ldquo;Origin Domain Name\u0026rdquo;. A list of your Amazon S3 buckets should pop up. Select the S3 bucket you want to use.","tags":null,"title":"S3 access with custom domain name via https","type":"post"},{"authors":null,"categories":[],"content":" 介绍 Macvlan支持从一个上层物理接口创建子接口，每个子接口有自己独立的MAC和IP地址。 应用程序，容器或虚机可以绑定到子接口，用子接口的IP和物理网络直接通信。\n 好处\n 现有的很多网络监控设备还不支持虚拟网络设备的监控，Macvlan支持 不需要新建iptable，nat，route单独管理容器网络  不足\n 交换机的每个端口上能连接的不同MAC有策略上限 网卡上过多的MAC会影响性能 Macvlan只支持LINUX   准备  需要4.0以上的内核  uname -r 4.15.0-45-generic   加载macvlan模块  sudo modprobe macvlan lsmod | grep macvlan ... macvlan 24576 0 ...   配置网卡为混杂模式     主机 IP     PC 192.168.1.5/24   VM1 192.168.1.10/24   Container1 192.168.1.128/25    MACVLAN四种工作模式  Macvlan VEPA Macvlan Bridge Macvlan Passthru  创建macvlan ip addr show enp0s3 enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:c0:91:4c brd ff:ff:ff:ff:ff:ff inet 192.168.1.10/24 ip route default via 192.168.1.1 dev enp0s3 onlink  docker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --ip-range=192.168.1.128/25 \\ --gateway=192.168.1.1 \\ -o parent=enp0s3 macLan  docker run --rm -itd --network macLan --name web nginx:1.14-alpine  macvlan限制  despite having the ability to communicate with other guests and other external hosts on the network, the guest cannot communicate with its own host.\nThis situation is actually not an error — it is the defined behavior of macvtap. Due to the way in which the host\u0026rsquo;s physical Ethernet is attached to the macvtap bridge, traffic into that bridge from the guests that is forwarded to the physical interface cannot be bounced back up to the host\u0026rsquo;s IP stack. Additionally, traffic from the host\u0026rsquo;s IP stack that is sent to the physical interface cannot be bounced back up to the macvtap bridge for forwarding to the guests.\n 变通方法：\n 预留ip \u0026ndash;aux-address=\u0026ldquo;macvlan_bridge=192.168.1.199\u0026rdquo;  docker network create -d macvlan \\ --subnet=192.168.1.0/24 \\ --ip-range=192.168.1.128/25 \\ --gateway=192.168.1.1 \\ -o parent=enp0s3 macLan \\ --aux-address=\u0026quot;macvlan_bridge=192.168.1.199\u0026quot;   创建一个新的macvlan  sudo ip link add macvlan_bridge link enp0s3 type macvlan mode bridge sudo ip addr add 192.168.1.199/32 dev macvlan_bridge sudo ip link set macvlan_bridge up   指示宿主通过macvlan_bridge访问部署在宿主上的容器网络  ip route add 192.168.1.128/27 dev macvlan_bridge  ","date":1488336572,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488336572,"objectID":"62f31bcf9c334866d594a59d86fd714a","permalink":"https://wubigo.com/post/docker_network_macvlan/","publishdate":"2017-03-01T10:49:32+08:00","relpermalink":"/post/docker_network_macvlan/","section":"post","summary":"介绍 Macvlan支持从一个上层物理接口创建子接口，每个子接口有自己独立的MAC和IP地址。 应用程序，容器或虚机可以绑定到子接口，用子接口的IP和物理网络直接通信。\n 好处\n 现有的很多网络监控设备还不支持虚拟网络设备的监控，Macvlan支持 不需要新建iptable，nat，route单独管理容器网络  不足\n 交换机的每个端口上能连接的不同MAC有策略上限 网卡上过多的MAC会影响性能 Macvlan只支持LINUX   准备  需要4.0以上的内核  uname -r 4.15.0-45-generic   加载macvlan模块  sudo modprobe macvlan lsmod | grep macvlan ... macvlan 24576 0 ...   配置网卡为混杂模式     主机 IP     PC 192.168.1.5/24   VM1 192.168.1.10/24   Container1 192.168.1.128/25    MACVLAN四种工作模式  Macvlan VEPA Macvlan Bridge Macvlan Passthru  创建macvlan ip addr show enp0s3 enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000 link/ether 08:00:27:c0:91:4c brd ff:ff:ff:ff:ff:ff inet 192.","tags":["CRI","DOCKER","NETWORK"],"title":"Docker网络macvlan","type":"post"},{"authors":null,"categories":null,"content":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"17dbab90328213a2dc7d8d2b1255dd35","permalink":"https://wubigo.com/post/2017-03-01-jekyll/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/post/2017-03-01-jekyll/","section":"post","summary":" How to allow unsafe ports in Chrome http://douglastarr.com/how-to-allow-unsafe-ports-in-chrome\nconfig $ cat _config.yml port: 6000  build for production $rm site.tar (remove before build otherwise tar-self is repackaged too) $JEKYLL_ENV=production bundle exec jekyll build $tar zcvf site.tar _site/*  replace gem source mirror gem sources --remove https://rubygems.org/ gem sources -a http://ruby.taobao.org/ gem sources -l http://ruby.taobao.org/  ","tags":null,"title":"jekyll notes","type":"post"},{"authors":null,"categories":[],"content":"  We can no longer assign a public IP address to your instance\nThe auto-assign public IP address feature for this instance is disabled because you specified multiple network interfaces. Public IPs can only be assigned to instances with one network interface. To re-enable the auto-assign public IP address feature, please specify only the eth0 network interface.\n ip MAC=`curl http://169.254.169.254/latest/meta-data/mac` curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/local-ipv4s  配置第二块网卡 ip a | grep ^[[:digit:]] tee -a /etc/network/interfaces.d/50-cloud-init.cfg \u0026lt;\u0026lt;-'EOF' auto eth1 iface eth1 inet dhcp EOF tee -a /etc/dhcp/dhclient-enter-hooks.d/restrict-default-gw \u0026lt;\u0026lt;-'EOF' case ${interface} in eth0) ;; *) unset new_routers ;; esac EOF systemctl restart networking  访问互联网 必须分配并关联EIP\n","date":1488270409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488270409,"objectID":"b29ae9ce5872c2d296311d8199959881","permalink":"https://wubigo.com/post/aws-ec2-secondary-network-interface/","publishdate":"2017-02-28T16:26:49+08:00","relpermalink":"/post/aws-ec2-secondary-network-interface/","section":"post","summary":"We can no longer assign a public IP address to your instance\nThe auto-assign public IP address feature for this instance is disabled because you specified multiple network interfaces. Public IPs can only be assigned to instances with one network interface. To re-enable the auto-assign public IP address feature, please specify only the eth0 network interface.\n ip MAC=`curl http://169.254.169.254/latest/meta-data/mac` curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/${MAC}/local-ipv4s  配置第二块网卡 ip a | grep ^[[:digit:]] tee -a /etc/network/interfaces.","tags":["AWS","SDN"],"title":"Aws EC2 多网卡配置","type":"post"},{"authors":null,"categories":[],"content":"","date":1488266530,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488266530,"objectID":"5a24b2063e7f22ac0652b42c9695b2bf","permalink":"https://wubigo.com/post/effective-coding-python/","publishdate":"2017-02-28T15:22:10+08:00","relpermalink":"/post/effective-coding-python/","section":"post","summary":"","tags":[],"title":"Effective Coding Python","type":"post"},{"authors":null,"categories":[],"content":" Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com. IN A www.wubigo.com. 1 IN CNAME wubigo.github.io. wubigo.github.io. 3462 IN A 185.199.111.153 wubigo.github.io. 3462 IN A 185.199.110.153 wubigo.github.io. 3462 IN A 185.199.109.153 wubigo.github.io. 3462 IN A 185.199.108.153  ","date":1487998304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487998304,"objectID":"d5e6285fd00911e2570af33a609fbf1e","permalink":"https://wubigo.com/post/dns-config-for-github-pages/","publishdate":"2017-02-25T12:51:44+08:00","relpermalink":"/post/dns-config-for-github-pages/","section":"post","summary":"Namecheap不支持根域(apex doamin/naked domain/bare domain)的ALIAS记录\n在代码仓库的根目录创建CNAME文件 CNME文件包括根域域名\ntee CNAME \u0026lt;\u0026lt; EOF wubigo.com EOF  在DNS控制台创建域名记录 一条指向根域的A记录，一条指向子域的CNAME记录 AN A Record for @(apex doamin) and a cname record(Alias) for github 不要在github页使用通配符 dns 记录! 否则没有指定的子域将有被别人使用的风险\n确认域名配置成功 dig www.wubigo.com +nostats +nocomments +nocmd ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.3-P4-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.wubigo.com +nostats +nocomments +nocmd ;; global options: +cmd ;www.wubigo.com. IN A www.wubigo.com. 1 IN CNAME wubigo.github.io. wubigo.github.io. 3462 IN A 185.199.111.153 wubigo.github.io. 3462 IN A 185.199.110.153 wubigo.github.io. 3462 IN A 185.","tags":["DNS","GITHUB"],"title":"DNS配置Github Pages","type":"post"},{"authors":null,"categories":["IT"],"content":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install  helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false  ","date":1487852920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487852920,"objectID":"c720be75ff6e1de6ced199d874f29384","permalink":"https://wubigo.com/post/k8s-monitor/","publishdate":"2017-02-23T20:28:40+08:00","relpermalink":"/post/k8s-monitor/","section":"post","summary":" setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc\u0026ndash;using-local-pv\n install  helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false  ","tags":["K8S","MONITOR"],"title":"K8S Monitor","type":"post"},{"authors":null,"categories":["IT"],"content":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","date":1487805691,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487805691,"objectID":"f7dd75f40aee72579bcfb6730c1fd8e9","permalink":"https://wubigo.com/post/ubungu-chinese/","publishdate":"2017-02-23T07:21:31+08:00","relpermalink":"/post/ubungu-chinese/","section":"post","summary":"/etc/default/locale\nupdate-locale LANG=zh_CN.UTF-8 # File generated by update-locale LANG=\u0026quot;en_US.UTF-8\u0026quot; LC_NUMERIC=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TIME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MONETARY=\u0026quot;zh_CN.UTF-8\u0026quot; LC_PAPER=\u0026quot;zh_CN.UTF-8\u0026quot; LC_NAME=\u0026quot;zh_CN.UTF-8\u0026quot; LC_ADDRESS=\u0026quot;zh_CN.UTF-8\u0026quot; LC_TELEPHONE=\u0026quot;zh_CN.UTF-8\u0026quot; LC_MEASUREMENT=\u0026quot;zh_CN.UTF-8\u0026quot; LC_IDENTIFICATION=\u0026quot;zh_CN.UTF-8\u0026quot; LANGUAGE=\u0026quot;zh_CN:en_US:en\u0026quot;  ","tags":["LINUX"],"title":"Ubungu Chinese locale","type":"post"},{"authors":null,"categories":[],"content":"  Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine. For example, the Docker container engine redirects those two streams to a logging driver\n The docker logs command is not available for drivers other than json-file and journald.\nlogging driver To configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.json file\nThe default logging driver is json-file. Thus, the default output for commands such as docker inspect  is JSON\n{ \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot;, \u0026quot;log-opts\u0026quot;: { \u0026quot;max-size\u0026quot;: \u0026quot;10m\u0026quot;, \u0026quot;max-file\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;labels\u0026quot;: \u0026quot;bigo.logg\u0026quot;, \u0026quot;env\u0026quot;: \u0026quot;os\u0026quot; } }   check the current driver  docker info --format '{{.LoggingDriver}}'  json-file the default location\n/var/lib/docker/containers/\u0026lt;INSTANCE_ID\u0026gt;/\u0026lt;INSTANCE_ID\u0026gt;-json.log\nor check log location via\ndocker inspect --format='{{.LogPath}}' $INSTANCE_ID  Supported logging drivers    Driver Description     none No logs are available for the container and docker logs does not return any output.   json-file The logs are formatted as JSON. The default logging driver for Docker.   local Writes logs messages to local filesystem in binary files using Protobuf.   syslog Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine.   journald Writes log messages to journald. The journald daemon must be running on the host machine.   gelf Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash.   fluentd Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine.   awslogs Writes log messages to Amazon CloudWatch Logs.    覆盖ENTRYPOINT  方法1    docker run -it --rm --entrypoint /bin/sh curl:v1   方法2 通过Dockerfile从镜像再建一个镜像，并重定义ENTRYPOINT  ","date":1487291065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487291065,"objectID":"0cb058a8ab8dee7bbcf361b377ca4d6a","permalink":"https://wubigo.com/post/docker-logging/","publishdate":"2017-02-17T08:24:25+08:00","relpermalink":"/post/docker-logging/","section":"post","summary":"Everything a containerized application writes to stdout and stderr is handled and redirected somewhere by a container engine. For example, the Docker container engine redirects those two streams to a logging driver\n The docker logs command is not available for drivers other than json-file and journald.\nlogging driver To configure the Docker daemon to default to a specific logging driver, set the value of log-driver to the name of the logging driver in the daemon.","tags":["DOCKER"],"title":"Docker日志","type":"post"},{"authors":null,"categories":[],"content":" Method\u0026rsquo;s receiver  go don\u0026rsquo;t user special name like( this or self ) for the receiver  func (p Point) MoveBy(factor float){ p.X += factor //-\u0026gt; this.X += factor }   Pointer receiver  project-layout Internal app/pkg Directory Clarification Using /internal/pkg is about consistency if you use the /pkg pattern. The public shared code goes in \u0026lsquo;/pkg\u0026rsquo; and the private shared code goes in /internal/pkg\nhttps://github.com/golang-standards/project-layout/issues/9\nvendor package go help gopath find . -name grpc | xargs rm -rf /go/pkg/mod/google.golang.org$ ls genproto@v0.0.0-20180817151627-c66870c02cf8 genproto@v0.0.0-20190127191240-7cdc0b958d75 grpc@v1.19.0 grpc@v1.19.1  Vendor directories do not affect the placement of new repositories being checked out for the first time by \u0026lsquo;go get\u0026rsquo;: those are always placed in the main GOPATH, never in a vendor subtree\nremove installed package go clean -i -n google.golang.org/grpc... cd /home/bigo/go/src/google.golang.org/grpc rm -f grpc.test grpc.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc.a cd /home/bigo/go/src/google.golang.org/grpc/balancer rm -f balancer.test balancer.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc/balancer.a cd /home/bigo/go/src/google.golang.org/grpc/balancer/base rm -f base.test base.test.exe rm -f /home/bigo/go/pkg/linux_amd64/google.golang.org/grpc/balancer/base.a cd /home/bigo/go/src/google.golang.org/grpc/balancer/grpclb rm -f grpclb.test grpclb.test.exe  identifier export In Go, a simple rule governs which identifiers are exported and which are not: exported identifiers start with an upper-case letter\nPackage-level names like the types and constants declared in one file of a package are visible to all the other files of the package, as if the source code were all in a single file\nCSP “Another lineage among Go’s ancestors, and one that makes Go distinctive among recent programming languages, is a sequence of little-known research languages developed at Bell Labs, all inspired by the concept of communicating sequential processes (CSP) from Tony Hoare’s seminal 1978 paper on the foundations of concurrency. In CSP, a program is a parallel composition of processes that have no shared state; the processes communicate and synchronize using channels.”\nhindsight “As a recent high-level language, Go has the benefit of hindsight, and the basics are done well: it has garbage collection, a package system, first-class functions, lexical scope, a system call interface, and immutable strings in which text is generally encoded in UTF-8. But it has comparatively few features and is unlikely to add more. For instance, it has no implicit numeric conversions, no constructors or destructors, no operator overloading, no default parameter values, no inheritance, no generics, no exceptions, no macros, no function annotations, and no thread-local storage”\ngofmt Go does not require semicolons at the ends of statements or declarations, except where two or more appear on the same line. In effect, newlines following certain tokens are converted into semicolons, so where newlines are placed matters to proper parsing of Go code. For instance, the opening brace { of the function must be on the same line as the end of the func declaration, not on a line by itself, and in the expression x + y, a newline is permitted after but not before the + operator.\npoint var  “The variable to which p points is written *p. The expression *p yields the value of that variable, an int, but since *p denotes a variable, it may also appear on the left-hand side of an assignment, in which case the assignment updates the variable.” “Expressions that denote variables are the only expressions to which the address-of operator \u0026amp; may be applied.” “Each time we take the address of a variable or copy a pointer, we create new aliases or ways to identify the same variable. For example, *p is an alias for v.”  Function Values have types declare a var with function value as type * func(int) int -\u0026gt; type\nvar f func(int) int  nil pointer dereference package main import \u0026quot;fmt\u0026quot; var f func(int) int func main() { //fmt.Println(f(2)) fmt.Println(f(2)) f = func(i int) int { if i == 0 { return 1 } return i * f(i-1) } fmt.Println(f(2)) } panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4850f0]goroutine 1 [running]:main.main() /home/bigo/go/src/github.com/gopl.io/ch5/t2/main.go:9 +0x30exit status 2Process exiting with code: 1  Capturing Iteration Variables “The problem of iteration variable capture is most often encountered when using the go statement or with defer since both may delay the execution of a function value until after the loop has finished. But the problem is not inherent to go or defer.”\nvar s1 [3]int = [3]int{1, 2, 3} var fs []func() func main() { for _, v := range s1 { //fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, v) //v is caputed and shared }) } for _, f := range fs { f() } fs = nil for _, v := range s1 { i := v fs = append(fs, func() { fmt.Printf(\u0026quot;%d\\n\u0026quot;, i) }) } for _, f := range fs { f() } }  The reason is a consequence of the scope rules for loop variables. In the program immediately above, the for loop introduces a new lexical block in which the variable dir is declared. All function values created by this loop “capture” and share the same variable—an addressable storage location, not its value at that particular moment. The value of dir is updated in successive iterations, so by the time the cleanup functions are called, the dir variable has been updated several times by the now-completed for loop.\nexpression in go/defer must be function call  express anonymous function as function call()  var s1 [3]string = [3]string{\u0026quot;go\u0026quot;, \u0026quot;func\u0026quot;, \u0026quot;value\u0026quot;} func f(s string) { fmt.Println(s) } func main() { fmt.Println(\u0026quot;s escapsed\u0026quot;) for _, s := range s1 { defer func() { fmt.Println(s) }() } fmt.Println(\u0026quot;work expected\u0026quot;) for _, s := range s1 { defer func(s string) { fmt.Println(s) }(s) } for _, s := range s1 { defer f(s) } for _, s := range s1 { go func(s string) { fmt.Println(\u0026quot;h %s\u0026quot;, s) }(s) } for _, s := range s1 { go f(s) } }  install go-1.10 ***vscode will report go-runtime read-only error which has been installed via snap\nsudo add-apt-repository ppa:gophers/archive sudo apt-get update sudo apt-get install golang-1.10-go export GOROOT=\u0026quot;/usr/lib/go-1.10\u0026quot; export GOPATH=$HOME/go export PATH=\u0026quot;$PATH:$GOPATH/bin:$GOROOT/bin/\u0026quot; go version  GOPATH directory cd $GOPATH ls src bin pkg     DIR      pkg 存放编译过的包   bin 存放可执行程序(package main)    import path两个功能 go help importpath   在本地文件系统$GOPATH/pkg/$GOOS_$GOARCH/查找  import \u0026quot;github.com/google/go-github/github\u0026quot; ... go/src/wubigo.com/cloud/github_client/main.go:7:2: cannot find package \u0026quot;github.com/google/go-github/github\u0026quot; in any of: /usr/lib/go-1.11/src/github.com/google/go-github/github (from $GOROOT) /home/bigo/go/src/github.com/google/go-github/github (from $GOPATH) Process exiting with code: 1 ...   如果没找到，需要go get通过安装  go get github.com/google/go-github/github   import path custom domain name   curl golang.org/x/tools/cmd/rename \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=utf-8\u0026quot;/\u0026gt; \u0026lt;meta name=\u0026quot;go-import\u0026quot; content=\u0026quot;golang.org/x/tools git https://go.googlesource.com/tools\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;go-source\u0026quot; content=\u0026quot;golang.org/x/tools https://github.com/golang/tools/ https://github.com/golang/tools/tree/master{/dir} https://github.com/golang/tools/blob/master{/dir}/{file}#L{line}\u0026quot;\u0026gt; \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; url=https://godoc.org/golang.org/x/tools/cmd/rename\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Nothing to see here; \u0026lt;a href=\u0026quot;https://godoc.org/golang.org/x/tools/cmd/rename\u0026quot;\u0026gt;move along\u0026lt;/a\u0026gt;. \u0026lt;/body\u0026gt;   Canonical import paths\n The syntax is simple: put an identifying comment on the package line\nhttps://github.com/golang/tools/blob/master/cmd/gorename/main.go\npackage main // import \u0026quot;golang.org/x/tools/cmd/gorename\u0026quot;  struct tags  List of well-known struct tags     Tag Documentation     xml https://godoc.org/encoding/xml   json https://godoc.org/encoding/json   asn1 https://godoc.org/encoding/asn1   reform https://godoc.org/gopkg.in/reform.v1   bigquery https://godoc.org/cloud.google.com/go/bigquery   datastore https://godoc.org/cloud.google.com/go/datastore   spanner https://godoc.org/cloud.google.com/go/spanner   bson https://godoc.org/labix.org/v2/mgo/bson   gorm https://godoc.org/github.com/jinzhu/gorm   yaml https://godoc.org/gopkg.in/yaml.v2    ","date":1486815889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486815889,"objectID":"0c7407e9740d27a2603d0206c57a357f","permalink":"https://wubigo.com/post/lang-go-notes/","publishdate":"2017-02-11T20:24:49+08:00","relpermalink":"/post/lang-go-notes/","section":"post","summary":"Method\u0026rsquo;s receiver  go don\u0026rsquo;t user special name like( this or self ) for the receiver  func (p Point) MoveBy(factor float){ p.X += factor //-\u0026gt; this.X += factor }   Pointer receiver  project-layout Internal app/pkg Directory Clarification Using /internal/pkg is about consistency if you use the /pkg pattern. The public shared code goes in \u0026lsquo;/pkg\u0026rsquo; and the private shared code goes in /internal/pkg\nhttps://github.com/golang-standards/project-layout/issues/9\nvendor package go help gopath find .","tags":["LANG","GO"],"title":"GO NOTES","type":"post"},{"authors":null,"categories":[],"content":" 准备  搭建测试环境\n可以参考从源代码构件K8S开发环境\n  ","date":1485396540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485396540,"objectID":"242bbaa1bf1a06a806159c8d68e30383","permalink":"https://wubigo.com/post/k8s_cni_l2_network_on_bare_metal/","publishdate":"2017-01-26T10:09:00+08:00","relpermalink":"/post/k8s_cni_l2_network_on_bare_metal/","section":"post","summary":" 准备  搭建测试环境\n可以参考从源代码构件K8S开发环境\n  ","tags":["K8S","CNI","NETWORK"],"title":"K8SCNI之L2 网络实现","type":"post"},{"authors":null,"categories":[],"content":" COOKIE \u0026amp; HTTP SESSION H5 addition that adds a key/value store to browsers and cookies\nstateful session Some examples of scaling stateful sessions:\nOnce you run multiple backend processes on a server: A Redis daemon (on that server) for session storage. Once you run on multiple servers: A dedicated server running Redis just for session storage. Once you run on multiple servers, in multiple clusters: Sticky sessions.  JWT session  Stateless JWT: A JWT token that contains the session data, encoded directly into the token. Stateful JWT: A JWT token that contains just a reference or ID for the session. The session data is stored server-side. Session token/cookie: A standard (optionally signed) session ID, like web frameworks have been using for a long time. The session data is stored server-side.  Stop using JWT for sessions http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/\n","date":1483584360,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483584360,"objectID":"a05f4c9e433afc07c2f3cb8ddd3f162c","permalink":"https://wubigo.com/post/http-session-management/","publishdate":"2017-01-05T10:46:00+08:00","relpermalink":"/post/http-session-management/","section":"post","summary":"COOKIE \u0026amp; HTTP SESSION H5 addition that adds a key/value store to browsers and cookies\nstateful session Some examples of scaling stateful sessions:\nOnce you run multiple backend processes on a server: A Redis daemon (on that server) for session storage. Once you run on multiple servers: A dedicated server running Redis just for session storage. Once you run on multiple servers, in multiple clusters: Sticky sessions.  JWT session  Stateless JWT: A JWT token that contains the session data, encoded directly into the token.","tags":["HTTP"],"title":"HTTP Session Management","type":"post"},{"authors":null,"categories":null,"content":" Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks. The first successful practical application of neural nets came in 1989 from Bell Labs, when Yann LeCun combined together the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of handwritten digits classification. The resulting network, dubbed \u0026ldquo;LeNet\u0026rdquo;, was used by the US Post Office in the 1990s to automate the reading of ZIP codes on mail envelopes\n逻辑回归(Logistic regression) Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes).\nIn logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (TRUE, success, pregnant, etc.) or 0 (FALSE, failure, non-pregnant, etc.).\nThe goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest\nhttps://www.medcalc.org/manual/logistic_regression.php\n神经网络 一个分类算法，其输出是样本属于某类别的概率值 P(y==k|x;Θ)\nactivation function is the sigmoid function self.activation_function = lambda x: scipy.special.expit(x) Instead of the usual def() definitions, we use the magic lambda to create a function there and then, quickly and easily. The function here takes x and returns scipy.special.expit(x) which is the sigmoid function. Functions created with lambda are nameless or anonymous, but here we’ve assigned it to the name self.activation_function(). All this means is that whenever someone needs to user the activation function, all they need to do is call self.activation_function()\n","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"4ba4deb87812a03bcc1368ba258e2527","permalink":"https://wubigo.com/post/2017-01-03-machinelearningbasic/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-01-03-machinelearningbasic/","section":"post","summary":"Early neural networks Although the core ideas of neural networks were investigated in toy forms as early as the 1950s, the approach took decades to really get started. For a long time, the missing piece was a lack of an efficient way to train large neural networks. This changed in the mid-1980s, as multiple people independently rediscovered the \u0026ldquo;backpropagation\u0026rdquo; algorithm, a way to train chains of parametric operations using gradient descent optimization (later in the book, we will go on to precisely define these concepts), and started applying it to neural networks.","tags":null,"title":"machine learning basic","type":"post"},{"authors":null,"categories":null,"content":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","date":1483401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483401600,"objectID":"79fe544dceb4c0e98497a333d4b12f8f","permalink":"https://wubigo.com/post/2017-02-03-numpynotes/","publishdate":"2017-01-03T00:00:00Z","relpermalink":"/post/2017-02-03-numpynotes/","section":"post","summary":" along an axis Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1)\n\u0026gt;\u0026gt;\u0026gt;print np.arange(12) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=1) \u0026gt;\u0026gt;\u0026gt;print np.arange(12).reshape(3, 4).mean(axis=0)  ","tags":null,"title":"numpy notes","type":"post"},{"authors":null,"categories":null,"content":" The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"9a5b10ed10e57e19a16308435c006b94","permalink":"https://wubigo.com/post/2017-01-01-hacknewsfavorites2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-hacknewsfavorites2017/","section":"post","summary":"The State Of Front-End Frameworks []()\nLet\u0026rsquo;s Encrypt now holds 35% of the market https://nettrack.info/ssl_certificate_issuers.html\nCode together in real time with Teletype for Atom https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html\nHow Firefox Got Fast Again https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/\nDo you need a VPN https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn","tags":null,"title":"Hacknews favorites 2017","type":"post"},{"authors":null,"categories":null,"content":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3abac9b183c99b7bd38bc7e32bcbd0b5","permalink":"https://wubigo.com/post/2017-01-01-discoursenotes/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post/2017-01-01-discoursenotes/","section":"post","summary":" discourse-setup discourse-setup script does, more-or-less, is just copy samples/standalone.yml to containers/app.yml and edit a bunch of stuff in response to the answers to a bunch of questions\n$cp samples/standalone.yml containers/app.yml  Discourse app.yml AWS setup example\nDiscourse app.yml AWS setup\nhow-to-specify-a-different-port-not-80-during-install\nfix these settings after bootstrapping, edit the /containers/app.yml then rebuild to take effect\n./launcher rebuild app  ","tags":null,"title":"discourse notes","type":"post"},{"authors":null,"categories":null,"content":" Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory\nmail: { from: 'from@email.com', transport: 'SMTP', options: { host: \u0026quot;your-amazon-host\u0026quot;, port: 465, service: \u0026quot;SES\u0026quot;, auth: { user: \u0026quot;your-amazon-user\u0026quot;, pass: \u0026quot;your-amazon-password\u0026quot; } } }  Amazon SES credential can be generated from amazon control panel. From address must be registered and verified as sender.\nfavicon.ico cp /home/ubuntu/favicon.ico /var/www/ghost/versions/1.18.2/core/server/public/  ","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"1e116d493d15f537536b7ff2446fe23c","permalink":"https://wubigo.com/post/2016-12-30-ghostnotes/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-ghostnotes/","section":"post","summary":"Adding swap memory If your system has less than 1GB memory, you may run into errors. To overcome this, configure a larger amount of swap memory:\ndd if=/dev/zero of=/var/swap bs=1k count=1024k mkswap /var/swap swapon /var/swap echo '/var/swap swap swap default 0 0' \u0026gt;\u0026gt; /etc/fstab  default-setting /var/www/ghost/versions/1.18.2/core/server/data/schema/default-settings.json  blog mail configuration using Amazon SES Ghost is a blogging platform written in nodejs. Edit the config.js file at the ghost root directory","tags":null,"title":"ghost notes","type":"post"},{"authors":null,"categories":null,"content":" good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3. Delegation skills: ability to translate ambiguous directives from above into actionable tasks for the team, ability to arrange tasks so that individuals have the creative freedom to figure out how to do it, predictability in decision-making, ability to shield team from external pressure, ability to give negative feedback immediately, ability to fire low performers before they fester into team morale and culture problems 4. Coaching skills: ability to give targeted advice, ability to match employee\u0026rsquo;s own aspirations and growth objectives to projects, enough technical expertise to point people in the right direction, ability to evaluate skills and hire for them\nDealing with ambiguity and developing resilience http://www.melanieallen.co.uk/articles/dealing-with-ambiguity/\nThe 2 Mental Shifts Highly Successful People Make https://medium.com/personal-growth/the-2-mental-shifts-highly-successful-people-make-7089450c2d7c\nwhy your programmers just want to code https://hackernoon.com/why-your-programmers-just-want-to-code-36da9973388e\nThree Powerful Conversations Managers Must Have To Develop Their People  Starting with kindergarten, tell me about your life\n Spot their lighthouse and bring it into focus “The idea is to try to get employees to start to talk to you about their dreams, or three to five of them if they don\u0026rsquo;t really want to commit to one idea. None of it should be time-bound — no 10-year plans. Ask what this person would be doing at the pinnacle of his or her career — when they’re feeling challenged, engaged and not wanting anything else.”\n Create a career action plan\n  leading snowflakes http://leadingsnowflakes.com/ Show How, Don\u0026rsquo;t Tell What - A Management Style\nProgressing from tech to leadership https://lcamtuf.blogspot.hk/2018/02/on-leadership.html\nShare your Manager README https://matthewnewkirk.com/2017/09/20/share-your-manager-readme/\nHow to Win Friends and Influence People https://www.amazon.com/How-Friends-Influence-People-Chinese\nBooks I read this year https://www.gatesnotes.com/About-Bill-Gates/Best-Books-2017\n","date":1483056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483056000,"objectID":"0f689655e2902582d054924032be3cbd","permalink":"https://wubigo.com/post/2016-12-30-softwareleadweekly/","publishdate":"2016-12-30T00:00:00Z","relpermalink":"/post/2016-12-30-softwareleadweekly/","section":"post","summary":"good practise There are several common things and These all take practice. 1. Organization skills/project management skills: ability to pull together internal/external resources, ability to create clear plans, ability to build relationships with other teams to make cross-team collaboration easier 2. Communication skills: ability to deescalate and mediate conflicts fairly, ability to give both positive and negative feedback regularly, ability to create psychological safety, ability to deliver bad news while maintaining psychological safety, ability to run efficient meetings, ability to make unpopular decisions while still commanding the team\u0026rsquo;s respect, ability to engage with different personality types and communication styles 3.","tags":null,"title":"software lead weekly","type":"post"},{"authors":null,"categories":[],"content":"RDMA (Remote Direct Memory Access), TOE (TCP Offload Engine), and OpenOnload. More recently, DPDK (Data Plane Development Kit) has been used in some applications to bypass the kernel, and then there are new emerging initiatives such as FD.io (Fast Data Input Output) based on VPP (Vector Packet Processing). More will likely emerge in the future.\nTechnologies like RDMA and TOE create a parallel stack in the kernel and solve the first problem (namely, the \u0026ldquo;kernel is too slow\u0026rdquo;) while OpenOnload, DPDK and FD.io (based on VPP) move networking into Linux user space to address both speed and technology plug-in requirements. When technologies are built in the Linux user space, the need for changes to the kernel is avoided, eliminating the extra effort required to convince the Linux kernel community about the usefulness of the bypass technologies and their adoption via upstreaming into the Linux kernel.\n","date":1482977759,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482977759,"objectID":"99566989824e91dfc183e1c6f9ea5012","permalink":"https://wubigo.com/post/kernel-bypass-networking/","publishdate":"2016-12-29T10:15:59+08:00","relpermalink":"/post/kernel-bypass-networking/","section":"post","summary":"RDMA (Remote Direct Memory Access), TOE (TCP Offload Engine), and OpenOnload. More recently, DPDK (Data Plane Development Kit) has been used in some applications to bypass the kernel, and then there are new emerging initiatives such as FD.io (Fast Data Input Output) based on VPP (Vector Packet Processing). More will likely emerge in the future.\nTechnologies like RDMA and TOE create a parallel stack in the kernel and solve the first problem (namely, the \u0026ldquo;kernel is too slow\u0026rdquo;) while OpenOnload, DPDK and FD.","tags":["SDN","NFV"],"title":"Kernel Bypass Networking","type":"post"},{"authors":null,"categories":[],"content":" QUERY aws dynamodb scan --table-name \u0026quot;orders\u0026quot;  ","date":1482217742,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482217742,"objectID":"5967be7e4296e3f9c0e9dc15a90bdedb","permalink":"https://wubigo.com/post/aws-dynamodb-notes/","publishdate":"2016-12-20T15:09:02+08:00","relpermalink":"/post/aws-dynamodb-notes/","section":"post","summary":" QUERY aws dynamodb scan --table-name \u0026quot;orders\u0026quot;  ","tags":["AWS","DB"],"title":"Aws Dynamodb Notes","type":"post"},{"authors":null,"categories":[],"content":" 本文所说的新手是指还在12个月免费期内的用户\n只要有一张信用卡，就可以注册一个AWS账号。\n对于AWS新注册账号，有12个月的免费使用额度\n具体额度如下：\n服务\n   服务 额度 当月使用统计     服务器 750小时 t2.micro    硬盘 30GB    硬盘快照 1GB    网盘 5G    数据库 25G    函数计算 1百万次调用     下图我这个月的使用额度\n计算和存储 从上面的图可以看出，AWS免费额度里面比较鸡肋的是\n网盘快照的额度太少，走常规的操作系统镜像备份是要\n花钱的，因为一个最小的ubuntu实例镜像就是8G，\n如何做到保存自己的最新工作成果，而又额外使用快照从而\n节省存储费用呢？\n解决办法如下：\n硬盘外挂  创建EC2\n 创建一块硬盘，小于20G即可，并把该硬盘外挂到EC2\n 在外挂的硬盘里面保存自己的操作数据\n 使用user_data初始化包括安装常用软件包，自动外挂硬盘等\n 使用完EC2销毁即可(卸载外挂硬盘千万不要销毁外挂的硬盘)\n  上面的操作可以使用基础设施配置工具(ansible, terraform, pupport等)\n进行自动化管理。\n下面以terraform为例介绍操作步骤：\n前提  安装AWS SDK\n 配置AWS访问权限\n  EC2  启动并外挂硬盘  使用容器镜像 AWS提供500M的镜像存储空间，可以把自己的数据保存到容器镜像\nEC2带宽费用 EIP双向收费，策略是使用S3+VPC中转\nIPv4: Data transferred “in” to and “out” from public or Elastic IPv4 address is charged at $0.01/GB in each direction. IPv6: Data transferred “in” to and “out” from an IPv6 address in a different VPC is charged at $0.01/GB in each direction.  搭建S3 web站点 web hosting\n","date":1482186688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482186688,"objectID":"8a13c51bd42e552b947be572782c4fbf","permalink":"https://wubigo.com/post/aws-new-guy-cheat-sheet/","publishdate":"2016-12-20T06:31:28+08:00","relpermalink":"/post/aws-new-guy-cheat-sheet/","section":"post","summary":"本文所说的新手是指还在12个月免费期内的用户\n只要有一张信用卡，就可以注册一个AWS账号。\n对于AWS新注册账号，有12个月的免费使用额度\n具体额度如下：\n服务\n   服务 额度 当月使用统计     服务器 750小时 t2.micro    硬盘 30GB    硬盘快照 1GB    网盘 5G    数据库 25G    函数计算 1百万次调用     下图我这个月的使用额度\n计算和存储 从上面的图可以看出，AWS免费额度里面比较鸡肋的是\n网盘快照的额度太少，走常规的操作系统镜像备份是要\n花钱的，因为一个最小的ubuntu实例镜像就是8G，\n如何做到保存自己的最新工作成果，而又额外使用快照从而\n节省存储费用呢？\n解决办法如下：\n硬盘外挂  创建EC2\n 创建一块硬盘，小于20G即可，并把该硬盘外挂到EC2\n 在外挂的硬盘里面保存自己的操作数据\n 使用user_data初始化包括安装常用软件包，自动外挂硬盘等\n 使用完EC2销毁即可(卸载外挂硬盘千万不要销毁外挂的硬盘)\n  上面的操作可以使用基础设施配置工具(ansible, terraform, pupport等)","tags":["AWS"],"title":"公有云羊毛党使用秘籍(新手篇)","type":"post"},{"authors":null,"categories":null,"content":" twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/\n","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"59adc335c5a4340a46dc2c9bf3b60801","permalink":"https://wubigo.com/post/2016-11-01-graphprocessing/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/post/2016-11-01-graphprocessing/","section":"post","summary":"twitter social network dataset http://law.di.unimi.it/datasets.php\nA comparison of state-of-the-art graph processing systems https://code.facebook.com/posts/319004238457019/a-comparison-of-state-of-the-art-graph-processing-systems/\nLinkedin graph database https://engineering.linkedin.com/teams/data/projects/search-and-discovery https://engineering.linkedin.com/teams/data\nWhy I left Apache Spark GraphX and returned to HBase for my graph database https://lbartkowski.wordpress.com/2015/04/21/why-i-left-apache-spark-graphx-and-returned-to-hbase-for-my-graph-database/","tags":null,"title":"Graph Processing","type":"post"},{"authors":null,"categories":[],"content":" push to a mirror repository push to github at same time when a commit is pushed to gitlab\nProtected Branches By default, protected branches are designed to:\n prevent their creation, if not already created, from everybody except Maintainers prevent pushes from everybody except Maintainers prevent anyone from force pushing to the branch prevent anyone from deleting the branch  Project members permissions NOTE:\nIn GitLab 11.0, the Master role was renamed to Maintainer The following table depicts the various user permission levels in a project.\nAction Guest Reporter Developer Maintainer Owner\nCreate new issue ✓ ✓ ✓ ✓ ✓\nCreate confidential issue ✓ ✓ ✓ ✓ ✓\nView confidential issues (✓) ✓ ✓ ✓ ✓\nLeave comments ✓ ✓ ✓ ✓ ✓\nSee related issues ✓ ✓ ✓ ✓ ✓\nSee a list of jobs ✓ ✓ ✓ ✓ ✓\nSee a job log ✓ ✓ ✓ ✓ ✓\nDownload and browse job artifacts ✓ ✓ ✓ ✓ ✓\nView wiki pages ✓ ✓ ✓ ✓ ✓\nPull project code\n✓ ✓ ✓ ✓\nDownload project\n✓ ✓ ✓ ✓\nAssign issues\n✓ ✓ ✓ ✓\nAssign merge requests\n✓ ✓ ✓\nLabel issues and merge requests\n✓ ✓ ✓ ✓\nCreate code snippets\n✓ ✓ ✓ ✓\nManage issue tracker\n✓ ✓ ✓ ✓\nManage labels\n✓ ✓ ✓ ✓\nSee a commit status\n✓ ✓ ✓ ✓\nSee a container registry\n✓ ✓ ✓ ✓\nSee environments\n✓ ✓ ✓ ✓\nSee a list of merge requests\n✓ ✓ ✓ ✓\nManage related issues [STARTER]\n✓ ✓ ✓ ✓\nLock issue discussions\n✓ ✓ ✓ ✓\nLock merge request discussions\n✓ ✓ ✓\nCreate new environments\n✓ ✓ ✓\nStop environments\n✓ ✓ ✓\nManage/Accept merge requests\n✓ ✓ ✓\nCreate new merge request\n✓ ✓ ✓\nCreate new branches\n✓ ✓ ✓\nPush to non-protected branches\n✓ ✓ ✓\nForce push to non-protected branches\n✓ ✓ ✓\nRemove non-protected branches\n✓ ✓ ✓\nAdd tags\n✓ ✓ ✓\nWrite a wiki\n✓ ✓ ✓\nCancel and retry jobs\n✓ ✓ ✓\nCreate or update commit status\n✓ ✓ ✓\nUpdate a container registry\n✓ ✓ ✓\nRemove a container registry image\n✓ ✓ ✓\nCreate/edit/delete project milestones\n✓ ✓ ✓\nUse environment terminals\n✓ ✓\nAdd new team members\n✓ ✓\nPush to protected branches\n✓ ✓\nEnable/disable branch protection\n✓ ✓\nTurn on/off protected branch push for devs\n✓ ✓\nEnable/disable tag protections\n✓ ✓\nRewrite/remove Git tags\n✓ ✓\nEdit project\n✓ ✓\nAdd deploy keys to project\n✓ ✓\nConfigure project hooks\n✓ ✓\nManage Runners\n✓ ✓\nManage job triggers\n✓ ✓\nManage variables\n✓ ✓\nManage GitLab Pages\n✓ ✓\nManage GitLab Pages domains and certificates\n✓ ✓\nRemove GitLab Pages\n✓\nManage clusters\n✓ ✓\nEdit comments (posted by any user)\n✓ ✓\nSwitch visibility level\n✓\nTransfer project to another namespace\n✓\nRemove project\n✓\nDelete issues\n✓\nRemove pages\n✓\nForce push to protected branches\nRemove protected branches\nView project Audit Events\n✓ ✓\nProject features permissions\n","date":1461834487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461834487,"objectID":"1e4e47a6c6acb1c121b28bd0a762ec85","permalink":"https://wubigo.com/post/gitlab-notes/","publishdate":"2016-04-28T17:08:07+08:00","relpermalink":"/post/gitlab-notes/","section":"post","summary":"push to a mirror repository push to github at same time when a commit is pushed to gitlab\nProtected Branches By default, protected branches are designed to:\n prevent their creation, if not already created, from everybody except Maintainers prevent pushes from everybody except Maintainers prevent anyone from force pushing to the branch prevent anyone from deleting the branch  Project members permissions NOTE:\nIn GitLab 11.0, the Master role was renamed to Maintainer The following table depicts the various user permission levels in a project.","tags":["SHELL","GIT"],"title":"Gitlab Notes","type":"post"},{"authors":null,"categories":[],"content":" LETTUCE VS JEDIS While Jedis is easy to use and supports a vast number of Redis features, it is not thread safe and needs connection pooling to work in a multi-threaded environment. Connection pooling comes at the cost of a physical connection per Jedis instance which increases the number of Redis connections.\nLettuce, on the other hand, is built on netty (https://netty.io/) and connection instances can be shared across multiple threads. So a multi-threaded application can use a single connection regardless the number of concurrent threads that interact with Lettuce.\nSYNC VS ASYNC One other reason we opted to go with Lettuce was that it facilitates asynchronicity from building the client on top of netty that is a multithreaded, event-driven I/O framework. Asynchronous methodologies allow you to utilize better system resources, instead of wasting threads waiting for network or disk I/O. Threads can be fully utilised to perform other work instead.\nFor the purpose of having a concurrently processing system, it’s convenient, in this scenario, to have all communication handled asynchronously. There are scenarios where this might not be the case, where you have quick running tasks and try to access data that has just been invalidated by a different task.\nConnecting to Redis spring-boot-starter-data-redis resolves Lettuce by default. Spring provides LettuceConnectionFactory to get connections. To get pooled connection factory we need to provide commons-pool2 on the classpath\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  redis-cli CONFIG GET databases INFO keyspace  ","date":1461834487,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461834487,"objectID":"8a6d1d482c97039b3a5e348f7fd96ad9","permalink":"https://wubigo.com/post/redis-with-spring-boot-v2/","publishdate":"2016-04-28T17:08:07+08:00","relpermalink":"/post/redis-with-spring-boot-v2/","section":"post","summary":"LETTUCE VS JEDIS While Jedis is easy to use and supports a vast number of Redis features, it is not thread safe and needs connection pooling to work in a multi-threaded environment. Connection pooling comes at the cost of a physical connection per Jedis instance which increases the number of Redis connections.\nLettuce, on the other hand, is built on netty (https://netty.io/) and connection instances can be shared across multiple threads.","tags":["CACHE","REDIS"],"title":"Redis With Spring Boot V2","type":"post"},{"authors":null,"categories":[],"content":"  namespace  kube-logging.yaml\nkind: Namespace apiVersion: v1 metadata: name: kube-logging   headless service  kubectl create -f kube-logging.yaml  elasticsearch_svc.yaml\nkind: Service apiVersion: v1 metadata: name: elasticsearch namespace: kube-logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node   PROVISION local PV for EFK  local PV\n Creating the StatefulSet  elasticsearch_statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: kube-logging spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.4.3 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.zen.ping.unicast.hosts value: \u0026quot;es-cluster-0.elasticsearch\u0026quot; - name: discovery.zen.minimum_master_nodes value: \u0026quot;1\u0026quot; - name: ES_JAVA_OPTS value: \u0026quot;-Xms512m -Xmx512m\u0026quot; initContainers: - name: fix-permissions image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026quot;] securityContext: privileged: true volumeMounts: - name: data mountPath: /usr/share/elasticsearch/data - name: increase-vm-max-map image: busybox command: [\u0026quot;sysctl\u0026quot;, \u0026quot;-w\u0026quot;, \u0026quot;vm.max_map_count=262144\u0026quot;] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;ulimit -n 65536\u0026quot;] securityContext: privileged: true volumeClaimTemplates: - metadata: name: data labels: app: elasticsearch spec: accessModes: [ \u0026quot;ReadWriteOnce\u0026quot; ] storageClassName: local-hdd resources: requests: storage: 10Gi  elasticsearch--oss suffix ensures the open-source version of Elasticsearch the default version(without suffix) containing X-Pack\ncheck Elasticsearch is ready kubectl port-forward es-cluster-0 9200:9200 --namespace=kube-logging curl http://localhost:9200/_cluster/state?pretty | grep master_node ... { \u0026quot;cluster_name\u0026quot; : \u0026quot;k8s-logs\u0026quot;, \u0026quot;compressed_size_in_bytes\u0026quot; : 230, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;9dm998tzS-Ko45dGEOtnDQ\u0026quot;, \u0026quot;version\u0026quot; : 2, \u0026quot;state_uuid\u0026quot; : \u0026quot;oLlQU_qiT8iit4SYx9S1-g\u0026quot;, \u0026quot;master_node\u0026quot; : \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot;, \u0026quot;blocks\u0026quot; : { }, \u0026quot;nodes\u0026quot; : { \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;es-cluster-0\u0026quot;, \u0026quot;ephemeral_id\u0026quot; : \u0026quot;PoResMCsTyG99qfK-U44_w\u0026quot;, \u0026quot;transport_address\u0026quot; : \u0026quot;10.2.12.86:9300\u0026quot;, \u0026quot;attributes\u0026quot; : { } } }, \u0026quot;metadata\u0026quot; : { \u0026quot;cluster_uuid\u0026quot; : \u0026quot;9dm998tzS-Ko45dGEOtnDQ\u0026quot;, \u0026quot;templates\u0026quot; : { }, \u0026quot;indices\u0026quot; : { }, \u0026quot;index-graveyard\u0026quot; : { \u0026quot;tombstones\u0026quot; : [ ] } }, \u0026quot;routing_table\u0026quot; : { \u0026quot;indices\u0026quot; : { } }, \u0026quot;routing_nodes\u0026quot; : { \u0026quot;unassigned\u0026quot; : [ ], \u0026quot;nodes\u0026quot; : { \u0026quot;icsCEGzzRCmITCGzGbsSSg\u0026quot; : [ ] } }, \u0026quot;restore\u0026quot; : { \u0026quot;snapshots\u0026quot; : [ ] }, \u0026quot;snapshots\u0026quot; : { \u0026quot;snapshots\u0026quot; : [ ] }, \u0026quot;snapshot_deletions\u0026quot; : { \u0026quot;snapshot_deletions\u0026quot; : [ ] } } ...  Deploy Kibana kibana.yaml\napiVersion: v1 kind: Service metadata: name: kibana namespace: kube-logging labels: app: kibana spec: ports: - port: 5601 selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: kube-logging labels: app: kibana spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: docker.elastic.co/kibana/kibana-oss:6.4.3 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601   检查kibana  kubectl port-forward svc/kibana 5601:5601 -n kube-logging curl http://localhost:5601  Deploy Fluentd DaemonSet DaemonSet is a Kubernetes workload type that runs a copy of a given Pod on each Node in the Kubernetes cluster\n ServiceAccount  fluentd.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: fluentd namespace: kube-logging labels: app: fluentd --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd labels: app: fluentd rules: - apiGroups: - \u0026quot;\u0026quot; resources: - pods - namespaces verbs: - get - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: kube-logging --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: kube-logging labels: app: fluentd spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: serviceAccount: fluentd serviceAccountName: fluentd tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule restartPolicy: Never containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 env: - name: FLUENT_ELASTICSEARCH_HOST value: \u0026quot;elasticsearch.kube-logging.svc.cluster.local\u0026quot; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026quot;9200\u0026quot; - name: FLUENT_ELASTICSEARCH_SCHEME value: \u0026quot;http\u0026quot; - name: FLUENT_UID value: \u0026quot;0\u0026quot; resources: limits: memory: 512Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers   tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule\n kubectl run fluent -it --image=fluent/fluentd-kubernetes-daemonset:v0.12-debian-elasticsearch --restart=Never Error attaching, falling back to logs: standard_init_linux.go:207: exec user process caused \u0026quot;no such file or directory\u0026quot; pod default/fluent terminated (Error)  https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes\n","date":1460873522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460873522,"objectID":"77fe77e3b44b5a69df1a34de25eebca0","permalink":"https://wubigo.com/post/k8s-logging-efk/","publishdate":"2016-04-17T14:12:02+08:00","relpermalink":"/post/k8s-logging-efk/","section":"post","summary":"namespace  kube-logging.yaml\nkind: Namespace apiVersion: v1 metadata: name: kube-logging   headless service  kubectl create -f kube-logging.yaml  elasticsearch_svc.yaml\nkind: Service apiVersion: v1 metadata: name: elasticsearch namespace: kube-logging labels: app: elasticsearch spec: selector: app: elasticsearch clusterIP: None ports: - port: 9200 name: rest - port: 9300 name: inter-node   PROVISION local PV for EFK  local PV\n Creating the StatefulSet  elasticsearch_statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster namespace: kube-logging spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch image: docker.","tags":["K8S","EFK"],"title":"K8s日志EFK","type":"post"},{"authors":null,"categories":[],"content":" Dockerfile ENTRYPOINT有两种形式\n exec shell      exec(preferred) shell     ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] command param1 param2   Command line arguments to docker run  appended not being used     ENTRYPOINT will be started as a subcommand of /bin/sh -c   default N/A /bin/sh -c (start it with exec to sned stop signal)   CMD [“exec_cmd”, “p1_cmd”] exec_entry p1_entry exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry    ENTRYPOINT exec FROM alpine:3.8 ENTRYPOINT [\u0026quot;top\u0026quot;, \u0026quot;-b\u0026quot;]  因为没有sh进程，所以命令行没有环境变量替换。\n可以增加sh\nFROM alpine:3.8 ENTRYPOINT [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;] CMD [\u0026quot;exec top -b -u $UID\u0026quot;]  ENTRYPOINT shell CMD无效\nFROM alpine:3.8 ENTRYPOINT exec top -b  ENTRYPOINT作为sh的子命令执行即\n实际执行/bin/sh -c \u0026quot;exec top -b\u0026quot;\n验证\ndocker build -t cmd . docker run -it --rm cmd cmd1 cmd2 cmd3 Mem: 4750556K used, 4318692K free, 555320K shrd, 176952K buff, 1907592K cached CPU: 0% usr 0% sys 0% nic 99% idle 0% io 0% irq 0% sirq Load average: 0.40 0.28 0.22 4/973 5 PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND 1 0 root R 1524 0% 1 0% top -b  ENTRYPOINT [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] CMD [\u0026quot;exec java -jar $APP_FILE\u0026quot;]  类似执行如下指令\n/bin/sh -c \u0026quot;exec java -jar $APP_FILE\u0026quot;  ","date":1460510580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1460510580,"objectID":"c230d1b1e6b16213b5cbce726a9cb373","permalink":"https://wubigo.com/post/docker-dockerfile-entrypoint/","publishdate":"2016-04-13T09:23:00+08:00","relpermalink":"/post/docker-dockerfile-entrypoint/","section":"post","summary":"Dockerfile ENTRYPOINT有两种形式\n exec shell      exec(preferred) shell     ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] command param1 param2   Command line arguments to docker run  appended not being used     ENTRYPOINT will be started as a subcommand of /bin/sh -c   default N/A /bin/sh -c (start it with exec to sned stop signal)   CMD [“exec_cmd”, “p1_cmd”] exec_entry p1_entry exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry    ENTRYPOINT exec FROM alpine:3.","tags":["DOCKER"],"title":"Docker Dockerfile ENTRYPOINT","type":"post"},{"authors":null,"categories":null,"content":" log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”\n“the throughput of a log remains more or less constant, since every message is written to disk anyway [18]. This behavior is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: such systems are fast when queues are short and become much slower when they start writing to”\n","date":1458518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458518400,"objectID":"f2461a9a52c992d8848c5d9cb177bea0","permalink":"https://wubigo.com/post/2016-03-21-streamdataprocessing/","publishdate":"2016-03-21T00:00:00Z","relpermalink":"/post/2016-03-21-streamdataprocessing/","section":"post","summary":"log-based vs memory-based broker “Thus, in situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. On the other hand, in situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.”","tags":null,"title":"streaming note","type":"post"},{"authors":null,"categories":null,"content":" Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud\n","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"5626be8c6b83784a26e59051195a8098","permalink":"https://wubigo.com/post/2016-03-01-backendsforfrontends/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/post/2016-03-01-backendsforfrontends/","section":"post","summary":"Backends for Frontends https://www.thoughtworks.com/insights/blog/bff-soundcloud","tags":null,"title":"Backends for Frontends","type":"post"},{"authors":null,"categories":[],"content":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.wikipedia.org/wiki/ANSI_escape_code#Colors black) code=30 ;; red) code=31 ;; green) code=32 ;; yellow) code=33 ;; blue) code=34 ;; magenta) code=35 ;; cyan) code=36 ;; white) code=37 ;; esac if [ \u0026quot;$code\u0026quot; ]; then codes=( \u0026quot;${codes[@]}\u0026quot; \u0026quot;$code\u0026quot; ) fi fi local IFS=';' echo -en '\\033['\u0026quot;${codes[*]}\u0026quot;'m' } wrap_color() { text=\u0026quot;$1\u0026quot; shift color \u0026quot;$@\u0026quot; echo -n \u0026quot;$text\u0026quot; color reset echo } wrap_good() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; white): $(wrap_color \u0026quot;$2\u0026quot; green)\u0026quot; } wrap_bad() { echo \u0026quot;$(wrap_color \u0026quot;$1\u0026quot; bold): $(wrap_color \u0026quot;$2\u0026quot; bold red)\u0026quot; } wrap_warning() { wrap_color \u0026gt;\u0026amp;2 \u0026quot;$*\u0026quot; red } check_flag() { if is_set_in_kernel \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled' elif is_set_as_module \u0026quot;$1\u0026quot;; then wrap_good \u0026quot;CONFIG_$1\u0026quot; 'enabled (as module)' else wrap_bad \u0026quot;CONFIG_$1\u0026quot; 'missing' EXITCODE=1 fi } check_flags() { for flag in \u0026quot;$@\u0026quot;; do echo -n \u0026quot;- \u0026quot;; check_flag \u0026quot;$flag\u0026quot; done } check_command() { if command -v \u0026quot;$1\u0026quot; \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then wrap_good \u0026quot;$1 command\u0026quot; 'available' else wrap_bad \u0026quot;$1 command\u0026quot; 'missing' EXITCODE=1 fi } check_device() { if [ -c \u0026quot;$1\u0026quot; ]; then wrap_good \u0026quot;$1\u0026quot; 'present' else wrap_bad \u0026quot;$1\u0026quot; 'missing' EXITCODE=1 fi } check_distro_userns() { source /etc/os-release 2\u0026gt;/dev/null || /bin/true if [[ \u0026quot;${ID}\u0026quot; =~ ^(centos|rhel)$ \u0026amp;\u0026amp; \u0026quot;${VERSION_ID}\u0026quot; =~ ^7 ]]; then # this is a CentOS7 or RHEL7 system grep -q \u0026quot;user_namespace.enable=1\u0026quot; /proc/cmdline || { # no user namespace support enabled wrap_bad \u0026quot; (RHEL7/CentOS7\u0026quot; \u0026quot;User namespaces disabled; add 'user_namespace.enable=1' to boot command line)\u0026quot; EXITCODE=1 } fi } if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;warning: $CONFIG does not exist, searching other paths for kernel config ...\u0026quot; for tryConfig in \u0026quot;${possibleConfigs[@]}\u0026quot;; do if [ -e \u0026quot;$tryConfig\u0026quot; ]; then CONFIG=\u0026quot;$tryConfig\u0026quot; break fi done if [ ! -e \u0026quot;$CONFIG\u0026quot; ]; then wrap_warning \u0026quot;error: cannot find kernel config\u0026quot; wrap_warning \u0026quot; try running this script again, specifying the kernel config:\u0026quot; wrap_warning \u0026quot; CONFIG=/path/to/kernel/.config $0 or $0 /path/to/kernel/.config\u0026quot; exit 1 fi fi wrap_color \u0026quot;info: reading kernel config from $CONFIG ...\u0026quot; white echo echo 'Generally Necessary:' echo -n '- ' cgroupSubsystemDir=\u0026quot;$(awk '/[, ](cpu|cpuacct|cpuset|devices|freezer|memory)[, ]/ \u0026amp;\u0026amp; $3 == \u0026quot;cgroup\u0026quot; { print $2 }' /proc/mounts | head -n1)\u0026quot; cgroupDir=\u0026quot;$(dirname \u0026quot;$cgroupSubsystemDir\u0026quot;)\u0026quot; if [ -d \u0026quot;$cgroupDir/cpu\u0026quot; -o -d \u0026quot;$cgroupDir/cpuacct\u0026quot; -o -d \u0026quot;$cgroupDir/cpuset\u0026quot; -o -d \u0026quot;$cgroupDir/devices\u0026quot; -o -d \u0026quot;$cgroupDir/freezer\u0026quot; -o -d \u0026quot;$cgroupDir/memory\u0026quot; ]; then echo \u0026quot;$(wrap_good 'cgroup hierarchy' 'properly mounted') [$cgroupDir]\u0026quot; else if [ \u0026quot;$cgroupSubsystemDir\u0026quot; ]; then echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'single mountpoint!') [$cgroupSubsystemDir]\u0026quot; else echo \u0026quot;$(wrap_bad 'cgroup hierarchy' 'nonexistent??')\u0026quot; fi EXITCODE=1 echo \u0026quot; $(wrap_color '(see https://github.com/tianon/cgroupfs-mount)' yellow)\u0026quot; fi if [ \u0026quot;$(cat /sys/module/apparmor/parameters/enabled 2\u0026gt;/dev/null)\u0026quot; = 'Y' ]; then echo -n '- ' if command -v apparmor_parser \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_good 'apparmor' 'enabled and tools installed')\u0026quot; else echo \u0026quot;$(wrap_bad 'apparmor' 'enabled, but apparmor_parser missing')\u0026quot; echo -n ' ' if command -v apt-get \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(use \u0026quot;apt-get install apparmor\u0026quot; to fix this)')\u0026quot; elif command -v yum \u0026amp;\u0026gt; /dev/null; then echo \u0026quot;$(wrap_color '(your best bet is \u0026quot;yum install apparmor-parser\u0026quot;)')\u0026quot; else echo \u0026quot;$(wrap_color '(look for an \u0026quot;apparmor\u0026quot; package for your distribution)')\u0026quot; fi EXITCODE=1 fi fi flags=( NAMESPACES {NET,PID,IPC,UTS}_NS CGROUPS CGROUP_CPUACCT CGROUP_DEVICE CGROUP_FREEZER CGROUP_SCHED CPUSETS MEMCG KEYS VETH BRIDGE BRIDGE_NETFILTER NF_NAT_IPV4 IP_NF_FILTER IP_NF_TARGET_MASQUERADE NETFILTER_XT_MATCH_{ADDRTYPE,CONNTRACK,IPVS} IP_NF_NAT NF_NAT NF_NAT_NEEDED # required for bind-mounting /dev/mqueue into containers POSIX_MQUEUE ) check_flags \u0026quot;${flags[@]}\u0026quot; if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -lt 8 ]; then check_flags DEVPTS_MULTIPLE_INSTANCES fi echo echo 'Optional Features:' { check_flags USER_NS check_distro_userns } { check_flags SECCOMP } { check_flags CGROUP_PIDS } { CODE=${EXITCODE} check_flags MEMCG_SWAP MEMCG_SWAP_ENABLED if [ -e /sys/fs/cgroup/memory/memory.memsw.limit_in_bytes ]; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently enabled)' bold black)\u0026quot; EXITCODE=${CODE} elif is_set MEMCG_SWAP \u0026amp;\u0026amp; ! is_set MEMCG_SWAP_ENABLED; then echo \u0026quot; $(wrap_color '(cgroup swap accounting is currently not enabled, you can enable it by setting boot option \u0026quot;swapaccount=1\u0026quot;)' bold black)\u0026quot; fi } { if is_set LEGACY_VSYSCALL_NATIVE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NATIVE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(dangerous, provides an ASLR-bypassing target with usable ROP gadgets.)' bold black)\u0026quot; elif is_set LEGACY_VSYSCALL_EMULATE; then echo -n \u0026quot;- \u0026quot;; wrap_good \u0026quot;CONFIG_LEGACY_VSYSCALL_EMULATE\u0026quot; 'enabled' elif is_set LEGACY_VSYSCALL_NONE; then echo -n \u0026quot;- \u0026quot;; wrap_bad \u0026quot;CONFIG_LEGACY_VSYSCALL_NONE\u0026quot; 'enabled' echo \u0026quot; $(wrap_color '(containers using eglibc \u0026lt;= 2.13 will not work. Switch to' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' \u0026quot;CONFIG_VSYSCALL_[NATIVE|EMULATE]\u0026quot; or use \u0026quot;vsyscall=[native|emulate]\u0026quot;' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' on kernel command line. Note that this will disable ASLR for the,' bold black)\u0026quot; echo \u0026quot; $(wrap_color ' VDSO which may assist in exploiting security vulnerabilities.)' bold black)\u0026quot; # else Older kernels (prior to 3dc33bd30f3e, released in v4.40-rc1) do # not have these LEGACY_VSYSCALL options and are effectively # LEGACY_VSYSCALL_EMULATE. Even older kernels are presumably # effectively LEGACY_VSYSCALL_NATIVE. fi } if [ \u0026quot;$kernelMajor\u0026quot; -lt 4 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 4 -a \u0026quot;$kernelMinor\u0026quot; -le 5 ]; then check_flags MEMCG_KMEM fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 18 ]; then check_flags RESOURCE_COUNTERS fi if [ \u0026quot;$kernelMajor\u0026quot; -lt 3 ] || [ \u0026quot;$kernelMajor\u0026quot; -eq 3 -a \u0026quot;$kernelMinor\u0026quot; -le 13 ]; then netprio=NETPRIO_CGROUP else netprio=CGROUP_NET_PRIO fi flags=( BLK_CGROUP BLK_DEV_THROTTLING IOSCHED_CFQ CFQ_GROUP_IOSCHED CGROUP_PERF CGROUP_HUGETLB NET_CLS_CGROUP $netprio CFS_BANDWIDTH FAIR_GROUP_SCHED RT_GROUP_SCHED IP_NF_TARGET_REDIRECT IP_VS IP_VS_NFCT IP_VS_PROTO_TCP IP_VS_PROTO_UDP IP_VS_RR ) check_flags \u0026quot;${flags[@]}\u0026quot; if ! is_set EXT4_USE_FOR_EXT2; then check_flags EXT3_FS EXT3_FS_XATTR EXT3_FS_POSIX_ACL EXT3_FS_SECURITY if ! is_set EXT3_FS || ! is_set EXT3_FS_XATTR || ! is_set EXT3_FS_POSIX_ACL || ! is_set EXT3_FS_SECURITY; then echo \u0026quot; $(wrap_color '(enable these ext3 configs if you are using ext3 as backing filesystem)' bold black)\u0026quot; fi fi check_flags EXT4_FS EXT4_FS_POSIX_ACL EXT4_FS_SECURITY if ! is_set EXT4_FS || ! is_set EXT4_FS_POSIX_ACL || ! is_set EXT4_FS_SECURITY; then if is_set EXT4_USE_FOR_EXT2; then echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext3 or ext4 as backing filesystem' bold black)\u0026quot; else echo \u0026quot; $(wrap_color 'enable these ext4 configs if you are using ext4 as backing filesystem' bold black)\u0026quot; fi fi echo '- Network Drivers:' echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags VXLAN | sed 's/^/ /' echo ' Optional (for encrypted networks):' check_flags CRYPTO CRYPTO_AEAD CRYPTO_GCM CRYPTO_SEQIV CRYPTO_GHASH \\ XFRM XFRM_USER XFRM_ALGO INET_ESP INET_XFRM_MODE_TRANSPORT | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ipvlan' blue)'\u0026quot;:' check_flags IPVLAN | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'macvlan' blue)'\u0026quot;:' check_flags MACVLAN DUMMY | sed 's/^/ /' echo ' - \u0026quot;'$(wrap_color 'ftp,tftp client in container' blue)'\u0026quot;:' check_flags NF_NAT_FTP NF_CONNTRACK_FTP NF_NAT_TFTP NF_CONNTRACK_TFTP | sed 's/^/ /' # only fail if no storage drivers available CODE=${EXITCODE} EXITCODE=0 STORAGE=1 echo '- Storage Drivers:' echo ' - \u0026quot;'$(wrap_color 'aufs' blue)'\u0026quot;:' check_flags AUFS_FS | sed 's/^/ /' if ! is_set AUFS_FS \u0026amp;\u0026amp; grep -q aufs /proc/filesystems; then echo \u0026quot; $(wrap_color '(note that some kernels include AUFS patches but not the AUFS_FS flag)' bold black)\u0026quot; fi [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'btrfs' blue)'\u0026quot;:' check_flags BTRFS_FS | sed 's/^/ /' check_flags BTRFS_FS_POSIX_ACL | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'devicemapper' blue)'\u0026quot;:' check_flags BLK_DEV_DM DM_THIN_PROVISIONING | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'overlay' blue)'\u0026quot;:' check_flags OVERLAY_FS | sed 's/^/ /' [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 echo ' - \u0026quot;'$(wrap_color 'zfs' blue)'\u0026quot;:' echo -n \u0026quot; - \u0026quot;; check_device /dev/zfs echo -n \u0026quot; - \u0026quot;; check_command zfs echo -n \u0026quot; - \u0026quot;; check_command zpool [ \u0026quot;$EXITCODE\u0026quot; = 0 ] \u0026amp;\u0026amp; STORAGE=0 EXITCODE=0 EXITCODE=$CODE [ \u0026quot;$STORAGE\u0026quot; = 1 ] \u0026amp;\u0026amp; EXITCODE=1 echo check_limit_over() { if [ $(cat \u0026quot;$1\u0026quot;) -le \u0026quot;$2\u0026quot; ]; then wrap_bad \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; wrap_color \u0026quot; This should be set to at least $2, for example set: sysctl -w kernel/keys/root_maxkeys=1000000\u0026quot; bold black EXITCODE=1 else wrap_good \u0026quot;- $1\u0026quot; \u0026quot;$(cat $1)\u0026quot; fi } echo 'Limits:' check_limit_over /proc/sys/kernel/keys/root_maxkeys 10000 echo exit $EXITCODE   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh\n","date":1456394317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456394317,"objectID":"dd1820587bd381fc88912b6de01b433f","permalink":"https://wubigo.com/post/docker-check-config/","publishdate":"2016-02-25T17:58:37+08:00","relpermalink":"/post/docker-check-config/","section":"post","summary":"#!/usr/bin/env bash set -e EXITCODE=0 # bits of this were adapted from lxc-checkconfig # see also https://github.com/lxc/lxc/blob/lxc-1.0.2/src/lxc/lxc-checkconfig.in possibleConfigs=( '/proc/config.gz' \u0026quot;/boot/config-$(uname -r)\u0026quot; \u0026quot;/usr/src/linux-$(uname -r)/.config\u0026quot; '/usr/src/linux/.config' ) if [ $# -gt 0 ]; then CONFIG=\u0026quot;$1\u0026quot; else : ${CONFIG:=\u0026quot;${possibleConfigs[0]}\u0026quot;} fi if ! command -v zgrep \u0026amp;\u0026gt; /dev/null; then zgrep() { zcat \u0026quot;$2\u0026quot; | grep \u0026quot;$1\u0026quot; } fi kernelVersion=\u0026quot;$(uname -r)\u0026quot; kernelMajor=\u0026quot;${kernelVersion%%.*}\u0026quot; kernelMinor=\u0026quot;${kernelVersion#$kernelMajor.}\u0026quot; kernelMinor=\u0026quot;${kernelMinor%%.*}\u0026quot; is_set() { zgrep \u0026quot;CONFIG_$1=[y|m]\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_in_kernel() { zgrep \u0026quot;CONFIG_$1=y\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } is_set_as_module() { zgrep \u0026quot;CONFIG_$1=m\u0026quot; \u0026quot;$CONFIG\u0026quot; \u0026gt; /dev/null } color() { local codes=() if [ \u0026quot;$1\u0026quot; = 'bold' ]; then codes=( \u0026quot;${codes[@]}\u0026quot; '1' ) shift fi if [ \u0026quot;$#\u0026quot; -gt 0 ]; then local code= case \u0026quot;$1\u0026quot; in # see https://en.","tags":["DOCKER"],"title":"Docker Check Config","type":"post"},{"authors":null,"categories":["IT"],"content":" K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。\nK8S解决的网络问题：\n 容器间通信问题： 由POD和localhost通信解决\n POD间通信问题： 由CNI解决 POD和服务的通信问题： 由SERVICE解决 外部系统和SERVICE的通信问题： 由SERVICE解决  ","date":1456313943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456313943,"objectID":"896fb0dffd51ac94259bf63b206f5f45","permalink":"https://wubigo.com/post/k8s-network-basic/","publishdate":"2016-02-24T19:39:03+08:00","relpermalink":"/post/k8s-network-basic/","section":"post","summary":" K8S网络基础\nK8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。\n Minikube kops kubeadm  如果你想搭建一个测试环境，请参考\n 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:\n 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型\n 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。\nK8S解决的网络问题：\n 容器间通信问题： 由POD和localhost通信解决\n POD间通信问题： 由CNI解决 POD和服务的通信问题： 由SERVICE解决 外部系统和SERVICE的通信问题： 由SERVICE解决  ","tags":["K8S","NETWORK"],"title":"K8S网络基础","type":"post"},{"authors":null,"categories":[],"content":" REMOVE ROLE delete policy before delete role\naws iam list-roles aws iam list-role-policies --role-name api-executor aws iam delete-role-policy --role-name api-executor -policy-name \u0026quot;log-writer\u0026quot; aws iam delete-role --role-name pizza-api-executor  ADD ROLE POLICY aws iam put-role-policy \\ --role-name pizza-api-executor \\ --policy-name PizzaApiDynamoDB \\ --policy-document file://./roles/dynamodb.json  You need to provide a path to dynamodb.json with the file:// prefix. If you are providing an absolute path, keep in mind that you will have three slashes after file:. The first two are for file://, and the third one is from the absolute path, because it starts with a slash.\n","date":1455949823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455949823,"objectID":"65a3bd0a087d29d461650cde1e7b892a","permalink":"https://wubigo.com/post/aws-iam-notes/","publishdate":"2016-02-20T14:30:23+08:00","relpermalink":"/post/aws-iam-notes/","section":"post","summary":"REMOVE ROLE delete policy before delete role\naws iam list-roles aws iam list-role-policies --role-name api-executor aws iam delete-role-policy --role-name api-executor -policy-name \u0026quot;log-writer\u0026quot; aws iam delete-role --role-name pizza-api-executor  ADD ROLE POLICY aws iam put-role-policy \\ --role-name pizza-api-executor \\ --policy-name PizzaApiDynamoDB \\ --policy-document file://./roles/dynamodb.json  You need to provide a path to dynamodb.json with the file:// prefix. If you are providing an absolute path, keep in mind that you will have three slashes after file:.","tags":["AWS","IAM"],"title":"Aws Iam Notes","type":"post"},{"authors":null,"categories":null,"content":" cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM\n","date":1455235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455235200,"objectID":"02b5377f7051d7bda55ee55bcbc7e96e","permalink":"https://wubigo.com/post/2016-02-12-openstacknotes/","publishdate":"2016-02-12T00:00:00Z","relpermalink":"/post/2016-02-12-openstacknotes/","section":"post","summary":"cost effective network solutions In general , commercial product is better. Nuage is software-based ,while 华为，华三 are hardware-based. Nuage support container, bare metal and VM","tags":null,"title":"openstack notes","type":"post"},{"authors":null,"categories":["IT"],"content":" 更新到最新正式发布版V1.13.3\nMain external dependencies  go docker cri cni  external-dependencies\nKUBEADM IS CURRENTLY IN BETA\nkubeadm maturity build k8s  docker v17.03  sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 k8s.gcr.io/kube-apiserver-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 k8s.gcr.io/kube-controller-manager-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 k8s.gcr.io/kube-scheduler-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 k8s.gcr.io/kube-proxy-amd64:v1.11.7 docker pull mirrorgooglecontainers/pause:3.1 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.18 docker tag mirrorgooglecontainers/etcd-amd64:3.2.18 k8s.gcr.io/etcd-amd64:3.2.18 docker pull coredns/coredns:1.1.3 docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3   cri-tools v1.11.0\ngit clone https://github.com/kubernetes-sigs/cri-tools.git $GOPATH/src/github.com/kubernetes-sigs/ git checkout tags/v1.13.0 -b v1.13.0 make $GOPATH/bin/crictl -version cp $GOPATH/bin/cri* /usr/local/bin/  install-go-1.10\n checkout v1.11.7\ngit clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/ git fetch --all git checkout tags/v1.11.7 -b v1.11.7  or\ngit clone -b v1.11.7 https://github.com/kubernetes/kubernetes.git  LOCAL ETCD INTEGRATION\n+++ source /home/bigo/go/src/k8s.io/kubernetes/hack/lib/etcd.sh ++++ ETCD_VERSION=3.2.24 ++++ ETCD_HOST=127.0.0.1 ++++ ++++ KUBE_INTEGRATION_ETCD_URL=http://127.0.0.1:2379  build v1.11.7\n  cd kubernetes git remote add upstream https://github.com/kubernetes/kubernetes.git git remote set-url --push upstream no_push git fetch upstream git tag|grep v1.11.7 git checkout tags/v1.11.7 -b \u0026lt;branch_name\u0026gt; docker pull mirrorgooglecontainers/kube-cross:v1.10.7-1 docker tag mirrorgooglecontainers/kube-cross:v1.10.7-1 k8s.gcr.io/kube-cross:v1.10.7-1  export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 bash -x ./build/run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1 ./_output/dockerized/bin/linux/amd64/kubeadm version| grep v1.11.7  or\nmake quick-release ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubeadm version| grep v1.11.7  deploy K8S with kubeadm  install kubectl\nsudo cp ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubectl /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubeadm /usr/bin/ sudo cp ./_output/release-stage/server/linux-amd64/kubernetes/server/bin/kubelet /usr/bin/  kubeadm kubectl bash completion\nkubeadm completion bash \u0026gt; ~/.kube/kubeadm_completion.bash.inc echo \u0026quot;source '$HOME/.kube/kubeadm_completion.bash.inc'\\n\u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc  install kubelet service\nsudo cp ./build/debs/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ./build/debs/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet sudo useradd -G systemd-journal $USER journalctl -xeu kubelet  disable swap\nsudo swapoff -a  build cni v0.6.0\ngit clone -b v0.6.0 https://github.com/containernetworking/cni.git cd cni ./build.sh mkdir -p /opt/cni/bin cp bin/* /opt/cni/bin/  Configure NetworkManager for calio\n  NetworkManager manipulates the routing table for interfaces in the default network namespace where Calico veth pairs are anchored for connections to containers. This can interfere with the Calico agent’s ability to route correctly. Create the following configuration file at /etc/NetworkManager/conf.d/calico.conf to prevent NetworkManager from interfering with the interfaces:\n[keyfile] unmanaged-devices=interface-name:cali*;interface-name:tunl*   bootstrap a secure Kubernetes cluster debug level with -v\nsudo kubeadm init --kubernetes-version=v1.11.7 --pod-network-cidr 10.2.0.0/16 -v 4  configure kubectl\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  calico setup\n  calico etcd setup\nkubectl apply -f calico.yaml (https://docs.projectcalico.org/v3.7/getting-started/kubernetes/installation/hosted/calico.yaml)   kubectl completion code for bash\n# Installing bash completion on Linux kubectl completion bash \u0026gt; ~/.kube/kubectl.bash.inc printf \u0026quot; # Kubectl shell completion source '$HOME/.kube/kubectl.bash.inc' \u0026quot; \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc  Control plane node isolation\n  By default, the cluster will not schedule pods on the master for security reasons. If you want to be able to schedule pods on the master, e.g. for a single-machine Kubernetes cluster for development, run:\nkubectl taint nodes --all node-role.kubernetes.io/master-  view cluster config kubectl describe configmaps kubeadm-config -n kube-system journalctl -xe | grep -i etcd  or\ncd /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml  ETCD liveness probe kubectl describe pods etcd-bigo-vm3 -n kube-system Liveness: exec [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] sudo curl -v -l https://127.0.0.1:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt --key /etc/kubernetes/pki/etcd/healthcheck-client.key  kubectl exec -it etcd-bigo-vm1 -n kube-system -- sh ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-cl ient.key member list  join a node  install docker v17.03 IPVS proxier load IPVS mod install ebtables socat  apt install ebtables socat   install kubelet service get token  kubeadm token list   get token-ca-cert-hash  openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //'   all in one shell\n deploy-work-node.sh\n token recreate By default, tokens expire after 24 hours. Joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the master node:\nkubeadm token create  Deploying the Dashboard\n  sa-admin-user.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system  rb-admin-user.yaml\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system  kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml  Bearer Token\nkubectl proxy kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/   depoly nginx and verify  creates a Deployment object and an associated ReplicaSet object with 2 pods\nkubectl run nginx1-14 --generator=run-pod/v1 --labels=\u0026quot;run=nginx1.14\u0026quot; --image=nginx:1.14-alpine --port=80 POD_IP=$(kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 1) curl $POD_IP kubectl get pods -o wide | grep nginx1-14 | awk '{print $6}' | head -n 2 |xargs printf -- 'http://%s\\n'|xargs curl  Kubernetes requires a none-stop app/CMD Docker container stop automatically after running **K8S will restart it at default if a container stop **\ntest/curl/Dockerfile\n***let kubectl never restart container\nFROM alpine:3.8 RUN apk add --no-cache curl CMD [\u0026quot;sh\u0026quot;] docker build . docker tag curl-alpine:1.0 kubectl run curl -it --image=curl-alpine:1.0 --restart=Never sh   tear down cluster  kubectl drain \u0026lt;node name\u0026gt; --delete-local-data --force --ignore-daemonsets kubectl delete node \u0026lt;node name\u0026gt;  Then, on the node being removed, reset all kubeadm installed state:\nkubeadm reset  The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:\niptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X  If you want to reset the IPVS tables, you must run the following command:\nipvsadm -C  sudo kubeadm init phase etcd local \u0026ndash;config=configfile.yaml -v4\n\u0026ndash;kubernetes-version=v1.11.7\nkubeadm init \u0026ndash;config\netcd: local: serverCertSANs: - \u0026quot;0.0.0.0\u0026quot; peerCertSANs: - \u0026quot;0.0.0.0\u0026quot; extraArgs: listen-client-urls: --listen-client-urls=https://0.0.0.0:2379  ","date":1454470707,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454470707,"objectID":"105972d42fa16a684ab40158e6690e53","permalink":"https://wubigo.com/post/k8s-local-development-setup/","publishdate":"2016-02-03T11:38:27+08:00","relpermalink":"/post/k8s-local-development-setup/","section":"post","summary":"Setup a local development environment from source code with kubeadm","tags":["PAAS","K8S"],"title":"K8S local development setup from source code","type":"post"},{"authors":null,"categories":null,"content":" Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.\nTokens are generated by the gateway, and sent to the underlying microservices: as they share a common secret key, microservices are able to validate the token, and authenticate users using that token.\nThose tokens are self-sufficient: they have both authentication and authorization information, so microservices do not need to query a database or an external system. This is important in order to ensure a scalable architecture\n","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"3c82f77b18f4006263734d8d02804c2e","permalink":"https://wubigo.com/post/2016-02-01-microservice-notes/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/post/2016-02-01-microservice-notes/","section":"post","summary":"Microservices at Netflix Scale https://gotocon.com/dl/goto-amsterdam-2016/slides/RuslanMeshenberg_MicroservicesAtNetflixScaleFirstPrinciplesTradeoffsLessonsLearned.pdf\nsecuring microservice with UAA\nuser accounting and authorizing service(UAA) Using JWT authentication without manually forwarding JWTs from request to internal request forces microservices to call other microservices over the gateway, which involves additional internal requests per one master requests. But even with forwarding, it’s not possible to cleanly separate user and machine authentication.\nJWT (JSON Web Token) JWT (JSON Web Token) is an industry standard, easy-to-use method for securing applications in a microservices architecture.","tags":["MICROSERVICE"],"title":"microservice notes","type":"post"},{"authors":null,"categories":[],"content":" Multi-stage builds in Docker only support for Doceker version \u0026gt; 17.05\nhttps://blog.alexellis.io/mutli-stage-docker-builds/\nFROM golang:1.10 as builder # build env and make target FROM alpine:latest WORKDIR /root/ COPY --from=builder ./  busybox nslookup busybox:latest has bug on nslookup\ndocker network create test 32024cd09daca748f8254468f4f00893afc2e1173c378919b1f378ed719f1618 docker run -dit --name nginx --network test nginx:alpine 7feaf1f0b4f3d421603bbb984854b753c7cbc6b581dd0a304d3b8fccf8c6604b $ docker run -it --rm --network test busybox:1.28 nslookup nginx Server: 127.0.0.11 Address 1: 127.0.0.11 Name: nginx Address 1: 172.22.0.2 nginx.test docker stop nginx docker network rm test  docker proxy /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service] Environment=\u0026quot;HTTP_PROXY=http://127.0.0.1:33351/\u0026quot; Environment=\u0026quot;HTTPS_PROXY=http://127.0.0.1:33351/\u0026quot;  sudo systemctl daemon-reload sudo systemctl restart docker systemctl show --property=Environment docker  docker clean up disk space  delete volumes   $ docker volume rm $(docker volume ls -qf dangling=true) $ docker volume ls -qf dangling=true | xargs -r docker volume rm   docker rmi $(docker images --filter \u0026quot;dangling=true\u0026quot; -q --no-trunc) docker rmi $(docker images | grep \u0026quot;none\u0026quot; | awk '/ / { print $3 }') docker rm $(docker ps -qa --no-trunc --filter \u0026quot;status=exited\u0026quot;)  Caution\ndocker system prune -a  ubuntu docker Post-installation steps  check to docker log for warning  journalctl -xu docker journalctl -xu docker.service   check-config   curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh \u0026gt; check-config.sh\n docker run -d --name web httpd:2.4.38-alpine docker run --name mysql -e MYSQL_ROOT_PASSWORD=mysql -d mysql:5.5 docker run -it --name curl bigo/curl:v1 sudo pipework br1 web 192.168.1.117/24 sudo pipework br1 mysql 192.168.1.118/24 sudo pipework br1 curl 192.168.1.119/24 docker exec -it curl curl 192.168.117 docker logs web 192.168.1.119 - - [28/Feb/2019:10:09:15 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 45 192.168.1.119 - - [28/Feb/2019:10:15:43 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 45  pipework eth2 web\n文件 CONTAINDER_ID = $(docker run -d image) NS_PID = $(head -n 1 /sys/fs/cgroup/devices/docker/$CONTAINDER_ID/tasks) LOCAL_PAIR_VETH=veth\u0026lt;NO\u0026gt;pl\u0026lt;NS_PID\u0026gt; GUEST_PAIR_VETH=veth\u0026lt;NO\u0026gt;pg\u0026lt;NS_PID\u0026gt; ip link set veth1pl1452 master br1 ip link set veth1pl1452 up ip link set veth1pg1452 netns 1452 ip netns exec 1452 ip link set veth1pg1452 name eth1 ip netns exec 1452 ip -4 addr add 192.168.1.118/24 dev eth1 ip netns exec 1452 ip -4 link set eth1 up  为容器配置路由\nsudo nsenter -t $(docker-pid web) -n ip route del default sudo nsenter -t $(docker-pid web) -n ip route add default via 192.168.1.1 dev eth0  容器间通信  icc inter-container communication  docker network create --driver bridge --subnet 192.168.200.0/24 --ip-range 192.168.200.0/24 -o \u0026quot;com.docker.network.bridge.enable_icc\u0026quot;=\u0026quot;false\u0026quot; no-icc-network   enable_ip_masquerade  是否允许NAT使用宿主的IP掩蔽来自容器访问宿主外的网络包的SOURCE IP\ncom.docker.network.bridge.enable_ip_masquerade  改变默认的数据存储位置和驱动  配置  daemon.json\n{ \u0026quot;data-root\u0026quot;: \u0026quot;/mnt/docker\u0026quot;, \u0026quot;storage-driver\u0026quot;: \u0026quot;overlay2\u0026quot; }   移动数据  systemctl stop docker mv /var/lib/docker/* /mnt/docker/ systemctl start docker  定位容器的VETH接口 docker exec CID sudo ethtool -S eth0 NIC statistics: peer_ifindex: 7 sudo ip link | grep 7   capture all incoming IP traffic destined to the node except local traffic\n sudo tcpdump -i enp0s25 tcp -n sudo tcpdump -i enp0s25 dst host 192.168.1.5 and not src net 192.168.1.0/24  [1] https://www.securitynik.com/2016/12/docker-networking-internals-how-docker_16.html\n","date":1453713065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453713065,"objectID":"cb36c07a249c5cfc1646698d05813209","permalink":"https://wubigo.com/post/docker-notes/","publishdate":"2016-01-25T17:11:05+08:00","relpermalink":"/post/docker-notes/","section":"post","summary":"Multi-stage builds in Docker only support for Doceker version \u0026gt; 17.05\nhttps://blog.alexellis.io/mutli-stage-docker-builds/\nFROM golang:1.10 as builder # build env and make target FROM alpine:latest WORKDIR /root/ COPY --from=builder ./  busybox nslookup busybox:latest has bug on nslookup\ndocker network create test 32024cd09daca748f8254468f4f00893afc2e1173c378919b1f378ed719f1618 docker run -dit --name nginx --network test nginx:alpine 7feaf1f0b4f3d421603bbb984854b753c7cbc6b581dd0a304d3b8fccf8c6604b $ docker run -it --rm --network test busybox:1.28 nslookup nginx Server: 127.0.0.11 Address 1: 127.0.0.11 Name: nginx Address 1: 172.","tags":["DOCKER"],"title":"Docker Notes","type":"post"},{"authors":null,"categories":[],"content":"","date":1449322208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1449322208,"objectID":"2f1e4183b542ba37ad2c2a22293ea15e","permalink":"https://wubigo.com/post/software-development-teams/","publishdate":"2015-12-05T21:30:08+08:00","relpermalink":"/post/software-development-teams/","section":"post","summary":"","tags":["DEV"],"title":"Software Development Teams","type":"post"},{"authors":null,"categories":null,"content":" git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.js to render Markdown as html  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;title\u0026gt;Bigo release\u0026lt;/title\u0026gt; \u0026lt;xmp theme=\u0026quot;united\u0026quot; style=\u0026quot;display:none;\u0026quot;\u0026gt; \u0026lt;/xmp\u0026gt; \u0026lt;script src=\u0026quot;http://strapdownjs.com/v/0.2/strapdown.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt;  ","date":1443830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443830400,"objectID":"d65862ec22ce872b3ad144e0f9cedc9c","permalink":"https://wubigo.com/post/2015-10-03-nginx-git-log-as-relasenote/","publishdate":"2015-10-03T00:00:00Z","relpermalink":"/post/2015-10-03-nginx-git-log-as-relasenote/","section":"post","summary":"git-changelog-maven-plugin \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;se.bjurr.gitchangelog\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;git-changelog-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.50\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;GenerateGitChangelog\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;git-changelog\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- A file on filesystem //--\u0026gt; \u0026lt;file\u0026gt;CHANGELOG.md\u0026lt;/file\u0026gt; \u0026lt;toRef\u0026gt;HEAD\u0026lt;/toRef\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;  get a copy of mustache template and save as changelog.mustache under the project home directory https://github.com/tomasbjerre/git-changelog-lib/tree/master/src/test/resources/templates  mvn compile to create the CHANGELOG.md mvn compile  upload the CHANGELOG.md to nginx as a release not config nginx support browser MD mime.types text/markdown md;  reload nginx and check the release note as text use template with StrapDown.","tags":null,"title":"Git log as release note","type":"post"},{"authors":null,"categories":null,"content":" Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com. 285 IN A 185.199.110.153 wubigo.com. 285 IN A 185.199.108.153 wubigo.com. 285 IN A 185.199.111.153 wubigo.com. 285 IN A 185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"9033a7185a446a46f551aacb1f1b98d7","permalink":"https://wubigo.com/post/2015-08-03-github-notes/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-github-notes/","section":"post","summary":"Configuring A records with DNS provider @ 185.199.108.153 @ 185.199.109.153 @ 185.199.110.153 @ 185.199.111.153  dig the custom domain to confirm DNS setup $dig +noall +answer wubigo.com wubigo.com. 285 IN A 185.199.110.153 wubigo.com. 285 IN A 185.199.108.153 wubigo.com. 285 IN A 185.199.111.153 wubigo.com. 285 IN A 185.199.109.153  https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider","tags":null,"title":"github notes","type":"post"},{"authors":null,"categories":null,"content":" GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters. With POST, form data appears within the message body of the HTTP request\nAuthors of services which use the HTTP protocol SHOULD NOT use GET based forms for the submission of sensitive data, because this will cause this data to be encoded in the Request-URI. Many existing servers, proxies, and user agents will log the request URI in some place where it might be visible to third parties. Servers can use POST-based form submission instead\nFinally, an important consideration when using GET for AJAX requests is that some browsers - IE in particular - will cache the results of a GET request. So if you, for example, poll using the same GET request you will always get back the same results, even if the data you are querying is being updated server-side. One way to alleviate this problem is to make the URL unique for each request by appending a timestamp.\nRestrictions on form data length Yes, since form data is in the URL and URL length is restricted. A safe URL length limit is often 2048 characters but varies by browser and web server.\nFrom the Dropbox developer blog:\n a browser doesn’t know exactly what a particular HTML form does, but if the form is submitted via HTTP GET, the browser knows it’s safe to automatically retry the submission if there’s a network error. For forms that use HTTP POST, it may not be safe to retry so the browser asks the user for confirmation first.  A \u0026ldquo;GET\u0026rdquo; request is often cacheable, whereas a \u0026ldquo;POST\u0026rdquo; request can hardly be. For query systems this may have a considerable efficiency impact, especially if the query strings are simple, since caches might serve the most frequent queries.\nhttp://www.diffen.com/difference/GET-vs-POST-HTTP-Requests\nrotate catalina out without restarting tomcat The catalina.out file is created by a shell redirection, ex \u0026ldquo;\u0026gt;\u0026gt; catalina.out 2\u0026gt;\u0026amp;1\u0026rdquo;. This catches anything written to System.out and System.err and places it into the catalina.out file. Given this, a good way to rotate catalina.out is to alter the script to pipe the output to a log rotation script rather than directly to a file. This will allow you to rotate the logs without restarting Tomcat and without copying the entire contents of the log to another file. It\u0026rsquo;s a pretty simple change to catalina.sh and it is described at this link.\n[http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2](http://marc.info/?l=tomcat-user\u0026amp;m=105640876032532\u0026amp;w=2) [https://dzone.com/articles/how-rotate-tomcat-catalinaout](https://dzone.com/articles/how-rotate-tomcat-catalinaout) cat /dev/null \u0026gt; logs/catalina.out  tomcat connector config  \u0026lt;Connector address=\u0026quot;127.0.0.1\u0026quot; port=\u0026quot;8080\u0026quot; protocol=\u0026quot;org.apache.coyote.http11.Http11Nio2Protocol\u0026quot; connectionTimeout=\u0026quot;30000\u0026quot; redirectPort=\u0026quot;8443\u0026quot; executor=\u0026quot;tomcatThreadPool\u0026quot; minProcessors=\u0026quot;100\u0026quot; maxProcessors=\u0026quot;300\u0026quot; enableLookups=\u0026quot;false\u0026quot; acceptCount=\u0026quot;500\u0026quot; maxPostSize=\u0026quot;-1\u0026quot; disableUploadTimeout=\u0026quot;false\u0026quot; connectionUploadTimeout=\u0026quot;600000\u0026quot; compression=\u0026quot;on\u0026quot; compressionMinSize=\u0026quot;2048\u0026quot; noCompressionUserAgents=\u0026quot;gozilla, traviata\u0026quot; acceptorThreadCount=\u0026quot;2\u0026quot; compressableMimeType=\u0026quot;text/html,text/xml,text/plain,text/css,text/javascript,application/javascript\u0026quot; URIEncoding=\u0026quot;utf-8\u0026quot;/\u0026gt;  A CharacterEncodingFilter sets the body encoding, but not the URI encoding. Need to set URIEncoding=\u0026ldquo;UTF-8\u0026rdquo; as an attribute in all the connectors in Tomcat server.xml\nThe request.setCharacterEncoding(\u0026ldquo;UTF-8\u0026rdquo;); only sets the encoding of the request body (which is been used by POST requests), not the encoding of the request URI (which is been used by GET requests).\nenabling gzip with nginx and verifying that it\u0026rsquo;s working curl -H \u0026quot;Accept-Encoding: gzip,deflate\u0026quot; -I http://web/resource  https://www.digitalocean.com/community/tutorials/how-to-add-the-gzip-module-to-nginx-on-ubuntu-16-04\n","date":1438560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438560000,"objectID":"5096f8e4f05b72c0d0fc1652271e313c","permalink":"https://wubigo.com/post/2015-08-03-http/","publishdate":"2015-08-03T00:00:00Z","relpermalink":"/post/2015-08-03-http/","section":"post","summary":"GET vs. POST HTTP POST requests supply additional data from the client (browser) to the server in the message body. In contrast, GET requests include all required data in the URL. Forms in HTML can use either method by specifying method=\u0026ldquo;POST\u0026rdquo; or method=\u0026ldquo;GET\u0026rdquo; (default) in the  element. The method specified determines how form data is submitted to the server. When the method is GET, all form data is encoded into the URL, appended to the action URL as query string parameters.","tags":null,"title":"http  TIL","type":"post"},{"authors":null,"categories":null,"content":" NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-auth-pam --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-dav-ext-module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-echo --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/nginx-upstream-fair --add-dynamic-module=/build/nginx-aqArPM/nginx-1.12.1/debian/modules/ngx_http_substitutions_filter_module  nginxproxy.md\nStep 3 \u0026ndash; Configure /\nLet say we want to configure nginx to route requests for /, /blog, and /mail, respectively onto localhost:8080, localhost:8181, and localhost:8282.\n +--- host --------\u0026gt; node.js on localhost:8080 | users --\u0026gt; nginx --|--- host/blog ---\u0026gt; node.js on localhost:8181 | +--- host/mail ---\u0026gt; node.js on localhost:8282  To route /, you need to edit your nginx config file.\nIn the config file, find the server section:\nserver { listen 80; ... location / { ... } ... }  This section is simply telling nginx how it should serve HTTP requests.\nNow, change the location section to this snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } ... }  proxy_pass simply tells nginx to forward requests to / to the server listening on http://127.0.0.1:8080.\nStep 4 \u0026ndash; Reload nginx\u0026rsquo;s Configuration\nTo reload nginx\u0026rsquo;s configuration run: nginx -s reload on your machine.\nReferesh your browser. Do you see the output from your node.js application? If yes, you are all set. If no, there is a problem with your config.\nStep 5 \u0026ndash; Add /blog and /mail\nTo redirect /mail and /blog, you simply need to add new entries the location section in the config file:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { proxy_pass http://127.0.0.1:8181; } location /mail { proxy_pass http://127.0.0.1:8282; } ... }  Step 6 \u0026ndash; Reload Your nginx Configuration\nRun nginx -s reload on your machine.\nLog onto localhost:$PORT/blog in your browser. Do you see the output from your second node.js application?\nThen log onto localhost:$PORT/mail. Do you see the output from your third node.js application?\nIf yes \u0026amp; yes, you are all set. If no, there is a problem with your config.\nStep 7 \u0026ndash; Rewriting Requests\nNow as you might have noticed in Step 6, nginx sends the same HTTP request to your node.js web apps which results into a 404 error. Why? Because, your node.js web application serves requests from / not from /blog and /mail. But, nginx is sending requests to /blog and /mail.\nTo fix this issue, we need rewrite the URL so that it matches the URL you can serve on your node.js applications.\nTo correctly rewrite URLs change your config file to match the following snippet:\nserver { listen ...; ... location / { proxy_pass http://127.0.0.1:8080; } location /blog { rewrite ^/blog(.*) /$1 break; proxy_pass http://127.0.0.1:8181; } location /mail { rewrite ^/mail(.*) /$1 break; proxy_pass http://127.0.0.1:8282; } ... }  This rewrite commands are simple regular expressions that transform strings like /blogWHAT_EVER and /mailWHAT_EVER to /WHAT_EVER in the HTTP requests.\nStep 8 \u0026ndash; Reload and Test.\nAll set?\nExercise 1\nConfigure your nginx to redirect URLs from /google to http://www.google.com\nStep 9 (optional) \u0026ndash; Redirecting Based on Host Name\nLet say you want to host example1.com, example2.com, and example3.com on your machine, respectively to localhost:8080, localhost:8181, and localhost:8282.\nNote: Since you don\u0026rsquo;t have access to a DNS server, you should add domain name entries to your /etc/hosts (you can\u0026rsquo;t do this on CDF machines):\n\u0026hellip; 127.0.0.1 example1.com example2.com example3.com \u0026hellip; To proxy eaxmple1.com we can\u0026rsquo;t use the location part of the default server. Instead we need to add another server section with a server_name set to our virtual host (e.g., example1.com, \u0026hellip;), and then a simple location section that tells nginx how to proxy the requests:\nserver { listen 80; server_name example1.com; location / { proxy_pass http://127.0.0.1:8080; } } server { listen 80; server_name example2.com; location / { proxy_pass http://127.0.0.1:8181; } } server { listen 80; server_name example3.com; location / { proxy_pass http://127.0.0.1:8282; } }  ","date":1435881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435881600,"objectID":"0039d8c563b2aa08c54a20528746eafd","permalink":"https://wubigo.com/post/2015-06-03-nginx-proxy-rewrite/","publishdate":"2015-07-03T00:00:00Z","relpermalink":"/post/2015-06-03-nginx-proxy-rewrite/","section":"post","summary":"NGINX 1.12 ON UBUNTU 16 https://launchpad.net/~nginx/+archive/ubuntu/stable\nubuntu@ip-192-168-133-137:/etc/nginx$ nginx -V nginx version: nginx/1.12.1 built with OpenSSL 1.0.2g 1 Mar 2016 TLS SNI support enabled configure arguments: --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -fPIC -Wdate-time -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -fPIC' --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-client-body-temp-path=/var/lib/nginx/body --http-fastcgi-temp-path=/var/lib/nginx/fastcgi --http-proxy-temp-path=/var/lib/nginx/proxy --http-scgi-temp-path=/var/lib/nginx/scgi --http-uwsgi-temp-path=/var/lib/nginx/uwsgi --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_geoip_module=dynamic --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_sub_module --with-http_xslt_module=dynamic --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-mail=dynamic --with-mail_ssl_module --add-dynamic-module=/build/nginx-aqArPM/nginx-1.","tags":null,"title":"nginx-proxy-rewrite","type":"post"},{"authors":null,"categories":null,"content":" update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.\nCreate a new branch and switch to it (so all of your latest commits are stored here)\ngit checkout -b your_new_branch\nSwitch back to your previous working branch (assume it\u0026rsquo;s master)\ngit checkout master\nRemove the latest x commits, keep master clean\ngit reset \u0026ndash;hard HEAD~x # in your case, x = 3\nFrom this moment on, all the latest x commits are only in the new branch, not in your previous working branch (master) any more.\nshow head commit and relationship betwen local and upstream branch git branch -a -vv (HEAD detached at v0.6.9) 0ad6fa1 update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 master 0ad6fa1 [origin/master] update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 (#158) remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master 0ad6fa1 update data-plane-api to 4f4ee118c54b7a52393c7d45b94f6bd5040a7118 (#158)  Ignoring an already checked-in directory git rm -r --cached \u0026lt;your directory\u0026gt;  The -r option causes the removal of all files under your directory. The \u0026ndash;cached option causes the files to only be removed from git\u0026rsquo;s index, not your working copy. By default git rm  would delete \npush a new local branch to a remote Git repository and track it git checkout -b \u0026lt;branch\u0026gt; | git branch \u0026lt;branch\u0026gt; git push -u origin \u0026lt;branch\u0026gt;  Adding Only Untracked Files git add -i. Type a (for \u0026ldquo;add untracked\u0026rdquo;), then * (for \u0026ldquo;all\u0026rdquo;), then q (to quit)\nDiscard all Changes not staged for commit git checkout \u0026ndash; .\nCreate a new empty branch and import from svn git checkout --orphan \u0026lt;branchname\u0026gt; git rm --cached -r . svn checkout git add . git commit -m \u0026quot;backup from svn tag\u0026quot; git push --set-upstream origin \u0026lt;branchname\u0026gt;  save username and password in git git config credential.helper store then git pull  ~/.git-credentials\nI delete a Git branch both locally and remotely Executive Summary $ git push -d origin  $ git branch -d  Delete Local Branch To delete the local branch use:\n$ git branch -d branch_name or use: $ git branch -D branch_name As of Git v1.7.0, you can delete a remote branch using $ git push origin \u0026ndash;delete \ngit without proxy method 1\n$ env|grep proxy http_proxy=http://192.168.0.119:3128/ socks_proxy=socks://192.168.0.119:3128/ https_proxy=https://192.168.0.119:3128/ $ unset http_proxy $ git pull  method 2(proxy for certain git urls/domains)\n@web:~/workspace/git/pub$ cat .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true [http] proxy = \u0026quot;\u0026quot; [https] proxy = \u0026quot;\u0026quot;  https://www.andrewpage.me/tracking-down-bugs-with-git-bisect/ https://medium.com/@fredrikmorken/why-you-should-stop-using-git-rebase-5552bee4fed1\ncheck the list of tags on the upstream repo without cloning or fetchingn git ls-remote https://github.com/go-delve/delve git ls-remote --tags https://github.com/go-delve/delve  the difference between origin and upstream on GitHub  upstream generally refers to the original repo that you have forked (see also \u0026ldquo;Definition of “downstream” and “upstream”\u0026rdquo; for more on upstream term) origin is your fork: your own repo on GitHub, clone of the original repo of GitHub  From the GitHub page:\n When a repo is cloned, it has a default remote called origin that points to your fork on GitHub, not the original repo it was forked from. To keep track of the original repo, you need to add another remote named upstream  git remote add upstream git://github.com/user/repo.git  You will use upstream to fetch from the original repo (in order to keep your local copy in sync with the project you want to contribute to).\ngit fetch upstream\n(git fetch alone would fetch from origin by default, which is not what is needed here)\n","date":1433289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433289600,"objectID":"fa9ccaaa4a347547a004bff4879385df","permalink":"https://wubigo.com/post/2015-06-03-git-notes/","publishdate":"2015-06-03T00:00:00Z","relpermalink":"/post/2015-06-03-git-notes/","section":"post","summary":"update a fork on GitHub with upstream git fetch upstream git rebase upstream/master (sync upstream update to local master branch) git push (update the fork by push)  Moving a git repository $ git remote show origin $ git remote rm origin $ git remote add origin https://github.com/wubigo/wubigo.github.io.git $ git remote show origin $ git pull origin master  Branch from a previous commit using Git The magic can be done by git reset.","tags":null,"title":"git note","type":"post"},{"authors":null,"categories":null,"content":" understand of the HBase data model http://jimbojw.com/#understanding hbase\n","date":1428019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428019200,"objectID":"e8284e4c6ce95a8614ae8baf1b4a00f2","permalink":"https://wubigo.com/post/2015-04-03-hbase-notes/","publishdate":"2015-04-03T00:00:00Z","relpermalink":"/post/2015-04-03-hbase-notes/","section":"post","summary":"understand of the HBase data model http://jimbojw.com/#understanding hbase","tags":null,"title":"HBase notes","type":"post"},{"authors":null,"categories":[],"content":" cloud data management https://dataschool.com/data-governance\n三层数据仓库架构 Generally a data warehouses adopts a three-tier architecture. Following are the three tiers of the data warehouse architecture.\n Bottom Tier − The bottom tier of the architecture is the data warehouse database server. It is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.\n Middle Tier − In the middle tier, we have the OLAP Server that can be implemented in either of the following ways.\n Top-Tier − This tier is the front-end client layer. This layer holds the query tools and reporting tools, analysis tools and data mining tools.\n  Star schema vs. Snowflake Schema     Star Schema Snowflake Schema     Understandability Easier for business users and analysts to query data Maybe more difficult for business users and analysts due to a number of tables they have to deal with   Dimension table Only has one dimension table for each dimension that groups related attributes. Dimension tables are not in the third normal form May have more than 1 dimension table for each dimension due to the further normalization of each dimension table. Dimension tables are in the third normal form (3NF)   Query complexity The query is very simple and easy to understand More complex query due to multiple foreign keys joins between dimension tables   Query performance High performance. The database engine can optimize and boost the query performance based on a predictable framework More foreign key joins, therefore, longer execution time of query in compare with star schema   When to use When dimension tables store a relatively small number of rows, space is not a big issue we can use star schema When dimension tables store a large number of rows with redundancy data and space is such an issue, we can choose snowflake schema to save space.   Foreign Key Joins Fewer Joins Higher number of joins   Data warehouse system Work best in any data warehouse/data mart Better for small data warehouse/ data mart    ETL工具    Vendor ETL Product Strengths Weaknesses     Informatica Data Integration Platform Highly rated by analystsExtensive product portfolio Reputation for high prices Overlapping products    基本概念  dimensions vs facts  It may help to think of dimensions as things or objects. A thing such as a product can exist without ever being involved in a business event. A dimension is your noun. It is something that can exist independent of a business event, such as a sale. Products, employees, equipment, are all things that exist. A dimension either does something, or has something done to it.\nEmployees sell, customers buy. Employees and customers are examples of dimensions, they do.\nProducts are sold, they are also dimensions as they have something done to them.\nFacts, are the verb. An entry in a fact table marks a discrete event that happens to something from the dimension table. A product sale would be recorded in a fact table. The event of the sale would be noted by what product was sold, which employee sold it, and which customer bought it. Product, Employee, and Customer are all dimensions that describe the event, the sale.\nIn addition fact tables also typically have some kind of quantitative data. The quantity sold, the price per item, total price, and so on.\n","date":1426040315,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426040315,"objectID":"d7490aa465c12cdb78e61643e26be628","permalink":"https://wubigo.com/post/data-warehouse/","publishdate":"2015-03-11T10:18:35+08:00","relpermalink":"/post/data-warehouse/","section":"post","summary":"cloud data management https://dataschool.com/data-governance\n三层数据仓库架构 Generally a data warehouses adopts a three-tier architecture. Following are the three tiers of the data warehouse architecture.\n Bottom Tier − The bottom tier of the architecture is the data warehouse database server. It is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.","tags":["DATAWAREHOUSE","OLAP","ETL"],"title":"数据仓库","type":"post"},{"authors":null,"categories":null,"content":" 10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/\n","date":1425340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425340800,"objectID":"9fd7942affbbeffab10c766676cf5991","permalink":"https://wubigo.com/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","publishdate":"2015-03-03T00:00:00Z","relpermalink":"/post/2015-03-03-10highly-impactful-books-you-should-definitely-check-out/","section":"post","summary":"10 Highly Impactful Books You Should Definitely Check Out https://thoughtcatalog.com/ayodeji-awosika/2015/06/10-highly-impactful-books-you-should-definitely-check-out/","tags":null,"title":"10 Highly Impactful Books You Should Definitely Check Out","type":"post"},{"authors":null,"categories":null,"content":" TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL. For making sure, your MySQL is UTF-8, run the following queries in your MySQL prompt: First query:\nmysql\u0026gt; show variables like \u0026lsquo;char%\u0026rsquo;;\ntomcat deploy for dev conf/Catalina/localhost/ROOT.xml \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;Context docBase=\u0026quot;/opt/etender\u0026quot; path=\u0026quot;\u0026quot; /\u0026gt; $TOMCAT_HOME/conf/server.xml \u0026lt;!-- \u0026lt;Context path=\u0026quot;/etender\u0026quot; docBase=\u0026quot;c:/WebRoot\u0026quot;\u0026gt; \u0026lt;/Context\u0026gt; --\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt;  show the last queries executed on MySQL temporarily If you prefer to output to a file:\nSET GLOBAL log_output = \u0026quot;FILE\u0026quot;; which is set by default. #set absolute path will report error,mysql.log=/var/lib/mysql/mysql.log SET GLOBAL general_log_file = \u0026quot;mysql.log\u0026quot;; SET GLOBAL general_log = 'ON'; tail -f /var/lib/mysql/mysql.log  Optimize Your Tomcat Installation on Ubuntu 14.04 hange JVM Heap Setting (-Xms -Xmx) of Tomcat – Configure setenv.sh file\ndefault no setenv.sh file under /bin directory. Have to create one with below parameters.\nXms=Xmx=1/2RAM( avoid having the costly memory allocation process running because the size of the allocated memory will be constant all the time) MaxPermSize=1/2mx\n$cat setenv.sh export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xms512m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -Xmx8192m\u0026rdquo; export CATALINA_OPTS=\u0026ldquo;$CATALINA_OPTS -XX:MaxPermSize=256m\u0026rdquo;\ncatalina.out to verify the setting in effect catalina.startup.VersionLoggerListener.log Command line argument: -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djdk.tls.ephemeralDHKeySize=2048 catalina.startup.VersionLoggerListener.log Command line argument: -Djava.protocol.handler.pkgs=org.apache.catalina.webresources org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xms4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Xmx4192m org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -XX:MaxPermSize=2000m\noptimize mysql showing current configuration variables mysql\u0026gt;SHOW VARIABLES LIKE \u0026lsquo;%max%\u0026rsquo;;\ninnodb_file_per_table = ON innodb_stats_on_metadata = OFF innodb_buffer_pool_instances = 8 innodb_buffer_pool_size = 1G query_cache_type = 0 query_cache_size = 0 innodb_log_file_size = 5242880 innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0 innodb_flush_method = O_DIRECT\ntomcat upstat on ubuntu14.04 /etc/init/tomcat.conf description \u0026ldquo;Tomcat Server\u0026rdquo;\nstart on runlevel [2345] stop on runlevel [!2345] respawn respawn limit 10 5\nsetuid tomcat setgid tomcat\nenv JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre env CATALINA_HOME=/opt/tomcat\n# Modify these options as needed env JAVA_OPTS=\u0026ldquo;-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom\u0026rdquo; env CATALINA_OPTS=\u0026ldquo;-Xms512M -Xmx1024M -server -XX:+UseParallelGC\u0026rdquo;\nexec $CATALINA_HOME/bin/catalina.sh run\n# cleanup temp directory after stop post-stop script rm -rf $CATALINA_HOME/temp/* end script\nsudo initctl reload-configuration Tomcat is ready to be run. Start it with this command: sudo initctl start tomcat\nsudo sh -c \u0026lsquo;echo manual \u0026gt;\u0026gt; /etc/init/tomcat.override\u0026rsquo;\ndeploy web app as the root context in tomcat in the $CATALINA_BASE/conf/[enginename]/[hostname]/ROOT.XML \u0026lt;?xml version=\u0026ldquo;1.0\u0026rdquo; encoding=\u0026ldquo;UTF-8\u0026rdquo;?\u0026gt; authbind tomcat sudo apt install authbind sudo touch /etc/authbind/byport/{443,80} sudo chmod 500 /etc/authbind/byport/{443,80} sudo chown tomcat:tomcat /etc/authbind/byport/{443,80}\nConfigure Tomcat sudo sed -i \u0026rsquo;s/8080/80/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml sudo sed -i \u0026rsquo;s/8443/443/g\u0026rsquo; /home/tomcat/apache-tomcat-8.0.35/conf/server.xml #TOMCAT_HOME/bin/setenv.sh export CATALINA_OPTS=\u0026ldquo;-Djava.net.preferIPv4Stack=true\u0026rdquo; #startup.sh: exec authbind \u0026ndash;deep \u0026ldquo;$PRGDIR\u0026rdquo;/\u0026ldquo;$EXECUTABLE\u0026rdquo; start \u0026ldquo;$@\u0026rdquo;\nupload big files with Nginx (Reverse proxy+SSL negotiation) and Tomcat solution 1: config nginx $TOMCAT_HOME/bin/server.xml\ndisableUploadTimeout=false In nginx.conf add: http { # at the END of this segment! client_max_body_size 1000m; }\nsolution 2 : config tomcat maxSwallowSize The maximum number of request body bytes (excluding transfer encoding overhead) that will be swallowed by Tomcat for an aborted upload. An aborted upload is when Tomcat knows that the request body is going to be ignored but the client still sends it. If Tomcat does not swallow the body the client is unlikely to see the response. If not specified the default of 2097152 (2 megabytes) will be used. A value of less than zero indicates that no limit should be enforced.\nMySql - changing innodb_file_per_table for a live db solution 1: mysql\u0026gt;set global innodb_file_per_table = 1 (set value to on doesn\u0026rsquo;t effect for mysql 5.5 )\nCross Origin Resource Sharing (CORS) with nginx  location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. # try_files $uri $uri/ =404; # Uncomment to enable naxsi on this location # include /etc/nginx/naxsi.rules add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';  remove Nginx Server Signature(reset server header in response) /etc/nginx/nginx.conf server_tokens off;  # 跨域的常见解决方法 目前来讲没有不依靠服务器端来跨域请求资源的技术 1.jsonp 需要目标服务器配合一个callback函数。 2.window.name+iframe 需要目标服务器响应window.name。 3.window.location.hash+iframe 同样需要目标服务器作处理。 4.html5的 postMessage+ifrme 这个也是需要目标服务器或者说是目标页面写一个postMessage，主要侧重于前端通讯。 5.CORS 需要服务器设置header ：Access-Control-Allow-Origin。 6.nginx反向代理 这个方法一般很少有人提及，但是他可以不用目标服务器配合，不过需要你搭建一个中转nginx服务器，用于转发请求。\nlocation / { #alias D:\\\\develop\\\\project1dir\\\\appDist\\\\; #此文件夹可以是项目打包后的上线代码文件，也可以是第二个项目源代码文件 # Frontend Server proxy_pass http://localhost:8002/; #前端服务器地址，比如gulp+browser-sync开启的服务器，能看到代码实时更新效果 } location /api/ { rewrite ^/api/(.*)$ /$1 break; #所有对后端的请求加一个api前缀方便区分，真正访问的时候移除这个前缀 # API Server proxy_pass http://serverB.com; #将真正的请求代理到serverB,即真实的服务器地址，ajax的url为/api/user/1的请求将会访问http://www.serverB.com/user/1 }  update time  sudo ntpdate ntp.ubuntu.com   https://help.ubuntu.com/lts/serverguide/NTP.html  ","date":1399075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399075200,"objectID":"c28933e174ef766366bbfa6f72dec90a","permalink":"https://wubigo.com/post/2014-05-03-doing_on_ubuntu/","publishdate":"2014-05-03T00:00:00Z","relpermalink":"/post/2014-05-03-doing_on_ubuntu/","section":"post","summary":"TZ $ export TZ=:/etc/localtime locale /etc/environment: LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8  1: set TZ=:/etc/localtime https://blog.packagecloud.io/eng/2017/02/21/set-environment-variable-save-thousands-of-system-calls/\nmysql utf8 For the recent version of MySQL,\ndefault-character-set = utf8 causes a problem. It\u0026rsquo;s deprecated I think.\nAs Justin Ball says in \u0026ldquo;Upgrade to MySQL 5.5.12 and now MySQL won’t start, you should:\nRemove that directive and you should be good. Then your configuration file (\u0026lsquo;/etc/my.cnf\u0026rsquo; for example) should look like that: [mysqld] collation-server = utf8_unicode_ci init-connect=\u0026lsquo;SET NAMES utf8\u0026rsquo; character-set-server = utf8 Restart MySQL.","tags":null,"title":"doing_on_ubuntu","type":"post"},{"authors":null,"categories":["IT"],"content":" The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie. don\u0026rsquo;t have any *.mydomain.com cookies), you can dramatically reduce the size (ie. number of packets sent) of the HTTP request, which would save on bandwidth and speed up requests significantly if you use cookies heavily on the main site. The page can benefit from a more simultaneous requests being made by the browser\nMost browsers will make simultaneous requests for page assets, like images, fonts, CSS, etc. The catch is that most browsers will only allow a limited number of open requests to a particular domain (somewhere around 5 I think). By spreading your assets across multiple sub-domains, you \u0026ldquo;trick\u0026rdquo; the browser, and allow more parallel requests, since the limit applies to each sub-domain.\nAdding an Alternate Domain Name Using the CloudFront Configure the DNS service for the domain to route traffic for the domain, such as example.com, to the CloudFront domain name for your distribution, such as d111111abcdef8.cloudfront.net. The method that you use depends on whether you\u0026rsquo;re using Amazon Route 53 as the DNS service provider for the domain:\nAmazon Route 53 Create an alias resource record set. With an alias resource record set, you don\u0026rsquo;t pay for Amazon Route 53 queries. In addition, you can create an alias resource record set for the root domain name (example.com), which DNS doesn\u0026rsquo;t allow for CNAMEs. For more information, see Routing Queries to an Amazon CloudFront Distribution in the Amazon Route 53 Developer Guide.\nAnother DNS service provider Use the method provided by your DNS service provider to add a CNAME resource record set to the hosted zone for your domain. This new CNAME resource record set will redirect DNS queries from your domain (for example, www.example.com) to the CloudFront domain name for your distribution (for example, d111111abcdef8.cloudfront.net). For more information, see the documentation provided by your DNS service provider.\nImportant If you already have an existing CNAME record for your domain name, update that resource record set or replace it with a new one that points to the CloudFront domain name for your distribution. In addition, confirm that your CNAME resource record set points to your distribution\u0026rsquo;s domain name and not to one of your origin servers.\nsteps to add cname record step 1: edit cloudfront distribution then add a Alternate Domain Name for cloudfront\nstep 2: add a CNAME resource record set by the DNS service provider\n","date":1396525143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396525143,"objectID":"687bdf0d2dfbe65200fb105b462c6165","permalink":"https://wubigo.com/post/2014-04-03-cdn/","publishdate":"2014-04-03T19:39:03+08:00","relpermalink":"/post/2014-04-03-cdn/","section":"post","summary":"The main point of a Content Distribution Network (CDN) is to put the content as close to the end-user as possible, thereby reducing the Distance component of the Round Trip Time (RTT) and speeding up the request. Simply serving static content from a s# sub-domain for CDN.\nThe advantages of serving content from such a sub-domain, however, are that\nThe sub-domain can be a cookie-less domain\nIf you use your cookies correctly (ie.","tags":["IAAS","NETWORK"],"title":"cdn note","type":"post"},{"authors":null,"categories":null,"content":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"c8218b6eddffe204e9229f561b8b4353","permalink":"https://wubigo.com/post/2014-03-03-boot/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-boot/","section":"post","summary":"1: all required dep on jsp tomcat\n\u0026lt;!-- Compile --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jstl\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Provided --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-tomcat\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-jasper\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  ","tags":null,"title":"SPRING-boot note","type":"post"},{"authors":null,"categories":null,"content":" Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io\n","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"31a1ec70c56747fca48bf551e86a68e8","permalink":"https://wubigo.com/post/2014-03-03-web-api-reference/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/post/2014-03-03-web-api-reference/","section":"post","summary":"Documenting the Web together https://blogs.windows.com/msedgedev/2017/10/18/documenting-web-together-mdn-web-docs/\nMDN https://developer.mozilla.org\nDEV DOCS https://devdocs.io","tags":null,"title":"web API reference","type":"post"},{"authors":null,"categories":[],"content":" JAVA 基础 JAVA 基础\n","date":1393571995,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393571995,"objectID":"7d63ec946220b61371d44d011cc913cb","permalink":"https://wubigo.com/post/effective-coding-java/","publishdate":"2014-02-28T15:19:55+08:00","relpermalink":"/post/effective-coding-java/","section":"post","summary":"JAVA 基础 JAVA 基础","tags":["LANG","JAVA"],"title":"Effective Coding Java","type":"post"},{"authors":null,"categories":null,"content":" steps to make ec2 access from outside  create vpc create internet gateway add route to the gateway into the routetable  RequestTimeTooSkewed error with S3 upload https://github.com/aws/aws-sdk-js/issues/399 https://aws.amazon.com/blogs/developer/clock-skew-correction/\ncloudfront set Origin Custom Headers https://w3guy.com/solution-font-origin-http-cdn-domain-blocked-loading-cors-policy/\n","date":1391385600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391385600,"objectID":"2b9ce00fb2f3a520ff4f334760374e55","permalink":"https://wubigo.com/post/2014-02-03-aws-notes/","publishdate":"2014-02-03T00:00:00Z","relpermalink":"/post/2014-02-03-aws-notes/","section":"post","summary":"steps to make ec2 access from outside  create vpc create internet gateway add route to the gateway into the routetable  RequestTimeTooSkewed error with S3 upload https://github.com/aws/aws-sdk-js/issues/399 https://aws.amazon.com/blogs/developer/clock-skew-correction/\ncloudfront set Origin Custom Headers https://w3guy.com/solution-font-origin-http-cdn-domain-blocked-loading-cors-policy/","tags":null,"title":"aws FAQ","type":"post"},{"authors":null,"categories":null,"content":" 1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\nBecause we’re trying to actually do something on the server, this action is asynchronous. All asynchronous methods in the ArangoDB driver return promises but you can also pass a node-style callback instead:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;, function (err) { if (!err) console.log(\u0026lsquo;Database created\u0026rsquo;); else console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err); });\nKeep in mind that the new database you’ve created is only available once the callback is called or the promise is resolved. Throughout this tutorial we’ll use the promise API because they’re available in recent versions of Node.js as well as most modern browsers.\n3: db = require(\u0026lsquo;arangojs\u0026rsquo;).Database; db = new Database(\u0026lsquo;http://127.0.0.1:8529'); db.useBasicAuth(\u0026lsquo;root\u0026rsquo;, \u0026lsquo;123123\u0026rsquo;); db.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );\ncheck arangodb status /etc/init.d/arangodb3 status\nenable remote connection /etc/arangodb3/arangod.conf #endpoint = tcp://[::]:8529\n","date":1388707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388707200,"objectID":"eda9722e1bb5103bd8e54f8995cb5bb1","permalink":"https://wubigo.com/post/2014-01-03-arangodb/","publishdate":"2014-01-03T00:00:00Z","relpermalink":"/post/2014-01-03-arangodb/","section":"post","summary":"1: ArangoDB automatically indexes the _key attribute in each collection. There is no need to index this attribute separately. Please note that a document\u0026rsquo;s _id attribute is derived from the _key attribute, and is thus implicitly indexed, too.\n2: Creating a database\nWe don’t want to mess with any existing data, so let’s start by creating a new database called “mydb”:\ndb.createDatabase(\u0026lsquo;mydb\u0026rsquo;).then( () =\u0026gt; console.log(\u0026lsquo;Database created\u0026rsquo;), err =\u0026gt; console.error(\u0026lsquo;Failed to create database:\u0026lsquo;, err) );","tags":null,"title":"arrangodb note","type":"post"},{"authors":null,"categories":null,"content":" 自动化发布 很多网站选择周四作为发布日\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"ec2354ee671052fa4120517131a73ab8","permalink":"https://wubigo.com/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/post/2014-01-01-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","section":"post","summary":"自动化发布 很多网站选择周四作为发布日","tags":null,"title":"大型网站技术架构","type":"post"},{"authors":null,"categories":[],"content":" JAVA 开发规范 目标：\n 容易维护 健壮 可复用     分类 指导     编码 UTF-8   注释 支持Swagger   git模式 PR(pull-request) / merge request   代码静态检查 遵守风格规范   代码提交 必须通过review   测试 所有代码都有单元测试   集成测试 所有代码必须通过集成测试才能提交到分支或主干    编码风格\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"fcc1bd42b5d7776465e12b1a08ddb837","permalink":"https://wubigo.com/post/java-coding-guidelines/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/java-coding-guidelines/","section":"post","summary":"JAVA 开发规范 目标：\n 容易维护 健壮 可复用     分类 指导     编码 UTF-8   注释 支持Swagger   git模式 PR(pull-request) / merge request   代码静态检查 遵守风格规范   代码提交 必须通过review   测试 所有代码都有单元测试   集成测试 所有代码必须通过集成测试才能提交到分支或主干    编码风格","tags":["JAVA","CODE"],"title":"Java编码规范","type":"post"},{"authors":null,"categories":[],"content":" kafka broker connected remotely by ip config/server.properties\nadvertised.listeners=PLAINTEXT://172.16.16.5:9092 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168  admin topic  list all topic  ./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --list   delete topic  delete.topic.enable option set to true\nDeleting a topic will also delete all its messages. This is not a reversible operation\n./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --delete --topic my-topic   describe topic  ./kafka-topics.sh --bootstrap-server 172.16.16.5:9092 --describe --topic alert1  admin consume group  list  ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --list  ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id   ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id --members CONSUMER-ID HOST CLIENT-ID #PARTITIONS consumer-2-c76c0c52-35e2-487e-9c63-39719cb2560b /192.168.200.67 consumer-2 0 consumer-2-25c4357a-38e4-4258-aa05-9072a824e03a /192.168.200.67 consumer-2 1   ./kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --describe --group group_id --members --verbose CONSUMER-ID HOST CLIENT-ID #PARTITIONS ASSIGNMENT consumer-2-c76c0c52-35e2-487e-9c63-39719cb2560b /192.168.200.67 consumer-2 0 - consumer-2-25c4357a-38e4-4258-aa05-9072a824e03a /192.168.200.67 consumer-2 1 users(0)   reset offset  test\nkafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group group_id --topic users --to-earliest --dry-run  execute\nkafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group group_id --topic users --to-earliest --execute  produce msg from console ./kafka-console-producer.sh --broker-list 172.16.16.5:9092 --topic users  consume msg from console  ./kafka-console-consumer.sh --bootstrap-server 172.16.16.5:9092 --topic users  spring boot with spring kafka Spring for Apache Kafka 2.0.x is not compatible with Spring Boot 2.1.x. You have to use Spring-Kafka 2.2.x. More over would be better to just rely on the dependency from Spring Boot per se. please, see https://start.spring.io/ for more info how properly start the project for Spring Boot.\noffset can\u0026rsquo;t be reset to earliest after log.rention period pass kafka-consumer-groups.sh --bootstrap-server 172.16.16.5:9092 --reset-offsets --group timon-alert --topic rawdata --to-earliest --execute TOPIC PARTITION NEW-OFFSET rawdata 0 174  counters for number of messages received since start-up kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 172.16.16.5:9092 --topic testrawdata --time -1 --offsets 1  Kafka consumer list kafka-consumer-groups.sh --list --bootstrap-server 172.16.16.5:9092  zkServer.sh start kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties  delete consumer group kafka-consumer-groups.sh --delete --bootstrap-server 172.16.16.5:9092 --group timon-raw  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"295678ee8ea547fa703a2f7ad5092651","permalink":"https://wubigo.com/post/kafka-config/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/kafka-config/","section":"post","summary":"kafka broker connected remotely by ip config/server.properties\nadvertised.listeners=PLAINTEXT://172.16.16.5:9092 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.","tags":["KAFKA","MQ"],"title":"Kafka Config","type":"post"},{"authors":null,"categories":[],"content":"    rabbit kafka     创建时间 2007 2011   开发语言 erlang scala   AMQP SUPPORT NO   AGENT SMART(broker-centric) keeps track of consumer state dumb(producer-centric)   存储空间 in-memory disk   INGRESS VOLUME 20K messages/sec 100k/sec messages/sec   CONSUMERS mostly online(balancing load to many consumer) online and batch consumer   ROUTING exchange, binding simple   history N/A replay(删除by size 或时间)   数据压缩 N Y   SPRING SUPPORT weak strong   安全 RBAC backed by a built-in data store, LDAP JAAS role based access   管理 Web 和 CLI JMX 和 CLI    ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"4c8acf1d549a1d6e8cb656a9afa85f2c","permalink":"https://wubigo.com/post/kafka-vs-rabbit/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/kafka-vs-rabbit/","section":"post","summary":"    rabbit kafka     创建时间 2007 2011   开发语言 erlang scala   AMQP SUPPORT NO   AGENT SMART(broker-centric) keeps track of consumer state dumb(producer-centric)   存储空间 in-memory disk   INGRESS VOLUME 20K messages/sec 100k/sec messages/sec   CONSUMERS mostly online(balancing load to many consumer) online and batch consumer   ROUTING exchange, binding simple   history N/A replay(删除by size 或时间)   数据压缩 N Y   SPRING SUPPORT weak strong   安全 RBAC backed by a built-in data store, LDAP JAAS role based access   管理 Web 和 CLI JMX 和 CLI    ","tags":["SHELL","MQ"],"title":"Kafka vs Rabbit","type":"post"},{"authors":null,"categories":[],"content":"settings.xml\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;central\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Maven Repository Switchboard\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://repo1.maven.org/maven2/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;repo2\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;Human Readable Name for this Mirror.\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://repo2.maven.org/maven2/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;!-- 中央仓库在中国的镜像 --\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;maven.net.cn\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;oneof the central mirrors in china\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.net.cn/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt;  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"4aa905e02147651434be7da9bdcb46f9","permalink":"https://wubigo.com/post/maven-mirror/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/maven-mirror/","section":"post","summary":"settings.xml\n\u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;central\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;Maven Repository Switchboard\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://repo1.maven.org/maven2/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;repo2\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;Human Readable Name for this Mirror.\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://repo2.maven.org/maven2/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;!-- 中央仓库在中国的镜像 --\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;maven.net.cn\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;oneof the central mirrors in china\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.net.cn/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt;  ","tags":["JAVA","MAVEN"],"title":"Maven Mirror","type":"post"},{"authors":null,"categories":[],"content":" mysqladmin status  MySQL is multithreaded, so there may be many clients issuing queries for a given table simultaneously. To minimize the problem with multiple client sessions having different states on the same table, the table is opened independently by each concurrent session. This uses additional memory but normally increases performance\nThe table_open_cache and max_connections system variables affect the maximum number of files the server keeps open. If you increase one or both of these values, you may run up against a limit imposed by your operating system on the per-process number of open file descriptors. Many operating systems permit you to increase the open-files limit, although the method varies widely from system to system. Consult your operating system documentation to determine whether it is possible to increase the limit and how to do so.\nTo determine whether your table cache is too small, check the Opened_tables status variable, which indicates the number of table-opening operations since the server started:\nmysql\u0026gt; SHOW GLOBAL STATUS LIKE 'Opened_tables';  ","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"abf5ecc089ebee4423bee7d57de0e064","permalink":"https://wubigo.com/post/mysql-tuning-on-many-tables/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-many-tables/","section":"post","summary":"mysqladmin status  MySQL is multithreaded, so there may be many clients issuing queries for a given table simultaneously. To minimize the problem with multiple client sessions having different states on the same table, the table is opened independently by each concurrent session. This uses additional memory but normally increases performance\nThe table_open_cache and max_connections system variables affect the maximum number of files the server keeps open. If you increase one or both of these values, you may run up against a limit imposed by your operating system on the per-process number of open file descriptors.","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on Many Tables","type":"post"},{"authors":null,"categories":[],"content":"innodb-memcached-multiple-get-range-query  native partitioning in-place APIs\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"64459f7f959493592c0eafcf23409fc7","permalink":"https://wubigo.com/post/mysql-tuning-on-query/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-query/","section":"post","summary":"innodb-memcached-multiple-get-range-query  native partitioning in-place APIs","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on Query","type":"post"},{"authors":null,"categories":[],"content":" install curl -O http://download.redis.io/redis-stable.tar.gz tar xzvf redis-stable.tar.gz cd redis-stable make make test sudo make install  config sudo mkdir /etc/redis sudo cp redis-stable/redis.conf /etc/redis sudo adduser --system --group --no-create-home redis sudo mkdir /var/lib/redis sudo chown redis:redis /var/lib/redis sudo chmod 770 /var/lib/redis  /etc/redis/redis.conf\nsupervised systemd dir /var/lib/redis # bind localhost  start redis-server /etc/redis/redis.conf  shutdown redis-cli shutdown redis-cli 127.0.0.1:6379\u0026gt; shutdown  run in docker docker run --name redis -network host -v /var/lib/redis:/data /etc/redis/redis.conf:/etc/redis/redis.conf -d redis:5.0-alpine3.9 redis-server /etc/redis/redis.conf --appendonly yes  https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-redis-on-ubuntu-16-04\n","date":1367140087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367140087,"objectID":"bbf927940d7fbfc43485a951a3d53874","permalink":"https://wubigo.com/post/redis-install-ubuntu/","publishdate":"2013-04-28T17:08:07+08:00","relpermalink":"/post/redis-install-ubuntu/","section":"post","summary":"install curl -O http://download.redis.io/redis-stable.tar.gz tar xzvf redis-stable.tar.gz cd redis-stable make make test sudo make install  config sudo mkdir /etc/redis sudo cp redis-stable/redis.conf /etc/redis sudo adduser --system --group --no-create-home redis sudo mkdir /var/lib/redis sudo chown redis:redis /var/lib/redis sudo chmod 770 /var/lib/redis  /etc/redis/redis.conf\nsupervised systemd dir /var/lib/redis # bind localhost  start redis-server /etc/redis/redis.conf  shutdown redis-cli shutdown redis-cli 127.0.0.1:6379\u0026gt; shutdown  run in docker docker run --name redis -network host -v /var/lib/redis:/data /etc/redis/redis.","tags":["REDIS","MYSQL"],"title":"Redis Install Ubuntu","type":"post"},{"authors":null,"categories":[],"content":" [mysqld] server-id = 2 relay-log-index = slave-relay-bin.index relay-log = slave-relay-bin  mysql\u0026gt;CHANGE MASTER TO MASTER_HOST = 'db2',MASTER_PORT = 3306, MASTER_USER = 'repl_user', MASTER_PASSWORD = 'xyzzy';  Connecting the Master mysql\u0026gt; START SLAVE;  ","date":1367053687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367053687,"objectID":"1a359b933d9a8ed2d9167b5509a43eb6","permalink":"https://wubigo.com/post/mysql-slave/","publishdate":"2013-04-27T17:08:07+08:00","relpermalink":"/post/mysql-slave/","section":"post","summary":" [mysqld] server-id = 2 relay-log-index = slave-relay-bin.index relay-log = slave-relay-bin  mysql\u0026gt;CHANGE MASTER TO MASTER_HOST = 'db2',MASTER_PORT = 3306, MASTER_USER = 'repl_user', MASTER_PASSWORD = 'xyzzy';  Connecting the Master mysql\u0026gt; START SLAVE;  ","tags":["SHELL","MYSQL"],"title":"Mysql Slave","type":"post"},{"authors":null,"categories":[],"content":" Server and Operating System  Kernel – vm.swappiness  Disables swapping completely while 1 causes the kernel to perform the minimum amount of swapping\n# Set the swappiness value as root echo 1 \u0026gt; /proc/sys/vm/swappiness # Alternatively, using sysctl sysctl -w vm.swappiness=1 # Verify the change cat /proc/sys/vm/swappiness 1 # Alternatively, using sysctl sysctl vm.swappiness vm.swappiness = 1  Filesystems – XFS/ext4/ZFS     FILE SIZE mount option     EXT4 16TB noatime,data=writeback,barrier=0,nobh,errors=remount-ro   XFS 8EiB defaults,nobarrier    Disk Subsystem – I/O scheduler Most modern Linux distributions come with noop or deadline I/O schedulers by default, both providing better performance than the cfq and anticipatory ones\n# View the I/O scheduler setting. The value in square brackets shows the running scheduler cat /sys/block/sdb/queue/scheduler noop deadline [cfq] # Change the setting sudo echo noop \u0026gt; /sys/block/sdb/queue/scheduler  Disk Subsystem – Volume optimization separation of OS and data partitions, not just logically but physically, will improve database performance. The RAID level can also have an impact: RAID-5 should be avoided as the checksum needed to ensure integrity is costly\nSystem Architecture – NUMA settings the innodb_numa_interleave option to be available, MySQL must be compiled on a NUMA-enabled Linux system\n","date":1367053687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367053687,"objectID":"e8ab0b0043c9320b772863ab5fb384c2","permalink":"https://wubigo.com/post/mysql-tuning-on-os/","publishdate":"2013-04-27T17:08:07+08:00","relpermalink":"/post/mysql-tuning-on-os/","section":"post","summary":"Server and Operating System  Kernel – vm.swappiness  Disables swapping completely while 1 causes the kernel to perform the minimum amount of swapping\n# Set the swappiness value as root echo 1 \u0026gt; /proc/sys/vm/swappiness # Alternatively, using sysctl sysctl -w vm.swappiness=1 # Verify the change cat /proc/sys/vm/swappiness 1 # Alternatively, using sysctl sysctl vm.swappiness vm.swappiness = 1  Filesystems – XFS/ext4/ZFS     FILE SIZE mount option     EXT4 16TB noatime,data=writeback,barrier=0,nobh,errors=remount-ro   XFS 8EiB defaults,nobarrier    Disk Subsystem – I/O scheduler Most modern Linux distributions come with noop or deadline I/O schedulers by default, both providing better performance than the cfq and anticipatory ones","tags":["SHELL","MYSQL"],"title":"Mysql Tuning on OS","type":"post"},{"authors":null,"categories":[],"content":" index buffer Depends on Storage Engine\nMyISAM (Caches Index Pages From .MYI files)\nSELECT FLOOR(SUM(index_length)/POWER(1024,2)) IndexSizesMB FROM information_schema.tables WHERE engine='MyISAM' AND table_schema NOT IN ('information_schema','performance_schema','mysql');  Subtract that from key_buffer_size.\nInnoDB (Caches Data and Index Pages)\nSELECT FLOOR(SUM(data_length+index_length)/POWER(1024,2)) InnoDBSizeMB FROM information_schema.tables WHERE engine='InnoDB';  Subtract that from innodb_buffer_pool_size.\nconvert all tables from InnoDB into MyISAM SET @DATABASE_NAME = 'guowang'; SELECT CONCAT('ALTER TABLE `', table_name, '` ENGINE=MyISAM;') AS sql_statements FROM information_schema.tables AS tb WHERE table_schema = @DATABASE_NAME AND `ENGINE` = 'InnoDB' AND `TABLE_TYPE` = 'BASE TABLE' ORDER BY table_name DESC;   mysql\u0026gt;show full processlist; mysql\u0026gt;Select concat('KILL ',id,';') from information_schema.processlist where user='root';  SHOW ENGINE INNODB STATUS  a method to find the best prefix length for a given column SELECT ROUND(SUM(LENGTH(`sno`)\u0026lt;10)*100/COUNT(`sno`),2) AS pct_length_10, ROUND(SUM(LENGTH(`sno`)\u0026lt;20)*100/COUNT(`sno`),2) AS pct_length_20, ROUND(SUM(LENGTH(`sno`)\u0026lt;50)*100/COUNT(`sno`),2) AS pct_length_50, ROUND(SUM(LENGTH(`sno`)\u0026lt;100)*100/COUNT(`sno`),2) AS pct_length_100 FROM `bs`;  show to file mysql --table -e \u0026quot;sELECT nbiot_create_time ,metric , totalTime FROM v_sel_g550 g WHERE g.nbiot_create_time \u0026gt; '2019-04-04'\u0026quot; -u root -p123456 guowang \u0026gt; guowang-status.txt  sELECT nbiot_create_time ,metric , totalTime FROM v_sel_g550 g\nWHERE g.nbiot_create_time \u0026gt; \u0026lsquo;2019-04-04\u0026rsquo;\nCREATE TABLE ` v_sel_g550` ( `id` int(32) NOT NULL AUTO_INCREMENT, `nbiot_create_time` timestamp, `metric` varchar(50), `totalTime` int, PRIMARY KEY (`id`) USING BTREE, ) ENGINE = InnoDB  ","date":1366621687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366621687,"objectID":"b5e999882747164a6eba2b96a8d6e5cc","permalink":"https://wubigo.com/post/mysql-5.7-innodb/","publishdate":"2013-04-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-innodb/","section":"post","summary":"index buffer Depends on Storage Engine\nMyISAM (Caches Index Pages From .MYI files)\nSELECT FLOOR(SUM(index_length)/POWER(1024,2)) IndexSizesMB FROM information_schema.tables WHERE engine='MyISAM' AND table_schema NOT IN ('information_schema','performance_schema','mysql');  Subtract that from key_buffer_size.\nInnoDB (Caches Data and Index Pages)\nSELECT FLOOR(SUM(data_length+index_length)/POWER(1024,2)) InnoDBSizeMB FROM information_schema.tables WHERE engine='InnoDB';  Subtract that from innodb_buffer_pool_size.\nconvert all tables from InnoDB into MyISAM SET @DATABASE_NAME = 'guowang'; SELECT CONCAT('ALTER TABLE `', table_name, '` ENGINE=MyISAM;') AS sql_statements FROM information_schema.","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 InnoDB","type":"post"},{"authors":null,"categories":[],"content":" mysql install ubuntu 16.04 install mysql 5.7 at default\nsudo apt-get update sudo apt-get install mysql-server   Enable root remote connection  mysql -u root -p mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '\u0026lt;password\u0026gt;' WITH GRANT OPTION; mysql\u0026gt;FLUSH PRIVILEGES;  SHOW current setting mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'innodb%';  MySQL 5.7 has significantly better default values. the following variables are set by default:\n[mysqld] innodb_buffer_pool_instances=8 innodb_flush_method=O_DIRECT  setting of mysql 5.7 Use utf8mb4 to fully support Unicode [client] default-character-set=utf8mb4 [mysql] default-character-set=utf8mb4 [mysqld] character-set-client-handshake = FALSE character-set-server = utf8mb4 collation-server = utf8mb4_unicode_ci  mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'character\\_set\\_%' OR Variable_name LIKE 'collation%'; +--------------------------+--------------------+ | Variable_name | Value | +--------------------------+--------------------+ | character_set_client | utf8mb4 | | character_set_connection | utf8mb4 | | character_set_database | utf8mb4 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | | character_set_server | utf8mb4 | | character_set_system | utf8 | | collation_connection | utf8mb4_unicode_ci | | collation_database | utf8mb4_unicode_ci | | collation_server | utf8mb4_unicode_ci | +--------------------------+--------------------+  Repair and optimize all tables to ensure there is no side effect.\nmysqlcheck -u root -p --auto-repair --optimize --all-databases  MySQL 5.7 Performance Tuning after installation  Basic settings  innodb_buffer_pool_size: this is the #1 setting to look at for any installation using InnoDB. The buffer pool is where data and indexes are cached: having it as large as possible will ensure you use memory and not disks for most read operations. Typical values are 5-6GB (8GB RAM), 20-25GB (32GB RAM), 100-120GB (128GB RAM).\ninnodb_log_file_size: this is the size of the redo logs. The redo logs are used to make sure writes are fast and durable and also during crash recovery. Up to MySQL 5.1, it was hard to adjust, as you wanted both large redo logs for good performance and small redo logs for fast crash recovery. Fortunately crash recovery performance has improved a lot since MySQL 5.5 so you can now have good write performance and fast crash recovery. Until MySQL 5.5 the total redo log size was limited to 4GB (the default is to have 2 log files). This has been lifted in MySQL 5.6.\nStarting with innodb_log_file_size = 512M (giving 1GB of redo logs) should give you plenty of room for writes. If you know your application is write-intensive and you are using MySQL 5.6, you can start with innodb_log_file_size = 4G.\nmax_connections: if you are often facing the ‘Too many connections’ error, max_connections is too low. It is very frequent that because the application does not close connections to the database correctly, you need much more than the default 151 connections. The main drawback of high values for max_connections (like 1000 or more) is that the server will become unresponsive if for any reason it has to run 1000 or more active transactions. Using a connection pool at the application level or a thread pool at the MySQL level can help here.\nhttps://www.percona.com/blog/2016/10/12/mysql-5-7-performance-tuning-immediately-after-installation/\n[mysqld] # other variables here innodb_buffer_pool_size = 1G # (adjust value here, 50%-70% of total RAM) innodb_log_file_size = 256M innodb_flush_log_at_trx_commit = 1 # may change to 2 or 0 innodb_flush_method = O_DIRECT     Variable Value     innodb_buffer_pool_size Start with 50% 70% of total RAM. Does not need to be larger than the database size   innodb_flush_log_at_trx_commit 1 (Default) 0/2 (more performance, less reliability)   innodb_log_file_size 128M – 2G (does not need to be larger than buffer pool)   innodb_flush_method O_DIRECT (avoid double buffering)    # InnoDB settings\n innodb_file_per_table  With MySQL 5.6, the default value is ON so you have nothing to do in most cases\n innodb_flush_log_at_trx_commit  the default setting of 1 means that InnoDB is fully ACID compliant. It is the best value when your primary concern is data safety, for instance on a master. However it can have a significant overhead on systems with slow disks because of the extra fsyncs that are needed to flush each change to the redo logs. Setting it to 2 is a bit less reliable because committed transactions will be flushed to the redo logs only once a second, but that can be acceptable on some situations for a master and that is definitely a good value for a replica. 0 is even faster but you are more likely to lose some data in case of a crash: it is only a good value for a replica.\n innodb_flush_method  this setting controls how data and logs are flushed to disk. Popular values are O_DIRECT when you have a hardware RAID controller with a battery-protected write-back cache and fdatasync (default value) for most other scenarios. sysbench is a good tool to help you choose between the 2 values.\n innodb_log_buffer_size  this is the size of the buffer for transactions that have not been committed yet. The default value (1MB) is usually fine but as soon as you have transactions with large blob/text fields, the buffer can fill up very quickly and trigger extra I/O load. Look at the Innodb_log_waits status variable and if it is not 0, increase innodb_log_buffer_size.\n# other setting\n query_cache_type  disables the query cache, as does setting query_cache_type=0. By default, the query cache is disabled.\n query_cache_size  the query cache is a well known bottleneck that can be seen even when concurrency is moderate. The best option is to disable it from day 1 by setting query_cache_size = 0 (now the default on MySQL 5.6) and to use other ways to speed up read queries: good indexing, adding replicas to spread the read load or using an external cache (memcache or redis for instance). If you have already built your MySQL application with the query cache enabled and if you have never noticed any problem, the query cache may be beneficial for you. So you should be cautious if you decide to disable it.\n log_bin  enabling binary logging is mandatory if you want the server to act as a replication master. If so, don’t forget to also set server_id to a unique value. It is also useful for a single server when you want to be able to do point-in-time recovery: restore your latest backup and apply the binary logs. Once created, binary log files are kept forever. So if you do not want to run out of disk space, you should either purge old files with PURGE BINARY LOGS or set expire_logs_days to specify after how many days the logs will be automatically purged.\nBinary logging however is not free, so if you do not need for instance on a replica that is not a master, it is recommended to keep it disabled.\n skip_name_resolve  when a client connects, the server will perform hostname resolution, and when DNS is slow, establishing the connection will become slow as well. It is therefore recommended to start the server with skip-name-resolve to disable all DNS lookups. The only limitation is that the GRANT statements must then use IP addresses only, so be careful when adding this setting to an existing system.\n","date":1366621687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366621687,"objectID":"bbc91ecfdca080a9a81e17e261103bb5","permalink":"https://wubigo.com/post/mysql5.7/","publishdate":"2013-04-22T17:08:07+08:00","relpermalink":"/post/mysql5.7/","section":"post","summary":"mysql install ubuntu 16.04 install mysql 5.7 at default\nsudo apt-get update sudo apt-get install mysql-server   Enable root remote connection  mysql -u root -p mysql\u0026gt;GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '\u0026lt;password\u0026gt;' WITH GRANT OPTION; mysql\u0026gt;FLUSH PRIVILEGES;  SHOW current setting mysql\u0026gt; SHOW VARIABLES WHERE Variable_name LIKE 'innodb%';  MySQL 5.7 has significantly better default values. the following variables are set by default:\n[mysqld] innodb_buffer_pool_instances=8 innodb_flush_method=O_DIRECT  setting of mysql 5.","tags":["SHELL","MYSQL"],"title":"Mysql5.7","type":"post"},{"authors":null,"categories":[],"content":" whether MySQL Server supports partitioning  mysql -u root -p123456 -e \u0026quot;SHOW PLUGINS;\u0026quot; |grep partition  ","date":1363943287,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363943287,"objectID":"1d747e6830189cdd8c820f74ba9cae89","permalink":"https://wubigo.com/post/mysql-partition/","publishdate":"2013-03-22T17:08:07+08:00","relpermalink":"/post/mysql-partition/","section":"post","summary":" whether MySQL Server supports partitioning  mysql -u root -p123456 -e \u0026quot;SHOW PLUGINS;\u0026quot; |grep partition  ","tags":["SHELL","MYSQL"],"title":"Mysql Partition","type":"post"},{"authors":null,"categories":[],"content":" Disable ONLY_FULL_GROUP_BY SHOW VARIABLES WHERE Variable_name LIKE 'sql_mode'; +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |  [mysqld] sql_mode = \u0026quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026quot;  mysql\u0026gt; SHOW CREATE TABLE \u0026lt;tablename\u0026gt;;  ","date":1361524087,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1361524087,"objectID":"8dea05fe1a8d04f747d2a60060f2edf7","permalink":"https://wubigo.com/post/mysql-5.7-sql-mode/","publishdate":"2013-02-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-sql-mode/","section":"post","summary":" Disable ONLY_FULL_GROUP_BY SHOW VARIABLES WHERE Variable_name LIKE 'sql_mode'; +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +---------------+-------------------------------------------------------------------------------------------------------------------------------------------+ | sql_mode | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |  [mysqld] sql_mode = \u0026quot;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\u0026quot;  mysql\u0026gt; SHOW CREATE TABLE \u0026lt;tablename\u0026gt;;  ","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 SQL MODE","type":"post"},{"authors":null,"categories":[],"content":"[mysqld] log-bin = master-bin log-bin-index = master-bin.index server-id = 1  Grant the user to retrieve the binary log from the master\nmysql\u0026gt;CREATE USER repl_user; GRANT REPLICATION SLAVE ON *.* TO repl_user IDENTIFIED BY 'xyzzy';  ","date":1358845687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358845687,"objectID":"6b27567c18df15ccca99eb4fd341797b","permalink":"https://wubigo.com/post/mysql-master/","publishdate":"2013-01-22T17:08:07+08:00","relpermalink":"/post/mysql-master/","section":"post","summary":"[mysqld] log-bin = master-bin log-bin-index = master-bin.index server-id = 1  Grant the user to retrieve the binary log from the master\nmysql\u0026gt;CREATE USER repl_user; GRANT REPLICATION SLAVE ON *.* TO repl_user IDENTIFIED BY 'xyzzy';  ","tags":["SHELL","MYSQL"],"title":"Mysql Master","type":"post"},{"authors":null,"categories":null,"content":" API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs\n","date":1357171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357171200,"objectID":"b51bc744bcd6ae992033af60c36f65b3","permalink":"https://wubigo.com/post/2013-01-03-api_design/","publishdate":"2013-01-03T00:00:00Z","relpermalink":"/post/2013-01-03-api_design/","section":"post","summary":"API design: Choosing between names and identifiers in URLs API design: Choosing between names and identifiers in URLs","tags":null,"title":"API_design Note","type":"post"},{"authors":null,"categories":null,"content":" mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others. Having to wait until all of the preceding job’s tasks have completed slows down the execution of the workflow as a whole. * Mappers are often redundant: they just read back the same file that was just written by a reducer, and prepare it for the next stage of partitioning and sorting. In many cases, the mapper code could be part of the previous reducer: if the reducer output was partitioned and sorted in the same way as mapper output, then reducers could be chained together directly, without interleaving with mapper stages. * “Storing intermediate state in a distributed filesystem means those files are replicated across several nodes, which is often overkill for such temporary data”\nDevelop Apache Spark Apps with IntelliJ IDEA on Windows OS https://www.linkedin.com/pulse/develop-apache-spark-apps-intellij-idea-windows-os-samuel-yee\nSpark notes http://stackoverflow.com/questions/40796818/how-to-append-a-resource-jar-for-spark-submit Set SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the console, e.g. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja\u0026hellip; Refer to Print Launch Command of Spark Scripts (or org.apache.spark.launcher.Main Standalone Application where this environment variable is actually used). Tip Avoid using scala.App trait for a Spark app docker run -v pwd:/data -e SPARK_PRINT_LAUNCH_COMMAND=1 -it heuermh/adam adam-shell Avoid using scala.App trait for a Spark application’s main class in Scala as\nreported in SPARK-4170 Closure problems when running Scala app that \u0026ldquo;extends\nApp\u0026rdquo;.\nRefer to Executing Main — runMain internal method in this document.\nMake sure to use the same version of Scala as the one used to build your distribution of Spark. Pre-built distributions of Spark 1.x use Scala 2.10, while pre-built distributions of Spark 2.0.x use Scala 2.11. 10 down vote\nSteps to install Spark(1.6.2-bin-hadoop2.6)prebuild in local mode on windows:\nInstall Java 7 or later. To test java installation is complete, open command prompt type javaand hit enter. If you receive a message \u0026lsquo;Java\u0026rsquo; is not recognized as an internal or external command. You need to configure your environment variables, JAVA_HOME and PATHto point to the path of jdk.\nDownload and install Scala.\nSet SCALA_HOME in Control Panel\\System and Security\\System goto \u0026ldquo;Adv System settings\u0026rdquo; and add %SCALA_HOME%\\bin in PATH variable in environment variables.\nInstall Python 2.6 or later from Python Download link.\nDownload SBT. Install it and set SBT_HOME as an environment variable with value as \u0026lt;\u0026gt;. Download winutils.exe from HortonWorks repo or git repo. Since we don\u0026rsquo;t have a local Hadoop installation on Windows we have to download winutils.exe and place it in a bindirectory under a created Hadoop home directory. Set HADOOP_HOME = \u0026lt;\u0026gt; in environment variable.and add it to path env We will be using a pre-built Spark package, so choose a Spark pre-built package for Hadoop Spark download. Download and extract it.\nSet SPARK_HOME and add %SPARK_HOME%\\bin in PATH variable in environment variables.\nRun command: spark-shell\nOpen http://localhost:4040/ in a browser to see the SparkContext web UI.\n$ cat rdd1.txt chr1 10016 chr1 10017 chr1 10018 chr1 20026 scala\u0026gt; val lines = sc.textFile(\u0026ldquo;/data/rdd1.txt\u0026rdquo;)\nscala\u0026gt; case class Chrom(name: String, value: Long)\ndefined class Chrom\nscala\u0026gt; val chroms = lines.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom(r(0), r(1).toLong))\nchroms: org.apache.spark.rdd.RDD[Chrom] = MapPartitionsRDD[5] at map at :28\nscala\u0026gt; val df = chroms.toDF\n16/10/28 16:17:42 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n16/10/28 16:17:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\ndf: org.apache.spark.sql.DataFrame = [name: string, value: bigint]\nscala\u0026gt; df.show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|10016|\n|chr1|10017|\n|chr1|10018|\n|chr1|20026|\n|chr1|20036|\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; df.filter(\u0026lsquo;value \u0026gt; 30000).show\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|name|value|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\n|chr1|30016|\n|chr1|30026|\n|chr2|40016|\n|chr2|40116|\n|chr2|50016|\n|chr3|70016|\n+\u0026mdash;-+\u0026mdash;\u0026ndash;+\nscala\u0026gt; case class Chrom2(name: String, value: Long, value: Long)\nscala\u0026gt; val chroms2 = rdd2.map(_.split(\u0026rdquo;\\s+\u0026ldquo;)).map(r =\u0026gt; Chrom2(r(0), r(1).toLong, r(2).toLong))\nchroms2: org.apache.spark.rdd.RDD[Chrom2] = MapPartitionsRDD[35] at map at :28\nscala\u0026gt; val df2=chroms2.toDF\ndf2: org.apache.spark.sql.DataFrame = [name: string, min: bigint \u0026hellip; 1 more field]\nscala\u0026gt; df.join(df2, Seq(\u0026ldquo;name\u0026rdquo;)).where($\u0026ldquo;value\u0026rdquo;.between($\u0026ldquo;min\u0026rdquo;, $\u0026ldquo;max\u0026rdquo;)).groupBy($\u0026ldquo;name\u0026rdquo;).count().show()\n$./bin/spark-shell \u0026ndash;packages com.databricks:spark-csv_2.11:1.2\n.0\nYour csv file does not have the same number of fields in each row - this cannot be parsed as is into a DataFrame\nAs of Spark 2.0.0, DataFrame - the flagship data abstraction of previous versions of Spark SQL - is currently a mere type alias for Dataset[Row] :\nA Dataset is local if it was created from local collections using SparkSession.emptyDataset or SparkSession.createDataset methods and their derivatives like toDF. If so, the queries on the Dataset can be optimized and run locally, i.e. without using Spark executors.\n","date":1349308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349308800,"objectID":"1ca6d42c66a5123f3068b4aef27a8f87","permalink":"https://wubigo.com/post/2012-10-04-big-data-notes/","publishdate":"2012-10-04T00:00:00Z","relpermalink":"/post/2012-10-04-big-data-notes/","section":"post","summary":"mapr downside with intermediate state “MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes: * A MapReduce job can only start when all tasks in the preceding jobs (that generate”“its inputs) have completed, whereas processes connected by a Unix pipe are started at the same time, with output being consumed as soon as it is produced. Skew or varying load on different machines means that a job often has a few straggler tasks that take much longer to complete than the others.","tags":null,"title":"Develop Apache Spark Apps with IntelliJ IDEA on Windows OS","type":"post"},{"authors":null,"categories":null,"content":" update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.7.11/bin/python python2.7.11\n$. ./python2.7.11/bin/activate $sudo pip install PyYAML Jinja2 httplib2 six Setting up Ansible to run out of checkout(~/.bashrc) export PATH=/home/ubuntu/ansible/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games export export PYTHONPATH=/home/ubuntu/ansible/lib: sudo PYTHONPATH=/home/ubuntu/ansible/lib ansible\nor /etc/sudoers\nDefaults env_reset\nDefaults env_keep += \u0026ldquo;PYTHONPATH\u0026rdquo;\nansible-galaxy(ansible \u0026gt; 2 otherwise of proxy bug) use role installed from galaxy $ansible-galaxy install angstwad.docker_ubuntu\n#create a play book to use the role\n hosts: all  sudo: yes\nroles:\n- angstwad.docker_ubuntu  #test to run the book\n$ansible-playbook t1.yml\nBuilding Role Scaffolding $ansible-galaxy init rolename\nBest Practices: Create a dedicated ansible node on the local net in the cloud to play book copy directory to remote node using scp dir remote:~/dir or using synchronize module instead of copy\nfacts about any system $ansible -m setup | less\n#facter comes as part of extra modules. to use the facter module, the \u0026ldquo;facter\u0026rdquo; and #\u0026ldquo;ruby-json\u0026rdquo; packages preinstalled on the target host.\n$ansible  -m facter | less\nvariables The following are the places from where Ansible accepts variables:\nThe default directory inside a role Inventory variables The host_vars and group_vars parameters defined in separate directories The host/group vars parameter defined in an inventory file Variables in playbooks and role parameters The vars directory inside a role and variables defined inside a play Extra variables provided with the -e option at runtime\nPatterns\nPatterns in Ansible are how we decide which hosts to manage.The following patterns address one or more groups. Groups separated by a colon indicate an “OR” configuration. This means the host may be in either one group or the other: hosts: name1:name2:group1:group2\nDisable SSH Host Key Checking For All Hosts set these options permanently in ~/.ssh/config (for the current user) or in /etc/ssh/ssh_config (for all users), either for all hosts or for a given set of IP addresses\nHost * StrictHostKeyChecking no UserKnownHostsFile=/dev/null\nAnsible looks for an ansible.cfg file in the following places, in this order:\nFile specified by the ANSIBLE_CONFIG environment variable\n./ansible.cfg (ansible.cfg in the current directory)\n~/.ansible.cfg (.ansible.cfg in your home directory)\n/etc/ansible/ansible.cfg\nAnsible will then move to the next task in the list, and go through these same four steps. It’s important to note that:\nAnsible runs each task in parallel across all hosts.\nAnsible waits until all hosts have completed a task before moving to the next task.\nAnsible runs the tasks in the order that you specify them.\nAnsible supports the ssh-agent program, so you don’t need to explicitly specify SSH key files in your inventory files. See “SSH Agent” for more details if you haven’t used ssh-agent before.\nYAML to get started with your first playbook:\nThe first line of a playbook should begin with \u0026ldquo;\u0026mdash; \u0026rdquo; (three hyphens) which indicates the beginning of the YAML document. Lists in YAML are represented with a hyphen followed by a white space. A playbook contains a list of plays; they are represented with \u0026ldquo;- \u0026ldquo;. Each play is an associative array, a dictionary, or a map in terms of key-value pairs. Indentations are important. All members of a list should be at the same indentation level. Each play can contain key-value pairs separated by \u0026ldquo;:\u0026rdquo; to denote hosts, variables, roles, tasks, and so on.\nrole dependencies pecify role dependency inside the meta subdirectory\nSafely limiting Ansible playbooks to a single machine There\u0026rsquo;s also a cute little trick that lets you specify a single host on the command line (or multiple hosts, I guess), without an intermediary inventory:\nansible-playbook -i \u0026ldquo;imac1-local,\u0026rdquo; user.yml Note the comma (,) at the end; this signals that it\u0026rsquo;s a list, not a file\n","date":1344038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1344038400,"objectID":"8d897195aec3cc72f94440472e18d4ed","permalink":"https://wubigo.com/post/2012-08-04-ansible/","publishdate":"2012-08-04T00:00:00Z","relpermalink":"/post/2012-08-04-ansible/","section":"post","summary":"update python to 2.7.11 on ubuntu apt_repository need python \u0026gt;2.7.9 ansible_galaxy has bug on ansible1.9.4 on proxy https://launchpad.net/~fkrull/+archive/ubuntu/deadsnakes-python2.7\n$sudo -E apt-add-repository ppa:fkrull/deadsnakes-python2.7 (deb http://ppa.launchpad.net/fkrull/deadsnakes-python2.7/ubuntu trusty main) $sudo -E apt-get install --only-upgrade python2.7 $python --version Python 2.7.11  Install $ sudo -E apt-get install software-properties-common $ sudo -E apt-add-repository ppa:ansible/ansible $ sudo -E apt-get update $ sudo -E apt-get install \u0026ndash;only-upgrade ansible Install latest ansible $git clone git://github.com/ansible/ansible.git \u0026ndash;recursive\n$virtualenv -p /home/whg/python2.","tags":null,"title":"ansible note","type":"post"},{"authors":null,"categories":null,"content":" B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树\n","date":1341360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341360000,"objectID":"8dd0cb0fdbf378cdcc8265da15f2651c","permalink":"https://wubigo.com/post/2012-07-04-rdbms-nosql/","publishdate":"2012-07-04T00:00:00Z","relpermalink":"/post/2012-07-04-rdbms-nosql/","section":"post","summary":"B+树 vs. LSM树 RDBMS使用B+树专门针对磁盘存储而优化的N叉排序树 NoSQL使用LSM树","tags":["RDBMS","NOSQL"],"title":"RDBMS vs NoSQL","type":"post"},{"authors":null,"categories":null,"content":" showing current configuration  mysql\u0026gt;SHOW VARIABLES;   mysqldump -u root -h 192.168.76.62 -pgld --all-databases \u0026gt; dump.sql mysqldump -u root -h db -pgld --all-databases \u0026gt; dump.sql  then import data in mysql shell\nmysql\u0026gt;source dump.sql\nupdate the database if directly upgrade from 5.0 to 5.5\nmysql_upgrade -u root -p\nservice mysql restart\nA typical mysqldump command to move data from an external database to an Amazon RDS DB instance looks similar to the following:\nmysqldump -u  \u0026ndash;databases  \u0026ndash;single-transaction \u0026ndash;compress \u0026ndash;order-by-primary -p | mysql -u  \u0026ndash;port= \u0026ndash;host= -p\n\u0026ndash;single-transaction – Use to ensure that all of the data loaded from the local database is consistent with a single point in time. If there are other processes changing the data while mysqldump is reading it, using this option helps maintain data integrity. \u0026ndash;compress – Use to reduce network bandwidth consumption by compressing the data from the local database before sending it to Amazon RDS. \u0026ndash;order-by-primary – Use to reduce load time by sorting each table\u0026rsquo;s data by its primary key.\nYou must create any stored procedures, triggers, functions, or events manually in your Amazon RDS database. If you have any of these objects in the database that you are copying, then exclude them when you run mysqldump by including the following arguments with your mysqldump command: \u0026ndash;routines=0 \u0026ndash;triggers=0 \u0026ndash;events=0.\nhow to remove mysql completely\nsudo apt-get remove \u0026ndash;purge mysql-server mysql-client mysql-common sudo apt-get autoremove sudo apt-get autoclean then try to install it again.\nsudo apt-get install mysql-server if you installing with dpkg command and if it show any dependency on other package then run command :\nsudo apt-get install -f\nWhat is disabled by default is remote root access. If you want to enable that, run this SQL command locally:\nGRANT ALL PRIVILEGES ON . TO \u0026lsquo;root\u0026rsquo;@\u0026lsquo;%\u0026rsquo; IDENTIFIED BY \u0026lsquo;\u0026rsquo; WITH GRANT OPTION; FLUSH PRIVILEGES;\nEnable remote connection mysql 5.7\nmysqld config /etc/mysql/mysql.conf.d mysql client config /etc/mysql/conf.d\nAnd then find the following line and comment it out in your my.cnf file, which usually lives on /etc/mysql/my.cnf on Unix/OSX systems.\nIf it\u0026rsquo;s a Windows system, you can find it in the MySQL installation directory, usually something like C:\\Program Files\\MySQL\\MySQL Server 5.5\\ and the filename will be my.ini.\nChange line\n bind-address = 127.0.0.1  to\n #bind-address = 127.0.0.1  And restart the MySQL server for the changes to take effect.\nsudo apt-get install mysql-server\nupdate t_supplier_subproject set attachInfo=\u0026ldquo;;commit;\nupdate t_supplier set email=\u0026lsquo;gin_369@163.Cnn\u0026rsquo; where supplierID=7;\nselect * from t_supplier where userid=\u0026lsquo;6156354693465407499\u0026rsquo; and email=\u0026lsquo;gin_369@163.COM\u0026rsquo;;\nALTER TABLE t_quoted_adopt4tbq MODIFY adoptRemark VARCHAR(255);\n查询分包商数量 SELECT count(*) FROM etender.t_supplier;\nALTER TABLE etender.t_supplier CHANGE COLUMN email email VARCHAR(40) NULL DEFAULT \u0026ldquo; COMMENT \u0026lsquo;供应商邮箱\u0026rsquo;;\n新增列 询价类型 alter table t_project_query add column inquiryType varchar(255) DEFAULT \u0026lsquo;2\u0026rsquo; COMMENT \u0026lsquo;询价类型\u0026rsquo;;\nalter table t_quoted_billitem4tbq add column itemType varchar(255) DEFAULT \u0026lsquo;1\u0026rsquo; COMMENT \u0026lsquo;清单类型\u0026rsquo;;\nSELECT COUNT(*),t2.email AS \u0026lsquo;总包邮箱\u0026rsquo;, t1.userID FROM etender.t_user t2 LEFT JOIN etender.t_supplier t1 ON t1.userID = t2.userID WHERE t1.logicDelete !=1 GROUP BY t2.userID ;\nMaking a Copy of a Database $mysqldump -u root -pg1d etender \u0026gt; dump.sql $mysqladmin -u root -pg1d create hongq $mysql -u root -pg1d hongq \u0026lt; dump.sql  export to csv SELECT b.email,a.name supplierName,a.email supplierEmail,a.telephone,a.trade,a.level,a.address,a.contacts FROM t_supplier a LEFT JOIN t_user b ON a.userID = b.userid and a.logicDelete !=1 ORDER BY a.userID limit 1000,1000 INTO OUTFILE '/var/lib/mysql-files/subcon1000.csv' FIELDS TERMINATED BY ',' ENCLOSED BY '\u0026quot;' LINES TERMINATED BY '\\n'  Creating SSH Tunnel From Linux for mysql $ ssh -L 3306:rdb:3306 ubuntu@ec2\nuse ip instead of hostname to avoid channel X on ubuntu 16 \u0026ldquo;channel X: open failed: administratively prohibited\u0026rdquo;\n","date":1338768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338768000,"objectID":"1bc16a0874c5c39c07a78ba84a4b7ccd","permalink":"https://wubigo.com/post/2012-06-04-mysql/","publishdate":"2012-06-04T00:00:00Z","relpermalink":"/post/2012-06-04-mysql/","section":"post","summary":"showing current configuration  mysql\u0026gt;SHOW VARIABLES;   mysqldump -u root -h 192.168.76.62 -pgld --all-databases \u0026gt; dump.sql mysqldump -u root -h db -pgld --all-databases \u0026gt; dump.sql  then import data in mysql shell\nmysql\u0026gt;source dump.sql\nupdate the database if directly upgrade from 5.0 to 5.5\nmysql_upgrade -u root -p\nservice mysql restart\nA typical mysqldump command to move data from an external database to an Amazon RDS DB instance looks similar to the following:","tags":null,"title":"dbms note","type":"post"},{"authors":null,"categories":["IT"],"content":" iptables规则配置  表与链   调用链顺序   检查内存 ram speed and type\n dmidecode  sudo dmidecode --type memory # dmidecode 3.0 Getting SMBIOS data from sysfs. SMBIOS 2.6 present. Handle 0x003E, DMI type 17, 28 bytes Memory Device Array Handle: 0x003C Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: 1 Locator: XMM1 Bank Locator: Not Specified Type: DDR3 Type Detail: Synchronous Speed: Unknown Manufacturer: JEDEC ID: Serial Number: Asset Tag: Not Specified Part Number: Rank: Unknown   lshw   sudo lshw -class memory memory:0 description: System Memory physical id: 3c slot: System board or motherboard *-bank:0 description: DIMM DDR3 Synchronous [empty] vendor: JEDEC ID: physical id: 0 slot: XMM1 *-bank:1 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: M378B5273DH0-CH9 vendor: JEDEC ID:80 CE physical id: 1 serial: D3894765 slot: XMM2 size: 4GiB width: 64 bits clock: 1333MHz (0.8ns) *-bank:2 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: M378B5273DH0-CH9 vendor: JEDEC ID:80 CE physical id: 2 serial: D4894765 slot: XMM3 size: 4GiB width: 64 bits clock: 1333MHz (0.8ns) *-bank:3 description: DIMM DDR3 Synchronous 1333 MHz (0.8 ns) product: 8JTF12864AZ-1G4G1 vendor: JEDEC ID:80 2C physical id: 3 serial: 07572436 slot: XMM4 size: 1GiB width: 64 bits clock: 1333MHz (0.8ns)  2R*8 代表双面，每面（RANK）8个芯片颗粒\ndownload manager sudo apt-get install uget uget-gtk  Enables forwarding of the authentication agent connection  client config .ssh/config  ForwardAgent yes   Enable ssh-agent on main device  .bashrc\nSSH_ENV=\u0026quot;$HOME/.ssh/environment\u0026quot; function start_agent { echo \u0026quot;Initialising new SSH agent...\u0026quot; /usr/bin/ssh-agent | sed 's/^echo/#echo/' \u0026gt; \u0026quot;${SSH_ENV}\u0026quot; echo succeeded chmod 600 \u0026quot;${SSH_ENV}\u0026quot; . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null /usr/bin/ssh-add; } # Source SSH settings, if applicable if [ -f \u0026quot;${SSH_ENV}\u0026quot; ]; then . \u0026quot;${SSH_ENV}\u0026quot; \u0026gt; /dev/null #ps ${SSH_AGENT_PID} doesn't work under cywgin ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ \u0026gt; /dev/null || { start_agent; } else start_agent; fi  enter password to unlock your keyring 方法1 - set password-store to basic\ndpkg -L google-chrome-stable |grep desktop | xargs cp {1} ~/.local/share/applications  修改.local/share/applications/google-chrome.desktop\nExec=/usr/bin/google-chrome-stable --password-store=basic %U   seahorse  seahorse  选择login，右键删除\nTCP DUMP  capture all incoming IP traffic destined to the node except local traffic\n sudo tcpdump -i enp0s25 tcp -n sudo tcpdump -i enp0s25 dst host 192.168.1.5 and not src net 192.168.1.0/24  终端窗口复制快捷键Ctrl-C  在命令终端窗口首选项里设置快捷键复制 -\u0026gt; Ctrl-C 设置终端驱动快捷键  stty -a stty intr \\^k stty -a   Ctrl-K to interrupt current command\n ctrl-c.sh\n#!/usr/bin/env bash stty intr \\^k  network manager Linux systems which use a GUI often have a network manager running, which uses a dnsmasq instance running on a loopback address such as 127.0.0.1 or 127.0.1.1 to cache DNS requests, and adds this entry to /etc/resolv.conf. The dnsmasq service speeds up DNS look-ups and also provides DHCP services\n /usr/sbin/dnsmasq --no-resolv --keep-in-foreground --no-hosts --bind-interfaces --pid-file=/var/run/NetworkManager/dnsmasq.pid --listen-address=127.0.1.1 --cache-size=0 --conf-file=/dev/null --proxy-dnssec --enable-dbus=org.freedesktop.NetworkManager.dnsmasq --conf-dir=/etc/NetworkManager/dnsmasq.d  update git to 2.20 sudo add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install git=2.20.1-0ppa1~ubuntu16.04.1  static ip sudo systemctl stop network-manager sudo systemctl disable network-manager.service echo \u0026quot;manual\u0026quot; | sudo tee /etc/init/network-manager.override  ubuntu18.04\nsudo systemctl stop NetworkManager-wait-online.service sudo systemctl disable NetworkManager-wait-online.service sudo systemctl stop NetworkManager-dispatcher.service sudo systemctl disable NetworkManager-dispatcher.service sudo systemctl stop network-manager.service sudo systemctl disable network-manager.service ystemctl unmask networking systemctl enable networking systemctl restart networking  cat /etc/network/interfaces auto enp0s25 iface enp0s25 inet static address 192.168.1.5 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 192.168.1.1  ubuntu18.04 only\necho \u0026quot;DNS=192.168.1.1\u0026gt;\u0026gt;/etc/systemd/resolved.conf systemctl restart systemd-resolved  ssh client config ~/.ssh/config\nhost * StrictHostKeyChecking no  instll docker v17.03 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; sudo apt-get update apt-cache madison docker-ce|grep 17.03 sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial sudo usermod -aG docker $USER  Disable Chrome session restore popup Type chrome://flags/#infinite-session-restore in address bar (Crtl+L). Click on the right drop-down menu and change the \u0026lsquo;Default\u0026rsquo; value to \u0026lsquo;Disable\u0026rsquo;. Then restart Chrome to apply that setting\ndocker to reclaim disk space  remove untagged images  docker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi   Clean up dead and exited containers(use the -v flag to remove the volumes along the container)  docker ps --filter status=dead --filter status=exited -aq \\ | xargs docker rm -v   docker volume cleanup  docker volume ls -qf dangling=true | xargs -r docker volume rm  IPVS for i in ip_vs_sh ip_vs ip_vs_rr ip_vs_wrr; do sudo modprobe $i; done  change the runlevel on systemd for VM sudo systemctl enable multi-user.target sudo systemctl set-default multi-user.target  List files in package $dpkg -L docker-ce /usr/bin/docker-containerd /usr/bin/docker-proxy /usr/bin/docker /usr/bin/docker-runc /usr/bin/dockerd /usr/bin/docker-containerd-ctr /usr/bin/docker-containerd-shim /usr/bin/docker-init /etc/init.d/docker /etc/default/docker /etc/init/docker.conf /lib/systemd/system/docker.service /lib/systemd/system/docker.socket  Find the latest file by modified date find /path -printf '%T+ %p\\n' | sort -r | head  ghost systemd service /etc/systemd/system/ghost.service\nRunning sudo command: ln -sf /var/www/ghost/system/files/ghost_localhost.service /lib/systemd/system/ghost_localhost.service Running sudo command: systemctl daemon-reload  ls /lib/systemd/system/ghost* sudo systemctl stop ghost_localhost  Admin URL As per the SSL section above, admin.url can be used to specify a different protocol for your admin panel. It can also be used to specify a different hostname (domain name). It cannot be used to affect the path at which the admin panel is served (this is always /ghost/).\n\u0026quot;admin\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://example.com\u0026quot; }  ubuntu@ip-192-168-114-240:/lib/systemd/system$ sudo systemctl disable ghost_54-169-190-39.service Removed symlink /etc/systemd/system/multi-user.target.wants/ghost_54-169-190-39.service. Removed symlink /etc/systemd/system/ghost_54-169-190-39.service.  Rotate Tomcat catalina.out https://dzone.com/articles/how-rotate-tomcat-catalinaout\n","date":1338723543,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338723543,"objectID":"2d2de0feaad6fc1dd7970449aaf561cc","permalink":"https://wubigo.com/post/linux-notes/","publishdate":"2012-06-03T19:39:03+08:00","relpermalink":"/post/linux-notes/","section":"post","summary":"iptables规则配置  表与链   调用链顺序   检查内存 ram speed and type\n dmidecode  sudo dmidecode --type memory # dmidecode 3.0 Getting SMBIOS data from sysfs. SMBIOS 2.6 present. Handle 0x003E, DMI type 17, 28 bytes Memory Device Array Handle: 0x003C Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: 1 Locator: XMM1 Bank Locator: Not Specified Type: DDR3 Type Detail: Synchronous Speed: Unknown Manufacturer: JEDEC ID: Serial Number: Asset Tag: Not Specified Part Number: Rank: Unknown   lshw   sudo lshw -class memory memory:0 description: System Memory physical id: 3c slot: System board or motherboard *-bank:0 description: DIMM DDR3 Synchronous [empty] vendor: JEDEC ID: physical id: 0 slot: XMM1 *-bank:1 description: DIMM DDR3 Synchronous 1333 MHz (0.","tags":["IAAS","LINUX"],"title":"linux note","type":"post"},{"authors":null,"categories":null,"content":" Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this. For EC2 users, make sure you use HVM based modern EC2 instances, like m3.medium. Otherwise fork() is too slow. Transparent huge pages must be disabled from your kernel. Use echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled to disable them, and restart your Redis process. If you are using a virtual machine, it is possible that you have an intrinsic latency that has nothing to do with Redis. Check the minimum latency you can expect from your runtime environment using ./redis-cli \u0026ndash;intrinsic-latency 100. Note: you need to run this command in the server not in the client. Enable and use the Latency monitor feature of Redis in order to get a human readable description of the latency events and causes in your Redis instance.  stop-writes-on-bgsave-error(save RDB snapshots) MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.\n$ redis-cli \u0026gt; config set stop-writes-on-bgsave-error no  Delete all the keys of the currently selected DB 127.0.0.1:6379\u0026gt;FLUSHALL or flushall  select database the number of databases is defined in the configuration file with the databases directive (the default value is 16). To switch between the databases, call SELECT. 127.0.0.1:6379\u0026gt;select \nDataType  DataType type = redisTemplate.type(key); if(DataType.NONE == type){ return null; }else if(DataType.STRING == type){ return super.redisTemplate.opsForValue().get(key); }else if(DataType.LIST == type){ return super.redisTemplate.opsForList().range(key, 0, -1); }else if(DataType.HASH == type){ return super.redisTemplate.opsForHash().entries(key); }else return null;  allowing remote connection to redis echo \u0026ldquo;bind 0.0.0.0\u0026rdquo; \u0026gt;\u0026gt; redis.conf\n1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.  ","date":1336003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336003200,"objectID":"22964e47b193e9ca7e1d767cea8d9b9b","permalink":"https://wubigo.com/post/2012-05-03-redis-notes/","publishdate":"2012-05-03T00:00:00Z","relpermalink":"/post/2012-05-03-redis-notes/","section":"post","summary":"Redis latency problems troubleshooting  Make sure you are not running slow commands that are blocking the server. Use the Redis Slow Log feature to check this. For EC2 users, make sure you use HVM based modern EC2 instances, like m3.medium. Otherwise fork() is too slow. Transparent huge pages must be disabled from your kernel. Use echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled to disable them, and restart your Redis process. If you are using a virtual machine, it is possible that you have an intrinsic latency that has nothing to do with Redis.","tags":null,"title":"redis note","type":"post"},{"authors":null,"categories":[],"content":" key_buffer_size  the size of the index buffers held in memory, which affects the speed of index reads\nrecommend: 25% or more of the available server memory\nA good way to determine whether to adjust the value is to compare the key_read_requests value, which is the total value of requests to read an index, and the key_reads values, the total number of requests that had to be read from disk.\n","date":1335085687,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1335085687,"objectID":"cdc1b77fba72f3b0fddd83601d68cc60","permalink":"https://wubigo.com/post/mysql-5.7-myisam/","publishdate":"2012-04-22T17:08:07+08:00","relpermalink":"/post/mysql-5.7-myisam/","section":"post","summary":"key_buffer_size  the size of the index buffers held in memory, which affects the speed of index reads\nrecommend: 25% or more of the available server memory\nA good way to determine whether to adjust the value is to compare the key_read_requests value, which is the total value of requests to read an index, and the key_reads values, the total number of requests that had to be read from disk.","tags":["SHELL","MYSQL"],"title":"Mysql 5.7 MyISAM","type":"post"},{"authors":null,"categories":null,"content":" NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.\n generate keys and certificates for wildcard * address as well. They will work on any machine. It will simplify certificates routine but increase security risks.\n  check X509v3 Subject Alternative Name(HOST) issued in server.pem\nopenssl x509 -in server.pem -text |grep DNS  Generate self-signed certificates  Download cfssl\nmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson}  Initialize a certificate authority\nmkdir ~/cfssl cd ~/cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json  Certificate types which are used inside Container Linux\n client certificate is used to authenticate client by server. . server certificate is used by server and verified by client for server identity. peer certificate is used by cluster as members communicate with each other in both ways.  Configure CA options ca-csr.json(Certificate Signing Request (CSR))\n{ \u0026quot;CN\u0026quot;: \u0026quot;wubigo CA\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;ecdsa\u0026quot;, \u0026quot;size\u0026quot;: 256 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;CN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;BJ\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Bei Jing\u0026quot; } ] }   ca-config.json( set expiry to 8760h (1 year))\n{ \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;168h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;server\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot; ] }, \u0026quot;client\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;client auth\u0026quot; ] }, \u0026quot;peer\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } }   generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - ls ca-key.pem ca.csr ca.pem  Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within the CA\n Generate server certificate\ncfssl print-defaults csr \u0026gt; server.json  Most important values for server certificate are Common Name (CN) and hosts. substitute them accordingly:\n  ... \u0026quot;CN\u0026quot;: \u0026quot;server\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.6\u0026quot;, \u0026quot;wubigo.com\u0026quot;, \u0026quot;localhost\u0026quot;, \u0026quot;127.0.0.1\u0026quot; ], ...   generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  result following files:\nserver-key.pem server.csr server.pem  Generate peer certificate\n  cfssl print-defaults csr \u0026gt; member1.json\nSubstitute CN and hosts values, for example:\n... \u0026quot;CN\u0026quot;: \u0026quot;member1\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;192.168.1.7\u0026quot;, \u0026quot;member1.wubigo.com\u0026quot;, \u0026quot;member1.local\u0026quot;, \u0026quot;member1\u0026quot; ], ...   generate member1 certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1  member1-key.pem member1.csr member1.pem  Generate client certificate\ncfssl print-defaults csr \u0026gt; client.json  For client certificate ignore hosts values and set only Common Name (CN) to client value:\n... \u0026quot;CN\u0026quot;: \u0026quot;client\u0026quot;, \u0026quot;hosts\u0026quot;: [ \u0026quot;\u0026quot; ]\u0026quot;, ...  Generate client certificate:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client   get following files:\nclient-key.pem client.csr client.pem  ","date":1328054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1328054400,"objectID":"c75fad4581620ab0c699c16ce7e01a99","permalink":"https://wubigo.com/post/2012-02-01-tls-notes/","publishdate":"2012-02-01T00:00:00Z","relpermalink":"/post/2012-02-01-tls-notes/","section":"post","summary":"NOTICE\n Don\u0026rsquo;t put ca-key.pem into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n Keep key files in safe. Don\u0026rsquo;t forget to set proper file permissions, i.e. chmod 0600 server-key.pem.\n Certificates in this TLDR example have both server auth and client auth X509 V3 extensions and you can use them with servers and clients\u0026rsquo; authentication.","tags":null,"title":"TLS notes","type":"post"},{"authors":null,"categories":["IT"],"content":" dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance. But the main reason that this confusion happens is because, it does not matter whatever tool you use to test the file system\u0026rsquo;s performance, what matter\u0026rsquo;s is the exact requirement.File system\u0026rsquo;s performance depends upon certain factors as follows.\nThe maximum rotational speed of your hard disk\nThe Allocated block size of a file system\nSeek Time\nThe performance rate of the file system\u0026rsquo;s metadata\nThe type of read/Write\nSeriously speaking its wonderful to realize that various different technologies made by different people and even different companies are working together in coordination inside a single box, and we call that box a computer. And its even more wonderful to realize that hard disk\u0026rsquo;s store\u0026rsquo;s almost all the information available in the world in digital format. Its a very complex thing to understand how really hard disks stores our data safely. Explaining different aspects of how a hard disk, and a file system on top of it, work together is beyond the scope of this article(But i will surely give it a try with couple of my posts about themwink)\nSo Lets begin our tutorial on file system benchmark test.\nIts advised that during this file system performance test, you must not run any other disk I/O intensive tasks. Otherwise your results about performance will be heavily deviated. Its better to stop all other process during this test.\nThe Simplest Performance Test Using dd command\nThe simplest read write performance test in Linux can be done with the help of dd command. This command is used to write or read from any block device in Linux. And you can do a lot of stuff with this command. The main plus point with this command, is that its readily available in almost all distributions out of the box. And is pretty easy to use.\nWith this dd command we will only be testing sequential read and sequential write.I will test the speed of my partition /dev/sda1 which is mounted on \u0026ldquo;/\u0026rdquo; (the only partition i have on my system)so can write the data to any where in my filesystem to test.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.0897865 seconds, 1.2 GB/s  In the above command you will be amazed to see that you have got 1.1GB/s. But dont be happy thats falsecheeky. Becasue the speed that dd reported to us is the speed with which data was cached to RAM memory, not to the disk. So we need to ask dd command to report the speed only after the data is synced with the disk.For that we need to run the below command.\n[root@slashroot2 ~]# dd if=/dev/zero of=speetest bs=1M count=100 conv=fdatasync 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 2.05887 seconds, 50.9 MB/s  As you can clearly see that with the attribute fdatasync the dd command will show the status rate only after the data is completely written to the disk. So now we have the actual sequencial write speed. Lets go to an amount of data size thats larger than the RAM. Lets take 200MB of data in 64kb block size.\n[root@slashroot2 ~]# dd if=/dev/zero of=speedtest bs=64k count=3200 conv=fdatasync 3200+0 records in 3200+0 records out 209715200 bytes (210 MB) copied, 3.51895 seconds, 59.6 MB/s  as you can clearly see that the speed came to 59 MB/s. You need to note that ext3 bydefault if you do not specify the block size, gets formatted with a block size thats determined by the programes like mke2fs . You can verify yours with the following commands.\ntune2fs -l /dev/sda1 dumpe2fs /dev/sda1  For testing the sequential read speed with dd command, you need to run the below command as below.\n[root@myvm1 sarath]# dd if=speedtest of=/dev/null bs=64k count=24000 5200+0 records in 5200+0 records out 340787200 bytes (341 MB) copied, 3.42937 seconds, 99.4 MB/s  Performance Test using HDPARM\nNow lets use some other tool other than dd command for our tests. We will start with hdparm command to test the speed. Hdparm tool is also available out of the box in most of the linux distribution.\n[root@myvm1 ~]# hdparm -tT /dev/sda1 /dev/sda1: Timing cached reads: 5808 MB in 2.00 seconds = 2908.32 MB/sec Timing buffered disk reads: 10 MB in 3.12 seconds = 3.21 MB/sec  There are multiple things to understand here in the above hdparm results. the -t Option will show you the speed of reading from the cache buffer(Thats why its much much higher).\nThe -T option will show you the speed of reading without precached buffer(which from the above output is low 3.21 MB/sec as shown above. )\nthe hdparm output shows you both the cached reads and disk reads separately. As mentioned before hard disk seek time also matters a lot for your speed you can check your hard disk seek time with the following linux command. seek time is the time required by the hard disk to reach the sector where the data is stored.Now lets use this seeker tool to find out the seek time by the simple seek command.\n[root@slashroot2 ~]# seeker /dev/sda1 Seeker v3.0, 2009-06-17, http://www.linuxinsight.com/how_fast_is_your_disk.html Benchmarking /dev/sda1 [81915372 blocks, 41940670464 bytes, 39 GB, 39997 MB, 41 GiB, 41940 MiB] [512 logical sector size, 512 physical sector size] [1 threads] Wait 30 seconds.............................. Results: 87 seeks/second, 11.424 ms random access time (26606211 \u0026lt; offsets \u0026lt; 41937280284)  its clearly mentioned that my disk did a 86 seeks for sectors containing data per second. Thats ok for a desktop Linux machine but for servers its not at all ok.\nRead Write Benchmark Test using IOZONE:\nNow there is one tool out there in linux that will do all these test in one shot. Thats none other than \u0026ldquo;IOZONE\u0026rdquo;. We will do some benchmark test against my /dev/sda1 with the help of iozone.Computers or servers are always purchased keeping some purpose in mind. Some servers needs to be highend performance wise, some needs to be fast in sequencial reads,and some others are ordered keeping random reads in mind. IOZONE will be very much helpful in carrying out large number of permance benchmark test against the drives. The output produced by iozone is too much brief.\nThe default command line option -a is used for full automatic mode, in which iozone will test block sizes ranging from 4k to 16M and file sizes ranging from 64k to 512M. Lets do a test using this -a option and see what happens.\n[root@myvm1 ~]# iozone -a /dev/sda1 Auto Mode Command line used: iozone -a /dev/sda1 Output is in Kbytes/sec Time Resolution = 0.000001 seconds. Processor cache size set to 1024 Kbytes. Processor cache line size set to 32 bytes. File stride size set to 17 * record size. \u0026lt;div id=\u0026quot;xdvp\u0026quot;\u0026gt;\u0026lt;a href=\u0026quot;http://www.ecocertico.com/no-credit-check-direct-lenders\u0026amp;#10;\u0026quot;\u0026gt;creditors you never heard\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt; random random bkwd record stride KB reclen write rewrite read reread read write read rewrite read fwrite frewrite fread freread 64 4 172945 581241 1186518 1563640 877647 374157 484928 240642 985893 633901 652867 1017433 1450619 64 8 25549 345725 516034 2199541 1229452 338782 415666 470666 1393409 799055 753110 1335973 2071017 64 16 68231 810152 1887586 2559717 1562320 791144 1309119 222313 1421031 790115 538032 694760 2462048 64 32 338417 799198 1884189 2898148 1733988 864568 1421505 771741 1734912 1085107 1332240 1644921 2788472 64 64 31775 811096 1999576 3202752 1832347 385702 1421148 771134 1733146 864224 942626 2006627 3057595 128 4 269540 699126 1318194 1525916 390257 407760 790259 154585 649980 680625 684461 1254971 1487281 128 8 284495 837250 1941107 2289303 1420662 779975 825344 558859 1505947 815392 618235 969958 2130559 128 16 277078 482933 1112790 2559604 1505182 630556 1560617 624143 1880886 954878 962868 1682473 2464581 128 32 254925 646594 1999671 2845290 2100561 554291 1581773 723415 2095628 1057335 1049712 2061550 2850336 128 64 182344 871319 2412939 609440 2249929 941090 1827150 1007712 2249754 1113206 1578345 2132336 3052578 128 128 301873 595485 2788953 2555042 2131042 963078 762218 494164 1937294 564075 1016490 2067590 2559306  Note: All the output you see above are in KB/Sec\nThe first column shows the file size used and second column shows the length of the record used.\nLets understand the output in some of the columns\nThe third Column-Write:This column shows the speed Whenever a new file is made in any file system under Linux. There is more overhead involved in the metadata storing. For example the inode for the file, and its entry in the journal etc. So creating a new file in a file system is always comparatively slower than overwriting an already created file.\nFourth column-Re-writing:This shows the speed reported in overwriting the file which is already created\nFifth column-Read:This reports the speed of reading an already existing file.\nseq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties submit jobs/job.{}.cfg seq 1 100 | xargs -I {} java -jar bin/ossimport2.jar -c conf/sys.properties clean jobs/job.{}.cfg  http://www.slashroot.in/linux-file-system-read-write-performance-test\n","date":1307101143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1307101143,"objectID":"5bdf905d3f48e9e1e5da49eaf0148f30","permalink":"https://wubigo.com/post/2011-06-03-io-performance/","publishdate":"2011-06-03T19:39:03+08:00","relpermalink":"/post/2011-06-03-io-performance/","section":"post","summary":"dstat $dstat -d -nt $dstat -nt $dstat -N eth2,eth3  pkstat sudo apt-get install pktstat sudo pktstat -i eth0 -nt  nethogs sudo apt-get install nethogs sudo nethogs  EPEL http://www.cyberciti.biz/faq/fedora-sl-centos-redhat6-enable-epel-repo/\n$ cd /tmp $ wget http://mirror-fpt-telecom.fpt.net/fedora/epel/6/i386/epel-release-6-8.noarch.rpm # rpm -ivh epel-release-6-8.noarch.rpm  How do I use EPEL repo?\nSimply use the yum commands to search or install packages from EPEL repo:\n# yum search nethogs # yum update # yum --disablerepo=\u0026quot;*\u0026quot; --enablerepo=\u0026quot;epel\u0026quot; install nethogs  System administrators responsible for handling Linux servers get confused at times when they are told to benchmark a file system\u0026rsquo;s performance.","tags":["IAAS","LINUX"],"title":"Linux File System Read Write Performance Test","type":"post"},{"authors":null,"categories":null,"content":" http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them. Near the bottom of your config file you will see a section for HPN related options; I used the following options from other guides I found:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nthe following are HPN related configuration options tcp receive buffer polling. disable in non autotuning kernels TcpRcvBufPoll yes\nallow the use of the none cipher #NoneEnabled no\ndisable hpn performance boosts. #HPNDisabled no\nbuffer size for hpn to non-hpn connections HPNBufferSize 8388608\nLinux supports both /proc and sysctl (using alternate forms of the variable names - e.g. net.core.rmem_max) for inspecting and adjusting network tuning parameters. The following is a useful shortcut for inspecting all tcp parameters:\nsysctl -a | fgrep tcp\nFor additional information on kernel variables, look at the documentation included with your kernel source, typically in some location such as /usr/src/linux-/Documentation/networking/ip-sysctl.txt. There is a very good (but slightly out of date) tutorial on network sysctl\u0026rsquo;s at http://ipsysctl-tutorial.frozentux.net/ipsysctl-tutorial.html.\nIf you would like to have these changes to be preserved across reboots, you can add the tuning commands to your the file /etc/rc.d/rc.local .\necho 1 \u0026gt; /proc/sys/net/ipv4/tcp_moderate_rcvbuf echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/wmem_max echo \u0026ldquo;8388608\u0026rdquo;\u0026gt; /proc/sys/net/core/rmem_max echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_rmem echo \u0026ldquo;4096 87380 8388608\u0026rdquo; \u0026gt; /proc/sys/net/ipv4/tcp_wmem\noptimization start increase TCP max buffer size setable using setsockopt() net.ipv4.tcp_rmem = 4096 87380 8388608\nnet.ipv4.tcp_wmem = 4096 87380 8388608\nincrease Linux auto tuning TCP buffer limits min, default, and max number of bytes to use set max to at least 4MB, or higher if you use very high BDP paths net.core.rmem_max = 8388608\nnet.core.wmem_max = 8388608\nnet.core.netdev_max_backlog = 5000\nnet.ipv4.tcp_window_scaling = 1\noptimization end [1] http://www.psc.edu/index.php/hpn-ssh\n[2]http://stackoverflow.com/questions/8849240/why-when-i-transfer-a-file-through-sftp-it-takes-longer-than-ftp\n[3]http://www.cyberciti.biz/tips/sshd-server-optimization.html\n","date":1301788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1301788800,"objectID":"9d351edd435efc376437776ae226dfa4","permalink":"https://wubigo.com/post/2011-04-03-high-performance-ssh/","publishdate":"2011-04-03T00:00:00Z","relpermalink":"/post/2011-04-03-high-performance-ssh/","section":"post","summary":"http://mirror.internode.on.net/pub/OpenBSD/OpenSSH/portable/ http://www.psc.edu/index.php/hpn-ssh-patches/hpn-14-kitchen-sink-patches/viewcategory/24 Extract OpenSSH:\n1\ntar -xzvf openssh-6.6p1.tar.gz\nChange directory in extracted folder and apply patch:\n1\n2\ncd openssh-6.6p1\nzcat /usr/src/openssh-6.6p1-hpnssh14v5.diff.gz | patch\nConfigure OpenSSH:\n1\n./configure –prefix=/usr –sysconfdir=/etc/ssh –with-pam\nRemove old config files to prevent any conflicts:\n1\n2\nrm /etc/ssh/ssh_config\nrm /etc/ssh/sshd_config\nCompile and install:\n1\n2\nmake\nmake install\nNow we have the newest version of OpenSSH installed and patched with the improvements from HPN-SSH; however we still need to make some changes to the /etc/ssh/sshd_config to take advantage of them.","tags":null,"title":"High Performance SSH/SCP","type":"post"},{"authors":null,"categories":[],"content":" 惠普混合云 惠普tripleO整体方案 惠普混合云主要模块 网络控制节点 授权认证部分源代码 医疗数据分析  医疗数据某业务流程  医疗数据处理图  医疗项目部分工作内容\n  部分任务 部分任务 部分源代码 ","date":1300960415,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1300960415,"objectID":"20111c36fe17b39a51abd0494c7132e2","permalink":"https://wubigo.com/post/hpcloud-notes/","publishdate":"2011-03-24T17:53:35+08:00","relpermalink":"/post/hpcloud-notes/","section":"post","summary":" 惠普混合云 惠普tripleO整体方案 惠普混合云主要模块 网络控制节点 授权认证部分源代码 医疗数据分析  医疗数据某业务流程  医疗数据处理图  医疗项目部分工作内容\n  部分任务 部分任务 部分源代码 ","tags":["IAAS"],"title":"Hpcloud Notes","type":"post"},{"authors":null,"categories":null,"content":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB. However, for high end HPC clusters, the NFS server can quickly become a major bottleneck as it does not scale well when used in large cluster environments. The NFS server also becomes a single point of failure, for which the consequences of it crashing can become severe.\nSAN (Storage Area Networks) SAN file systems are capable of very high performance, but are extremely expensive to scale up since they are implemented using Fibre Channel and therefore, each node that connects to the SAN must have a Fibre Channel card to connect to the Fibre channel switch.\nLustre (a Global Parallel File System) The main advantage of Lustre, a global parallel file system, over NFS and SAN file systems is that it provides; wide scalability, both in performance and storage capacity; a global name space, and the ability to distribute very large files across many nodes. Because large files are shared across many nodes in the typical cluster environment, a parallel file system, such as Lustre, is ideal for high end HPC cluster I/O systems.\nA typical storage system consists of a variety of components, including disks, storage controllers, IO cards, storage servers, storage area network switches, and related management software. Fitting all these components together and tuning them to achieve optimal performance presents significant challenges.\nIf you are managing your own infrastructure in your own private data center, then you are bound to go through a selection of different storage offerings. Selecting a storage solution pretty much depends on your requirement. Before finalizing a particular storage option for your use case, a little bit of understanding about the technology is always helpful.\nI was actually going to write an article about object storage (which is the current hottest storage option in the cloud). But before going and discussing that part of the storage arena, I thought its better to discuss the two main storage methods which co-exists together from a very long time, used by companies internally for their needs.\nThe decision of your storage type will depend on many factors like the below ones.\nType of data that you want to store\nUsage pattern\nScaling concerns\nFinally your budget\nWhen you begin your career as a system administrator, you will often hear your colleagues talking about different storage methods like SAN, NAS, DAS etc. And without a little bit of digging, you are bound to get confused with different terms in storage. The confusion arises often because of the similarities between the different approaches in storage. The only hard and fast rule to stay up to date in technical terms, is to keep on reading stuffs (especially concepts behind a certain technology.)\nToday we will be discussing two different methods that defines the structure of storage in your environment. Your choice of the two in your architecture should only depend on your use case, and type of data that you store.\nBy the end of this tutorial, I hope you will have a clear picture about the main two types of storage methods, and what to select for your need.\nSAN (Storage Area Network) and NAS(Network Attached Storage)\nThe main things that differentiate each of these technologies are mentioned below.\nHow a storage is connected to a system. In short how the connection is made between the accessing system and the storage component (directly attached or network attached)\nType of cabling used to connect. In short this is the type of cabling done to connect a system to the storage component (eg. Ethernet \u0026amp; Fiber channel)\nHow are input and output requests done. In short this is the protocol used to conduct input and output requests (eg. SCSI, NFS, CIFS etc)\nRelated: How to monitor IO on linux\nLet\u0026rsquo;s discuss SAN first and then NAS, and at the end, let\u0026rsquo;s compare each of these technologies to clear the differences between them.\nSAN(Storage Area Network)\nToday\u0026rsquo;s applications are very much resource intensive, due to the kind of requests that needs to be processed simultaneously per second. Take example of an e-commerce website, where thousands of people are making orders per second, and all needs to be stored properly in the database for later retrieval. The storage technology used to store such high traffic data bases must be fast in request serving and response(in short it should be fast in Input and Output).\nRelated: Web server Performance test\nIn such cases(where you need high performance, and fast I/O ) we can use SAN.\nSAN is nothing but a high speed network that makes connections between storage devices and servers.\nTraditionally application servers used to have their own storage devices attached to them. Server\u0026rsquo;s talk to these devices by a protocol known as SCSI(Small Computer System Interface). SCSI is nothing but a standard used to communicate between servers and storage devices. All normal hard disks, tape drives etc uses SCSI. In the beginning the storage needs of a server was fulfilled by a storage devices that was included inside the server(the server used to talk to those internal storage device, using SCSI. This is very much similar to how a normal desktop talks to its internal hard disk.).\nDevices like Compact Disk drives are attached to the server(which are part of the server) using SCSI. The main advantage of SCSI for connecting devices to a server was its high throughput. Although this architecture is sufficient for low end requirements, there are few limitations like the below mentioned ones.\nThe server can only access data on the devices, which are directly attached to it. If something happens to the server, access to data will fail (because the storage device is part of the server and is attached to it using SCSI) There is a limit in the number of storage devices the server can access. In case the server needs more storage space, there will be no more space that can be attached, as the SCSI bus can accommodate only a finite number of devices. Also the server using the SCSI storage has to be near the storage device(because parallel SCSI, which is the normal implementation in most computer\u0026rsquo;s and servers, has some distance limitations. It can work up to 25 meters.)\nSome of these limitations can be overcame using DAS (Directly Attached Storage). The media used to directly connect storage to the server can be any one of SCSI, Ethernet, Fiber channel etc.). Low complexity, Low investment, Simplicity in deployment caused DAS to be adopted by many for normal requirement\u0026rsquo;s. The solution was good even performance wise, if used with faster mediums like fiber channel.\nEven an external USB drive attached to a server is also a DAS(well conceptually its DAS, as its directly attached to the server\u0026rsquo;s USB bus). But USB drives are normally not used due to the speed limitation of USB bus. Normally for heavy and large DAS storage solutions, the media used are SAS(Serially attached SCSI). Internally the storage device can use RAID(which normally is the case) or anything to provide storage volumes to servers. SAS storage options provide 6Gb/s speed these days.\nAn example of DAS storage device is Dell\u0026rsquo;s MD1220\nTo the server, a DAS storage will appear very much similar to its own internal drive or an external drive that you plugged in.\nAlthough DAS is good for normal needs and gives good performance, there are limitations like the number of servers that can access it. Storage device, or say DAS storage has to be near to the server (in the same rack or within the limits of the accepted distance of the medium used.).\nIt can be argued that, directly attached storage(DAS) is faster than any other storage methods. This is because it does not involve any overhead of data transfer over the network (all data transfer occurs on a dedicated connection between the server and the storage device. Mostly its Serially attached SCSI or SAS). However due to latest improvement\u0026rsquo;s in fiber channel and other caching mechanism\u0026rsquo;s, SAN also provides better speed\u0026rsquo;s similar to DAS, and in some cases, it surpasses the speed provided by a DAS.\nBefore getting inside SAN, let\u0026rsquo;s understand several media types and methods that are used to interconnect storage devices(when i say storage devices, please dont consider it as one single hard disk. Take it as an array of disk\u0026rsquo;s, probably in some RAID level. Consider it as something like Dell\u0026rsquo;s MD1200).\nwhat is SAS(Serially Attached SCSI), FC(Fibre Channel), and iSCSI (Internet Small Computer System Interface)?\nTraditionally the SCSI devices like the internal hard disk\u0026rsquo;s are connected to a shared parallel SCSI bus. This means all devices attached, will be using the same bus to send/receive data. But shared parallel connections are not good for high accuracy, and create issues during high speed transfers. However a serial connection between the device and the server can increase the overall throughput of the data transfer. SAS connections between storage devices and servers uses a dedicated 300 MB/Sec per disk. Think of SCSI bus that shares the same speed for all devices connected.\nSAS uses the same SCSI commands to send and receive data from a device. Also please do not think that SCSI is only used for internal storage. It is also used for external storage device to be connected to the server.\nIf data transfer performance and reliability is the choice, then using SAS is the best solution. In terms of reliability and error rate SAS disks are much better compared to the old SATA disks. SAS was designed by keeping performance in mind, due to which it is full-duplex. This means, data can be send and received simultaniously from a device using SAS. Also a single SAS host port can connect to multiple SAS drives using expanders. SAS uses point to point data transfer by using serial communication between devices (storage device, like disk drives \u0026amp; disk array\u0026rsquo;s) and hosts.\nThe first generation of SAS provided around 3Gb/s of speed. The second generation of SAS improved this to 6Gb/s. And the third generation (which is currently used by many organization\u0026rsquo;s for extremly high throughput) improved this to 12Gb/s.\nFiber Channel Protocol Fiber Channel is a relatively new interconnection technology used for fast data transfer. The main purpose of its design is to enable transport of data at faster rates with a very less/negligible delay. It can be used to interconnect workstations, peripherals, storage array\u0026rsquo;s etc.\nThe major factor that distinguishes fiber channel from other interconnecting method is that, it can manage both networking and I/O communication over a single channel using the same adapters.\nANSI (American National Standards Institute) standardized Fiber channel during 1988. When we say Fiber (in Fiber channel) do not think that it only supports optical fiber medium. Fiber is a term used for any medium used to interconnect in fiber channel protocol. You can even use copper wire for lower cost.\nPlease note the fact that fiber channel standard from ANSI supports networking, storage and data transfer. Fiber channel is not aware of the type of data that you transfer. It can send SCSI commands encapsulated inside a fiber channel frame(it does not have its own I/O commands to send and receive storage). The main advantage is that it can incorporate widely adopted protocols like SCSI and IP inside.\nThe components of making a fiber channel connection are mentioned below. The below requirement is very minimal to achieve a point to point connection. Typically this can be used for a direct connection between a storage array and a host.\nAn HBA (Host Bus Adapter) with Fiber channel port\nDriver for the HBA card\nCables to interconnect devices in HBA fiber channel port\nAs mentioned earlier, SCSI protocol is encapsulated inside fiber channel. So normally SCSI data has to be modified to a different format that fiber channel can deliver to the destination. And when the destination receives the data it then retranslates it to SCSI.\nYou might be thinking that why do we need this mapping and re-mapping, why cant we directly use SCSI to deliver data. Its because SCSI cannot deliver data to greater distances to large number of devices (or large number of hosts).\nFiber cannel can be used to interconnect systems as far as 10KM (if used with optical fibers. You can increase this distance by having repeaters in between). And you can also transfer data to an extent of 30m using a copper wire for lower cost in fiber cannel.\nWith the emergence of fiber channel switches from variety of major vendors, connecting many large number of storage devices and servers have now become an easy task(provided you have the budget to invest). The networking ability of fiber channel led to the advanced adoption of SAN(Storage Area Networks) for faster, long distance, and reliable data access. Most of the high computing environment\u0026rsquo;s(which requires fast and large volume data transfers) uses fiber channel SAN with optical fiber cables.\nThe current fiber channel standard (called as 16GFC) can transmit data at the rate of 1600MB/s(dont forget the fact that this standard was released in 2011). The upcoming standards in the coming years are expected to provide 3200MB/s and 6400MB/s speed.\niSCSI(Internet Small Computer System Interface )\niSCSI is nothing but an IP based standard for interconnecting storage arrays and hosts. It is used to carry SCSI traffic over IP networks. This is the simplest and cheap solution(although not the best) to connect to a storage device.\nThis is a nice technology for location independent storage. Because it can establish connection to a storage device using local area networks, Wide area network. Its a Storage Area Network interconnection standard. It does not require special cabling and equipments like the case of a fiber channel network.\nTo the system using a storage array with iSCSI, the storage appears as a locally attached disk. This technology came after fiber channel and was widely adopted due to it low cost.\nIts a networking protocol which is made on top of TCP/IP. You can guess that its not at all good performance wise, when compared with fiber channel(simply because everything is running over TCP with no special hardware and modifications to your architecture.)\niSCSI introduces a little bit of CPU load on the server, because the server has to do the extra processing for all storage requests over the network, with the regular TCP.\nRelated: Linux CPU performance Monitoring\niSCSI has the following disadvantages, compared to fiber channel\niSCSI introduces a little bit more latency compared to fiber channel, due to the overhead of IP headers Database applications have small read and write operations, which when done on iSCSI will introduce more latency iSCSI when done on the same LAN, which contains other normal traffic (other infrastructure traffic other than iSCSI), it will introduce a read/write lag or say low performance. The maximum speed/bandwidth is limited to your ethernet and network speed. Even if you aggregate multiple links, it does not scal to the level of a fiber channel.\nNAS(Network Attached Storage)\nThe simplest definition of NAS is \u0026ldquo;Any server that shares its own storage with others on the network and acts as a file server is the simplest form NAS\u0026rdquo;.\nPlease make a note of the fact that Network Attached Storage shares files over the network. Not storage device over the network.\nNAS will be using an ethernet connection for sharing files over the network. The NAS device will have an IP address, and then will be accessible over the network through that IP address. When you access files on a file server on your windows system, its basically NAS.\nThe main difference is in how your computer or the server treats a particular storage. If the computer treats a storage as part of itself(similar to how you attach a DAS to your server), in other words, if the server\u0026rsquo;s processor is responsible for managing the storage attached, it will be some sort of DAS. And if the computer/server treats the storage attached as another computer, which is sharing its data through the network, then its a NAS.\nDirectly attached storage(DAS) can be viewed as any other peripheral device like mouse keyboard etc. Because to the server/computer, its a directly attached storage device. However NAS is another server, or say an equipment having its own computing features that can share its own storage with others.\nEven SAN storage can also be considered as an equipment that has its own processing/computing power. So the main difference between NAS, SAN and DAS is how the server/computer accessing it sees. A DAS storage device appears to the server as part of itself. The server sees it as its own physical part. Although the DAS storage device might not be inside the server(its normally another device with its own storage array), the server sees it as its own internal part(DAS storage appears to the server as its own internal storage)\nWhen we talk about NAS, we need to call them shares rather than storage devices. Because NAS appears to a server as a shared folder instead of a shared device over the network. Please do not forget the fact that NAS devices are computers in themselves, who can share their storage space with others. When you share a folder with access control using SAMBA, its NAS.\nAlthough NAS is a cheaper option for your storage needs. It really does not suit for an enterprise level high performance application. Never ever think of using a database storage (which needs to be high performing) with a NAS. The main downside of using NAS is its performance issue, and dependency on network(most of the times, the LAN which is used for normal traffic is also used for sharing storage with NAS, which makes it more congested)\nRelated: Linux Network Performance Tuning\nWhen you share an export with NFS over the network, its also a form of NAS.\nRelated: NFS Tutorial in Linux\nA NAS is nothing but a device/equipmet/server attached to TCP/IP network, that shares its own storage with other\u0026rsquo;s. If you dig a little deeper, when a file read/write request is send to a NAS share attached to a server, the request is sent in the form of a CIFS(Common internet file system) or NFS(Network File system) requests over the network. The receiving end(NAS device), on receiving the NFS, CIFS request, will then convert it into the local storage I/O command set. This is the reason, why a NAS device has its own processing and computing power.\nSo NAS is file level storage(Because its basically a file sharing technology). This is because it hides the actual file system under the hood. It gives the users an interface to access its shared storage space using NFS, or CIFS.\nRelated: How to do NFS Performance Tuning in Linux\nA common use of NAS you can find is to provide each user with a home directory. These home directories are stored in a NAS device, and mounted to the computer, where the user logs in. As the home directory is networkly accessible, the user can log in from any computer on the network.\nAdvantages of NAS\nNAS has a less complex architecture compared to SAN Its cheaper to deploy in an existing architecture. No modification is required on your architecture, as a normal TCP/IP network is the only requirement\nDisadvantages of NAS NAS is slow Lowever throughput and high latency, due to which it cannot be used for high performance applications\nGetting Back to SAN\nNow let\u0026rsquo;s get back to our discussion of SAN(Storage area network) which we started earlier in the beginning.\nThe first and foremost thing to understand about SAN (apart from the things we already discussed in the beginning) is the fact that its a block level storage solution. And SAN is optimized for high volume of block level data transfer. SAN is performs best when used with fiber channel medium (optical fibers, and a fiber channel switch )\nBoth NAS and SAN solves the problem of keeping the storage device nearer to the server accessing it(which was the case with DAS). A SAN storage can be alloted to a server, which in tern can share it with other\u0026rsquo;s using NAS. Please do not forget the fact that the underlying disks on a DAS, NAS and a SAN can be in any form of a RAID (what makes the real difference is how the server access these storage devices, using which protocol and media).\nThe name Storage Area Network itself implies that the storage resides in its own dedicated network. Hosts can attach the storage device to itself using either Fiber channel, TCP/IP network (SAN uses iSCSI when used over tcp/ip network).\nSAN can be considered as a technology that combines the best features of both DAS and NAS. If you remember, DAS appears to the computer as its own storage device, and is known for good speed, DAS is also a block level storage solution(if you remember, we never talked of CIFS or NFS during DAS). NAS is known for its flexibility, primary access through network, access control etc. SAN combines the best features of both of these worlds together because\u0026hellip;.\nSAN storage also appears to the server as its own storage device Its a block level storage solution Good performance/speed Networking features using iSCSI\nSAN and NAS are not competing technologies, but were designed for different needs and purposes. As SAN is a block level storage solution, its best suited for high performance data base storage, email storage etc. Most modern SAN solutions provide, disk mirroring, archiving backup and replication features as well.\nSAN is a dedicated network of storage devices(can include tape drives storages, raid disk arrays etc) all working together to provide an excellent block level storage. While NAS is a single device/server/computing appliance, sharing its own storage over the network.\nMajor Differences between SAN and NAS\nSAN\nNAS\nBlock level data access\nFile Level Data access\nFiber channel is the primary media used with SAN.\nEthernet is the primary media used with NAS\nSCSI is the main I/O protocol\nNFS/CIFS is used as the main I/O protocol in NAS\nSAN storage appears to the computer as its own storage\nNAS appers as a shared folder to the computer\nIt can have excellent speeds and performance when used with fiber channel media\nIt can sometimes worsen the performance, if the network is being used for other things as well(which normally is the case)\nUsed primarily for higher performance block level data storage\nIs used for long distance small read and write operations\n[1] http://www.slashroot.in/san-vs-nas-difference-between-storage-area-network-and-network-attached-storage\n[2]http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/architecting-lustre-storage-white-paper.pdf\n","date":1296691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296691200,"objectID":"82375dedd8ea5f1b31f88c532e3a0709","permalink":"https://wubigo.com/post/2011-02-03-san/","publishdate":"2011-02-03T00:00:00Z","relpermalink":"/post/2011-02-03-san/","section":"post","summary":"NFS VS. SAN VS. lUSTRE NFS (Network File System) NFS has been around for over 20 years, is very stable, easy to use and most systems administrators, as well as users, are generally familiar with its strengths and weaknesses. In low end HPC storage environments, NFS can still be a very effective medium for distributing data, where low end HPC storage systems are defined as capacity under 100TB and high end generally above 1PB.","tags":null,"title":"NFS VS. SAN VS. lUSTRE","type":"post"},{"authors":null,"categories":null,"content":" Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; $cat /etc/kubernetes/manifests/etcd.yaml - etcd - --advertise-client-urls=https://192.168.1.11:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.1.11:2380 - --initial-cluster=bigo-vm1=https://192.168.1.11:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.11:2379 - --listen-peer-urls=https://192.168.1.11:2380 - --name=bigo-vm1 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt image: K8S.gcr.io/etcd:3.2.24 imagePullPolicy: IfNotPresent livenessProbe: exec: command: - /bin/sh - -ec - ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo  check kube apiserver access of etcd with curl sudo curl -L -v https://192.168.1.11:2379/v3/keys --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/apiserver-etcd-client.crt --key /etc/kubernetes/pki/apiserver-etcd-client.key * Trying 192.168.1.11... * Connected to 192.168.1.11 (192.168.1.11) port 2379 (#0) * found 1 certificates in /etc/kubernetes/pki/etcd/ca.crt * found 597 certificates in /etc/ssl/certs * ALPN, offering http/1.1 * SSL connection using TLS1.2 / ECDHE_RSA_AES_128_GCM_SHA256 * server certificate verification OK * server certificate status verification SKIPPED * common name: bigo-vm3 (matched) * server certificate expiration date OK * server certificate activation date OK * certificate public key: RSA * certificate version: #3 * subject: CN=bigo-vm3 * start date: Sun, 17 Feb 2019 14:15:39 GMT * expire date: Mon, 17 Feb 2020 14:15:40 GMT * issuer: CN=etcd-ca * compression: NULL * ALPN, server did not agree to a protocol \u0026gt; GET /v3/keys HTTP/1.1 \u0026gt; Host: 192.168.1.11:2379 \u0026gt; User-Agent: curl/7.47.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 404 Not Found \u0026lt; Content-Type: text/plain; charset=utf-8 \u0026lt; X-Content-Type-Options: nosniff \u0026lt; Date: Mon, 18 Feb 2019 02:56:03 GMT \u0026lt; Content-Length: 19 \u0026lt; 404 page not found * Connection #0 to host 192.168.1.11 left intact  ","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"a0a926156dd65e503d0d3db920b3a467","permalink":"https://wubigo.com/post/2011-02-01-etcd-notes/","publishdate":"2011-02-01T00:00:00Z","relpermalink":"/post/2011-02-01-etcd-notes/","section":"post","summary":"Verify etcd CA data sudo openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text ... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Subject Alternative Name: DNS:bigo-vm3, DNS:localhost, IP Address:192.168.1.11, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1 ...   server.crt is signed for DNS names [bigo-vm3 localhost] and IPs [192.168.1.11 127.0.0.1 ::1]\n etcd config $kubeadm init phase etcd local -v 4 [etcd] wrote Static Pod manifest for a local etcd member to \u0026quot;/etc/kubernetes/manifests/etcd.","tags":null,"title":"ETCD notes","type":"post"},{"authors":null,"categories":null,"content":" TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk\n","date":1294012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1294012800,"objectID":"890ebb9e3a5d4cfcfbb67b297f57eeea","permalink":"https://wubigo.com/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","publishdate":"2011-01-03T00:00:00Z","relpermalink":"/post/2011-01-03-the-difference-between-a-tmpfs-and-ramfs-ram-disk/","section":"post","summary":"TEMPFS vs RAMFS https://www.jamescoyle.net/knowledge/951-the-difference-between-a-tmpfs-and-ramfs-ram-disk","tags":null,"title":"the-difference-between-a-tmpfs-and-ramfs-ram-disk","type":"post"},{"authors":null,"categories":["IT"],"content":" push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.json \u0026lt;\u0026lt; EOF { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://registry.docker-cn.com\u0026quot;, \u0026quot;https://11h2ev58.mirror.aliyuncs.com\u0026quot;] } EOF scp daemon.json $VM:~/ tee d.sh \u0026lt;\u0026lt; EOF sudo mkdir -p /etc/docker sudo mv daemon.json /etc/docker sudo systemctl daemon-reload sudo systemctl restart docker EOF  ssh $VM \u0026lsquo;bash -s\u0026rsquo; \u0026lt; d.sh rm daemon.json\nclaim docker disk space docker-clean.sh\n#!/usr/bin/env bash # ignoring pipe fail of non-zero exit code set -o pipefail docker images --no-trunc | grep '\u0026lt;none\u0026gt;' | awk '{ print $3 }' | xargs -r docker rmi docker ps --filter status=dead --filter status=exited -aq | xargs docker rm -v [ ! -z \u0026quot;$VM\u0026quot; ] \u0026amp;\u0026amp; ssh $VM 'bash -s' \u0026lt; docker-clean.sh.sh  kube build export K8S_VERSION = v1.13.3 git clone https://github.com/kubernetes/kubernetes.git $GOPATH/src/K8S.io/ git fetch --all git checkout tags/$K8S_VERSION -b v$K8S_VERSION  #!/usr/bin/env bash export ETCD_HOST=192.168.1.9 export KUBE_INTEGRATION_ETCD_URL=http://$ETCD_HOST:2379 cd $GOPATH/src/K8S.io/kubernetes/build/ bash -x ./run.sh make \u0026gt; run.log 2\u0026gt;\u0026amp;1  kubeadm init #!/usr/bin/env bash cat \u0026lt;\u0026lt; EOF \u0026gt; init.sh #!/usr/bin/env bash sudo kubeadm reset -f sudo kubeadm init --kubernetes-version=v1.13.3 --pod-network-cidr 10.2.0.0/16 -v 4 \u0026gt; kubeadm.init.log 2\u0026gt;\u0026amp;1 mkdir -p $HOME/.kube sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config EOF ssh $VM 'bash -s' \u0026lt; init.sh rm init.sh  kube image pull then tag #!/usr/bin/env bash docker pull mirrorgooglecontainers/kube-apiserver:v1.13.3 docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.3 docker pull mirrorgooglecontainers/kube-scheduler:v1.13.3 docker pull mirrorgooglecontainers/kube-proxy:v1.13.3 docker pull mirrorgooglecontainers/pause:3.1 docker pull mirrorgooglecontainers/etcd:3.2.24 docker pull coredns/coredns:1.2.6 docker tag mirrorgooglecontainers/kube-apiserver:v1.13.3 K8S.gcr.io/kube-apiserver:v1.13.3 docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.3 K8S.gcr.io/kube-controller-manager:v1.13.3 docker tag mirrorgooglecontainers/kube-scheduler:v1.13.3 K8S.gcr.io/kube-scheduler:v1.13.3 docker tag mirrorgooglecontainers/kube-proxy:v1.13.3 K8S.gcr.io/kube-proxy:v1.13.3 docker tag mirrorgooglecontainers/pause:3.1 K8S.gcr.io/pause:3.1 docker tag mirrorgooglecontainers/etcd:3.2.24 K8S.gcr.io/etcd:3.2.24 docker tag coredns/coredns:1.2.6 K8S.gcr.io/coredns:1.2.6  prepare kubelet for kubeadm deploy  build\ncd build run.sh make scp ~/go/src/K8S.io/kubernetes/_output/dockerized/bin/linux/amd64/kube??? vm1:~/   deploy K8S master #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi if [ ! -z \u0026quot;$KV\u0026quot; ]; then KV = v1.13.3 echo \u0026quot;VAR VM is not set ,set to $KV\u0026quot; exit fi if [ ! -z \u0026quot;$PN\u0026quot; ]; then PN = 10.2.0.0/16 echo \u0026quot;VAR PN is not set, set to $PN\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf # sudo kubeadm init --kubernetes-version=$KV --pod-network-cidr 10.2.0.0/16 -v 4 if [ -d \u0026quot;$HOME/.kube\u0026quot; ]; then mkdir -p $HOME/.kube fi sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config curl https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/calico.yaml\u0026gt; calico.yaml # calico etcd setup sed -i -e \u0026quot;s/\\(^etcd_endpoints: \\\u0026quot;http.*$\\)/etcd_endpoints: \\\u0026quot;https:\\/\\/$VM:2379\\\u0026quot;/g\u0026quot; calico.yaml # etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; sed -i -e 's/etcd_ca: \\\u0026quot;\\\u0026quot; \\# \\\u0026quot;\\/calico-secrets/etcd-ca\\\u0026quot;/etcd_ca: \\\u0026quot;\\/calico-secrets\\/etcd-ca\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_cert: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/etcd_cert: \\\u0026quot;\\/calico-secrets\\/etcd-cert\\\u0026quot;/g' calico.yaml sed -i -e 's/etcd_key: \\\u0026quot;\\\u0026quot; # \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/etcd_key: \\\u0026quot;\\/calico-secrets\\/etcd-key\\\u0026quot;/g' calico.yaml CA=$(cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) CERT=$(cat /etc/kubernetes/pki/etcd/server.crt | base64 -w 0) KEY=$(sudo cat /etc/kubernetes/pki/etcd/server.key | base64 -w 0) sed -i -e \u0026quot;s/# etcd-ca: null/etcd-ca: $CA/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-cert: null/etcd-cert: $CERT/g\u0026quot; calico.yaml sed -i -e \u0026quot;s/# etcd-key: null/etcd-key: $KEY/g\u0026quot; calico.yaml kubectl apply -f calico.yaml EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  deploy K8S working node #!/usr/bin/env bash if [ ! -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi scp ~/go/src/K8S.io/kubernetes/build/debs/kubelet.service $VM:~/ scp ~/go/src/K8S.io/kubernetes/build/debs/10-kubeadm.conf $VM:~/ scp ~/go/src/github.com/containernetworking/cni/bin/* $VM:~/ scp ~/go/bin/cri* $VM:~/ cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash sudo cp ~/kube??? /usr/bin/ sudo cp ~/kubelet.service /etc/systemd/system/kubelet.service sudo mkdir -p /etc/kubernetes/manifests sudo mkdir -p /etc/systemd/system/kubelet.service.d/ sudo cp ~/10-kubeadm.conf /etc/systemd/system/kubelet.service.d/10-kubeadm.conf sudo systemctl daemon-reload sudo systemctl enable kubelet --now sudo systemctl start kubelet mkdir -p /opt/cni/bin sudo cp ~/cnitool ~/noop /opt/cni/bin sudo cp ~/cri* /usr/local/bin/ #clean up rm -f kube??? crictl critest cnitool noop kubelet.service 10-kubeadm.conf EOF TOKEN=$(kubeadm token list) CA_HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //') cat \u0026lt;\u0026lt; EOF \u0026gt; d.sh #!/usr/bin/env bash kubeadm join 192.168.1.11:6443 --token $TOKEN --discovery-token-ca-cert-hash sha256:$CA_HASH EOF # execute the local script on the remote server ssh $VM 'bash -s' \u0026lt; d.sh rm d.sh  replace spaces in file names using a bash script find -name \u0026quot;* *\u0026quot; -type d | rename 's/ /_/g' # do the directories first find -name \u0026quot;* *\u0026quot; -type f | rename 's/ //g'  docker PID $PATH/docker-pid\n#!/usr/bin/env bash exec docker inspect --format '{{ .State.Pid }}' \u0026quot;$@\u0026quot;  #!/usr/bin/env bash\n$PATH/docker-ip\n#!/usr/bin/env bash exec docker inspect --format '{{ .NetworkSettings.IPAddress }}' \u0026quot;$@\u0026quot;  ","date":1293881943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293881943,"objectID":"0ac1ed1f5371d94cd4838907db81c365","permalink":"https://wubigo.com/post/2011-01-01-shell-script/","publishdate":"2011-01-01T19:39:03+08:00","relpermalink":"/post/2011-01-01-shell-script/","section":"post","summary":"push docker images to ali  registry-mirrors   https://cr.console.aliyun.com\n #!/usr/bin/env bash docker login --username=wubigo registry.cn-beijing.aliyuncs.com docker images | grep v1.13 | awk '{ print $1 }' | sed --expression=s'/K8S.gcr.io\\///' | xargs -i -t docker tag K8S.gcr.io/{}:v1.13.3 registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3 docker images |grep \u0026quot;registry.cn-beijing.aliyuncs.com\u0026quot;| awk '{ print $1 }'| sed --expression=s'/registry.cn-beijing.aliyuncs.com\\/co1\\///' | xargs -i -t docker push registry.cn-beijing.aliyuncs.com/co1/{}:v1.13.3  docker push through cache #!/usr/bin/env bash if [ -z \u0026quot;$VM\u0026quot; ]; then VM = t1 echo \u0026quot;VAR VM is not set\u0026quot; exit fi tee daemon.","tags":["SHELL","LINUX"],"title":"shell script","type":"post"},{"authors":null,"categories":null,"content":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\n util ","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"2f8ee50015c93c05ef78deeb590ca3e7","permalink":"https://wubigo.com/post/2011-01-01-dockerfile/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/post/2011-01-01-dockerfile/","section":"post","summary":" tiller (tag=$(tiller version)) FROM alpine:3.8 RUN apk update \u0026amp;\u0026amp; apk add ca-certificates socat \u0026amp;\u0026amp; rm -rf /var/cache/apk/* ENV HOME /tmp COPY tiller /tiller EXPOSE 44134 USER 65534 ENTRYPOINT [\u0026quot;/tiller\u0026quot;]   docker push registry.cn-beijing.aliyuncs.com/k4s/tiller:v2.12.3\n util ","tags":null,"title":"Dockerfile","type":"post"},{"authors":null,"categories":[],"content":" LINUX shell常用工具提供强大的功能，在日常中熟练掌握能给我 带来不少动能\n grep cat find head/tail wc awk shuf  查找 在logs目录下查找所有包含2010_05_02的日志文件\nls logs/ | grep 2010_05_02  pip freeze | grep scipy scipy==1.1.0  grep -oP \u0026quot;'[\\w]+ == [\\d.]+'\u0026quot; setup.py scipy == 1.1.0  #\nfind . -name '..*swp' -delete  awk head -n 1 data.csv | awk -F ',' '{print NF}'  shuf 从数据集中随机取50个样本\ncat big_csv.csv | shuf | head -n 50 \u0026gt; sample_csv.csv  ","date":1267948331,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267948331,"objectID":"94f29d0e25b5c2aeb31c2f3caf05857d","permalink":"https://wubigo.com/post/6-linux-command-should-be-in-control/","publishdate":"2010-03-07T15:52:11+08:00","relpermalink":"/post/6-linux-command-should-be-in-control/","section":"post","summary":" LINUX shell常用工具提供强大的功能，在日常中熟练掌握能给我 带来不少动能\n grep cat find head/tail wc awk shuf  查找 在logs目录下查找所有包含2010_05_02的日志文件\nls logs/ | grep 2010_05_02  pip freeze | grep scipy scipy==1.1.0  grep -oP \u0026quot;'[\\w]+ == [\\d.]+'\u0026quot; setup.py scipy == 1.1.0  #\nfind . -name '..*swp' -delete  awk head -n 1 data.csv | awk -F ',' '{print NF}'  shuf 从数据集中随机取50个样本\ncat big_csv.csv | shuf | head -n 50 \u0026gt; sample_csv.csv  ","tags":["SHELL"],"title":"应该掌握的linux命令","type":"post"},{"authors":null,"categories":[],"content":" iproute2 SCTP transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams. QUIC provides similar multi-stream semantics. SCTP also allows data to be transferred over two outgoing paths when a host is connected to two or more networks, optional delivery of out-of-order data, and a number of other features. SCTP’s flow- and congestion-control algorithms are essentially the same as in TCP.\ntunnel The tunneling protocol works by using the data portion of a packet (the payload) to carry the packets that actually provide the service.\nTypically, the delivery protocol operates at an equal or higher level in the layered model than the payload protocol. As an example of network layer over network layer, Generic Routing Encapsulation (GRE), a protocol running over IP (IP protocol number 47), often serves to carry IP packets, with RFC 1918 private addresses, over the Internet using delivery packets with public IP addresses\nDHCP In addition to host IP address assignment, DHCP also allows a host to learn additional information, such as its subnet mask, the address of its first-hop router (often called the default gateway), and the address of its local DNS server.\nlink layer implement Is a host’s link layer implemented in hardware or software? Is it implemented on a separate card or chip, and how does it interface with the rest of a host’s hardware and operating system components? For the most part, the link layer is implemented in a network adapter, also sometimes known as a network interface card (NIC). At the heart of the network adapter is the link-layer controller, usually a single, special-purpose chip that implements many of the link-layer services (framing, link access, error detection, and so on). Thus, much of a link-layer controller’s functionality is implemented in hardware\nlink-layer switches do not have link-layer addresses associated with their interfaces that connect to hosts and routers. This is because the job of the link-layer switch is to carry datagrams between hosts and routers; a switch does this job transparently, that is, without the host or router having to explicitly address the frame to the intervening switch\nMPLS MPLS performs switching based on labels, without needing to consider the IP address of a packet. The true advantages of MPLS and the reason for current interest in MPLS, however, lie not in the potential increases in switching speeds, but rather in the new traffic management capabilities that MPLS enables.\nMPLS provides the ability to forward packets along routes that would not be possible using standard IP routing protocols. This is one simple form of traffic engineering using MPLS\nIt can be used to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a precomputed failover path in response to link failure MPLS can, and has, been used to implement so-called virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses its MPLS-enabled network to connect together the customer’s various networks. MPLS can be used to isolate both the resources and addressing used by the customer’s VPN from that of other users crossing the ISP’s network\nWhy we use the Linux kernel\u0026rsquo;s TCP stack Since the Linux kernel cannot sustain the 10G packet rate, then some bypass technologies for a fast path are used. The main bypass technologies are either based on a limited set of features such as Open vSwitch (OVS) with its DPDK user space implementation or based on a full feature and offload of Linux processing such as 6WIND Virtual Accelerator.\nhttps://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf\n","date":1264115455,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264115455,"objectID":"67bb97234b88dae460701b1fe2d840cf","permalink":"https://wubigo.com/post/2010-01-22-computer-networking/","publishdate":"2010-01-22T07:10:55+08:00","relpermalink":"/post/2010-01-22-computer-networking/","section":"post","summary":"iproute2 SCTP transport-layer protocols are implemented in the end systems but not in network routers.\nThe Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a reliable, message-oriented protocol that allows several different application-level “streams” to be multiplexed through a single SCTP connection (an approach known as “multi-streaming”). From a reliability standpoint, the different streams within the connection are handled separately, so that packet loss in one stream does not affect the delivery of data in other streams.","tags":["NETWORK"],"title":"computer networking","type":"post"},{"authors":null,"categories":null,"content":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","date":1262649600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262649600,"objectID":"70b96e6c4b5506535b6e45525eca92f7","permalink":"https://wubigo.com/post/2010-01-05-%E5%90%8D%E8%A8%80/","publishdate":"2010-01-05T00:00:00Z","relpermalink":"/post/2010-01-05-%E5%90%8D%E8%A8%80/","section":"post","summary":"杨绛\n我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系  星云大师\n一个人倘若一心除恶，表示他看到的都是恶。 真正有益于世界的做法不是除恶，而是行善；不是打击负能量，而是弘扬正能量  ","tags":null,"title":"名言","type":"post"},{"authors":null,"categories":null,"content":" turn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  ","date":1262476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262476800,"objectID":"15cda36938065ff8abd3117931fa057d","permalink":"https://wubigo.com/post/2010-01-03-windows-note/","publishdate":"2010-01-03T00:00:00Z","relpermalink":"/post/2010-01-03-windows-note/","section":"post","summary":" turn on IE proxy @ECHO OFF ECHO Turn on proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 1 /f  turn off IE proxy @ECHO OFF ECHO Turn off IE Proxy! please wait... REG ADD \u0026quot;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\u0026quot; /v ProxyEnable /t REG_DWORD /d 0 /f  ","tags":null,"title":"windows notes","type":"post"},{"authors":null,"categories":[],"content":" JVM MEMORY MODEL javax.net.ssl.SSLException: Received fatal alert: protocol_version On Java 1.8 default TLS protocol is v1.2. On Java 1.6 and 1.7 default is obsoleted TLS1.0. I get this error on Java 1.8, because url use old TLS1.0\necho 'export JAVA_TOOL_OPTIONS=\u0026quot;-Dhttps.protocols=TLSv1.2\u0026quot;' \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  访问可见性    修饰符 类 包 子类 所有人     public 是 是 是 是   protected 是 是 是 否   没有修饰符 是 是 否 否   private 是 否 否 否    方法签名 方法签名包括\n 方法名 参数类型 参数顺序  不包括\n 返回类型 可见性 抛出例外  过载和覆盖  过载: 方法名称相同但签名不同 覆盖:  签名相同，而且返回类型也必须相同 可见性不能减少，可以增加可见性 例外必须相同或者是父类例外的子类   visibility and Atomicity in the absence of synchronization, there are a number of reasons a thread might not immediately ‐ or ever ‐ see the results of an operation in another thread. Compilers may generate instructions in a different order than the \u0026ldquo;obvious\u0026rdquo; one suggested by the source code, or store variables in registers instead of in memory; processors may execute instructions in parallel or out of order; caches may vary the order in which writes to variables are committed to main memory; and values stored in processor‐local caches may not be visible to other processors. These factors can prevent a thread from seeing the most up‐to‐date value for a variable and can cause memory actions in other threads to appear to happen out of order ‐ if you don\u0026rsquo;t use adequate synchronization.\nLock and ReentrantLock Before Java 5.0, the only mechanisms for coordinating access to shared data were synchronized and volatile. Java 5.0 adds another option: ReentrantLock. Contrary to what some have written, ReentrantLock is not a replacement for intrinsic locking, but rather an alternative with advanced features for when intrinsic locking proves too limited\nIntrinsic locking works fine in most situations but has some functional limitations ‐ it is not possible to interrupt a thread waiting to acquire a lock, or to attempt to acquire a lock without being willing to wait for it forever. Intrinsic locks also must be released in the same block of code in which they are acquired; this simplifies coding and interacts nicely with exception handling, but makes non‐blockstructured locking disciplines impossible\nReadWriteLock The locking strategy implemented by read‐write locks allows multiple simultaneous readers but only a single writer. In practice, read‐write locks can improve performance for frequently accessed read‐mostly data structures on multiprocessor systems; under other conditions they perform slightly worse than exclusive locks due to their greater complexity. Whether they are an improvement in any given situation is best determined via profiling; because ReadWriteLock uses Lock for the read and write portions of the lock, it is relatively easy to swap out a read‐write lock for an exclusive one if profiling determines that a read‐write lock is not a win. hashtable  Hashtable is synchronized, whereas HashMap is not. This makes HashMap better for non-threaded applications, as unsynchronized Objects typically perform better than synchronized ones. Hashtable does not allow null keys or values. HashMap allows one null key and any number of null values. One of HashMap\u0026rsquo;s subclasses is LinkedHashMap, so in the event that you\u0026rsquo;d want predictable iteration order (which is insertion order by default), you could easily swap out the HashMap for a LinkedHashMap. This wouldn\u0026rsquo;t be as easy if you were using Hashtable.  HashTable is obsolete in Java 1.7 and it is recommended to use ConcurrentMap implementation\nJava Memory Model The Java Memory Model is specified in terms of actions, which include reads and writes to variables, locks and unlocks of monitors, and starting and joining with threads. The JMM defines a partial ordering [2] called happens‐before on all actions within the program\nsoft reference four different degrees of reference strength: strong, soft, weak, and phantom, in order from strongest to weakest\nSoftReferences aren\u0026rsquo;t required to behave any differently than WeakReferences, but in practice softly reachable objects are generally retained as long as memory is in plentiful supply. This makes them an excellent foundation for a cache, such as the image cache described above\nchecked exception The cardinal rule in deciding whether to use a checked or an unchecked exception is this: use checked exceptions for conditions from which the caller can reasonably be expected to recover. By throwing a checked exception, you force the caller to handle the exception in a catch clause or to propagate it outward. Each checked exception that a method is declared to throw is therefore a potent indication to the API user that the associated condition is a possible outcome of invoking the method.\n","date":1262329580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262329580,"objectID":"68d3cc9b5739a3634423fe25924e34eb","permalink":"https://wubigo.com/post/java-notes/","publishdate":"2010-01-01T15:06:20+08:00","relpermalink":"/post/java-notes/","section":"post","summary":"JVM MEMORY MODEL javax.net.ssl.SSLException: Received fatal alert: protocol_version On Java 1.8 default TLS protocol is v1.2. On Java 1.6 and 1.7 default is obsoleted TLS1.0. I get this error on Java 1.8, because url use old TLS1.0\necho 'export JAVA_TOOL_OPTIONS=\u0026quot;-Dhttps.protocols=TLSv1.2\u0026quot;' \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc  访问可见性    修饰符 类 包 子类 所有人     public 是 是 是 是   protected 是 是 是 否   没有修饰符 是 是 否 否   private 是 否 否 否    方法签名 方法签名包括","tags":["JAVA","LANG"],"title":"Java Notes","type":"post"},{"authors":null,"categories":null,"content":" certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le\nNginx configuration to enable ACME Challenge support  #Rule for legitimate ACME Challenge requests (like /.well-known/acme-challenge/xxxxxxxxx) #We use ^~ here, so that we don't check other regexes (for speed-up). We actually MUST cancel #other regex checks, because in our other config files have regex rule that denies access to files with dotted names. location ^~ /.well- known/acme-challenge/ { #Set correct content type. According to this: #https://community.letsencrypt.org/t/using-the-webroot-domain-verification-method/1445/29 #Current specification requires \u0026quot;text/plain\u0026quot; or no content header at all. #It seems that \u0026quot;text/plain\u0026quot; is a safe option. default_type \u0026quot;text/plain\u0026quot;; #This directory must be the same as in /etc/letsencrypt/cli.ini #as \u0026quot;webroot-path\u0026quot; parameter. Also don't forget to set \u0026quot;authenticator\u0026quot; parameter #there to \u0026quot;webroot\u0026quot;. #Do NOT use alias, use root! Target directory is located here: #/var/www/common/letsencrypt/.well-known/acme-challenge/ root /var/www/letsencrypt; } #Hide /acme-challenge subdirectory and return 404 on all requests. #It is somewhat more secure than letting Nginx return 403. #Ending slash is important! location = /.well-known/acme-challenge/ { return 404; }  nginx need restart after certbot renew to avoid sec_error_expired_certificate ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b2a09ac3f3428209e3c3505d95b51b0f","permalink":"https://wubigo.com/post/letsencrypt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/letsencrypt/","section":"post","summary":"certbot  Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/et.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/et.com/privkey.pem Your cert will expire on 2018-02-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again with the \u0026ldquo;certonly\u0026rdquo; option. To non-interactively renew all of your certificates, run \u0026ldquo;certbot renew\u0026rdquo;  If you like Certbot, please consider supporting our work by:   Donating to ISRG / Let\u0026rsquo;s Encrypt: https://letsencrypt.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.0.17 Loaded Configuration File /etc/php/7.0/fpm/php.ini sudo apt-get install php-geoip php-dev libgeoip-dev sudo pecl install geoip sudo nano /etc/php/7.0/fpm/php.ini [PHP] ;AFTER THE PHP SECTION NOT BEFORE extension=geoip.so [gd] ;AFTER THE gd SECTION NOT BEFORE geoip.custom_directory=/usr/share/nginx/html/piwik/misc cd /usr/share/nginx/html/piwik/misc sudo wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz sudo gunzip GeoLiteCity.dat.gz PECL extension won't recognize the database if it's named GeoLiteCity.dat so make sure it is named GeoIPCity.dat: sudo mv GeoLiteCity.dat GeoIPCity.dat Restart the Apache Web Server: sudo service nginx restart Step Five - Configure Piwik to use GeoIP PECL Open your browser and login into your Piwik page, go to settings, Geolocation, and choose GeoIP (PECL) as your location provider. Updating Previous Visits and Updating the GeoIP Database sudo apt-get install php-mysql sudo php /usr/share/nginx/html/piwik/console usercountry:attribute 2017-01-01,2017-08-10  nginx http { geoip_country /usr/share/nginx/html/piwik/misc/GeoIP.dat; geoip_city /usr/share/nginx/html/piwik/misc/GeoIPCity.dat;  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e16ad23ea6e49aa8256164d577385637","permalink":"https://wubigo.com/post/piwik/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/piwik/","section":"post","summary":"Configuring Piwik accessed via an Nginx reverse proxy public Nginx server configured as\nlocation ^~ /piwik/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $host; proxy_pass http://192.168.79.4/piwik/; }  config.ini.php config on piwi nginx site\n[General] proxy_client_headers[] = \u0026quot;HTTP_X_FORWARDED_FOR\u0026quot; proxy_client_headers[] = \u0026quot;X-Real-IP\u0026quot; proxy_host_headers[] = \u0026quot;HTTP_X_FORWARDED_HOST\u0026quot; proxy_ips[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;192.168.79.4\u0026quot; trusted_hosts[] = \u0026quot;\u0026lt;public-domain-server\u0026gt;\u0026quot;  Configure GeoIP (PECL) With Piwik check php version\ncurl http://localhost/info.php PHP Version 7.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":" Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.e. \u0026ldquo;postgres\u0026rdquo;, to the database called \u0026ldquo;postgres\u0026rdquo; (1st argument to psql).\nSet a password for the \u0026ldquo;postgres\u0026rdquo; database role using the command:\n\\password postgres and give your password when prompted. The password text will be hidden from the console for security purposes.\nType Control+D or \\q to exit the posgreSQL prompt.\nCreate database\nTo create the first database, which we will call \u0026ldquo;mydb\u0026rdquo;, simply type:\nsudo -u postgres createdb mydb\n#sudo nano /etc/postgresql/9.3/main/pg_hba.conf and change the line host all all 0.0.0.0/0 md5\nDatabase administrative login by Unix domain socket local all postgres peer to\nDatabase administrative login by Unix domain socket local all postgres md5 Now you should reload the server configuration changes and connect pgAdmin III to your PostgreSQL database server.\nsudo /etc/init.d/postgresql reload\ncayley_v0.6.1_linux_amd64$ cat cayley.cfg { \u0026quot;listen_host\u0026quot;: \u0026quot;0.0.0.0\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;sql\u0026quot;, \u0026quot;db_path\u0026quot;: \u0026quot;postgres://postgres:psql@db/cayley?sslmode=disable\u0026quot;, \u0026quot;read_only\u0026quot;: false } $cayley init --config=cayley.cfg $cayley http --config=cayley.cfg $cayley load --config=cayley.cfg --quads=data/testdata.nq  Cayley looks in the following locations for the configuration file Command line flag The environment variable $CAYLEY_CFG /etc/cayley.cfg  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c5f2097b456fdf36712de2e9110e7c5a","permalink":"https://wubigo.com/post/psql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/psql/","section":"post","summary":"Basic Server Setup To start off, we need to set the password of the PostgreSQL user (role) called \u0026ldquo;postgres\u0026rdquo;; we will not be able to access the server externally otherwise. As the local “postgres” Linux user, we are allowed to connect and manipulate the server using the psql command.\nIn a terminal, type:\nsudo -u postgres psql postgres this connects as a role with same name as the local user, i.","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53d0412037dd477a59505788df13a330","permalink":"https://wubigo.com/post/smtp_ec2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/smtp_ec2/","section":"post","summary":"https://stackoverflow.com/questions/21087564/gmail-smtp-is-not-working-in-ec2-instance\nhttps://linuxconfig.org/configuring-gmail-as-sendmail-email-relay\nGo to https://www.google.com/settings/security/lesssecureapps and set Access for less secure apps to On\ngoto : https://accounts.google.com/DisplayUnlockCaptcha\nand click continue this going to allow access from other servers.\nhttps://www.mkyong.com/java/javamail-api-sending-email-via-gmail-smtp-example/","tags":null,"title":"","type":"post"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8S on 为人民服务</title>
    <link>https://wubigo.com/tags/k8s/</link>
    <description>Recent content in K8S on 为人民服务</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Mar 2019 11:13:20 +0800</lastBuildDate>
    
	<atom:link href="https://wubigo.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8S CORE DEVELOPMENT</title>
      <link>https://wubigo.com/post/k8s-core-development/</link>
      <pubDate>Mon, 04 Mar 2019 11:13:20 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-core-development/</guid>
      <description>git clone git@github.com:wubigo/kubernetes.git git remote add upstream https://github.com/kubernetes/kubernetes.git git fetch --all git checkout tags/v1.13.3 -b v1.13.3 git branch -av|grep 1.13 * fix-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2. remotes/origin/release-1.13 4807084f79 Add/Update CHANGELOG-1.13.md for v1.13.2.  </description>
    </item>
    
    <item>
      <title>通过SDK驾驭腾讯公有云</title>
      <link>https://wubigo.com/post/dev-on-tencent-cloud-sdk-in-go/</link>
      <pubDate>Sun, 03 Mar 2019 20:24:49 +0800</pubDate>
      
      <guid>https://wubigo.com/post/dev-on-tencent-cloud-sdk-in-go/</guid>
      <description>基于腾讯云Go SDK开发
下载开发工具集 go get -u github.com/tencentcloud/tencentcloud-sdk-go  为集群准备CVM 从本地开发集群K8S读取安全凭证secretId和secretKey配置信息， 然后把安全凭证传送给SDK客户端
secretId, secretKey:= K8SClient.Secrets(&amp;quot;namespace=tencent&amp;quot;).Get(&amp;quot;cloud-pass&amp;quot;) credential := CloudCommon.NewCredential(&amp;quot;secretId&amp;quot;, &amp;quot;secretKey&amp;quot;) client, _ := cvm.NewClient(credential, regions.Beijing)  request := cvm.NewAllocateHostsRequest() request.FromJsonString(K8SClient.Configs(&amp;quot;namespace=tencent&amp;quot;).Get(&amp;quot;K8S-TENCENT-PROD&amp;quot;)) response, err := client.AllocateHosts(request)  通过ANSIBLE在CVM搭建K8S集群 Ansible.Hosts().Get(response.ToJsonString())  调用ANSIBLE开始在CVM部署K8S集群</description>
    </item>
    
    <item>
      <title>K8S CNI操作指引</title>
      <link>https://wubigo.com/post/k8s-cni/</link>
      <pubDate>Sun, 24 Feb 2019 16:18:43 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-cni/</guid>
      <description> 简介 CNI是K8S的网络插件实现规范，与docker的CNM并不兼容，在K8S和docker的博弈过程中， K8S把docker作为默认的runtime并没有换来docker对K8S的支持。K8S决定支持CNI规范。 许多网络厂商的产品都提供同时都支持CNM和CNI的产品。
在容器网络环境，经常看到docker看不到K8S POD的IP网络配置， DOCKER容器有时候和POD无法通信。
CNI相对CNM是一个轻量级的规范。网络配置是基于JSON格式， 网络插件支持创建和删除指令。POD启动的时候发送创建指令。
POD运行时首先为分配一个网络命名空间，并把该网络命名空间制定给容器ID， 然后把CNI配置文件传送给CNI网络驱动。网络驱动连接容器到自己的网络， 并把分配的IP地址通过JSON文件报告给POD运行时POD终止的时候发送删除指令。
当前CNI指令负责处理IPAM, L2和L3, POD运行时处理端口映射(L4)
K8S网络基础 K8S网络基础
CNI插件 CNI实现方式 CNI有很多实现，在这里之列举熟悉的几个实现。并提供详细的说明文档。
 Flannel
 Kube-router
Kube-router
 OpenVSwitch
 Calico
Calico可以以非封装或非覆盖方式部署以支持高性能，高扩展扩展性数据中心网络需求
CNI-Calico
 Weave Net
 网桥
CNI 网桥
  </description>
    </item>
    
    <item>
      <title>kubeamd cheat sheet</title>
      <link>https://wubigo.com/post/kubeamd-cheat-sheet/</link>
      <pubDate>Mon, 11 Feb 2019 11:38:27 +0800</pubDate>
      
      <guid>https://wubigo.com/post/kubeamd-cheat-sheet/</guid>
      <description>version notes
some only works on 1.13
kubeadm version: &amp;amp;version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;13&amp;quot;, GitVersion:&amp;quot;v1.13.3&amp;quot;, GitCommit:&amp;quot;721bfa751924da8d1680787490c54b9179b1fed0&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2019-02-16T15:29:34Z&amp;quot;, GoVersion:&amp;quot;go1.11.5&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}  Starting with Kubernetes 1.12, the K8S.gcr.io/kube-${ARCH}, K8S.gcr.io/etcd and K8S.gcr.io/pause images don’t require an -${ARCH} suffix
 get all Pending pods  kubectl get pods --field-selector=status.phase=Pending   images list  kubeadm config images list -v 4 I0217 07:28:13.305268 14495 interface.go:384] Looking for default routes with IPv4 addresses I0217 07:28:13.307275 14495 interface.</description>
    </item>
    
    <item>
      <title>微服务安全：认证，授权和审计(AAA)</title>
      <link>https://wubigo.com/post/microservice-security-aaa/</link>
      <pubDate>Sat, 01 Dec 2018 08:01:48 +0800</pubDate>
      
      <guid>https://wubigo.com/post/microservice-security-aaa/</guid>
      <description>微服务安全要点  通信链路加密 灵活的服务访问控制，包括细粒度访问策略 访问日志审计 服务提供方可替代性(batteries included)和可集成性  基本概念  安全标识  在K8S，安全标识(service account)代表一个用户，一个服务或一组服务。
 安全命名  安全命名定义可运行服务的安全标识
微服务认证  传输层认证 终端用户认证  每一个终端请求通过JWT(JSON Web Token)校验, 支持Auth0, Firebase。
https://medium.facilelogin.com/securing-microservices-with-oauth-2-0-jwt-and-xacml-d03770a9a838</description>
    </item>
    
    <item>
      <title>MicroK8S</title>
      <link>https://wubigo.com/post/2018-11-24-microk8s/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wubigo.com/post/2018-11-24-microk8s/</guid>
      <description>Normally, ${SNAP_DATA} points to /var/snap/microK8S/current. snap.microK8S.daemon-docker, is the docker daemon started using the arguments in ${SNAP_DATA}/args/dockerd
$snap start microK8S $microK8S.docker pull registry.cn-beijing.aliyuncs.com/google_containers/pause:3.1 $microK8S.docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 K8S.gcr.io/pause:3.1  for resource under namespace kube-system all-namespaces don&amp;rsquo;t include kube-system
$microK8S.kubectl describe po calico-node-4sq5r --namespace=kube-system  https://events.static.linuxfound.org/sites/events/files/slides/2016%20-%20Linux%20Networking%20explained_0.pdf</description>
    </item>
    
    <item>
      <title>Choosing a CNI Network Provider for Kubernetes</title>
      <link>https://wubigo.com/post/2018-11-22-cninetworkproviderforkubernetes/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wubigo.com/post/2018-11-22-cninetworkproviderforkubernetes/</guid>
      <description>The Container Network Interface (CNI) is a library definition, and a set of tools under the umbrella of the Cloud Native Computing Foundation project. For more information visit their GitHub project. Kubernetes uses CNI as an interface between network providers and Kubernetes networking.
Why Use CNI Kubernetes default networking provider, kubenet, is a simple network plugin that works with various cloud providers. Kubenet is a very basic network provider, and basic is good, but does not have very many features.</description>
    </item>
    
    <item>
      <title>K8s CNI之Calico实现</title>
      <link>https://wubigo.com/post/k8s_cni_calico/</link>
      <pubDate>Tue, 26 Jun 2018 11:10:47 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s_cni_calico/</guid>
      <description>准备  搭建测试环境  可以参考从源代码构件K8S开发环境</description>
    </item>
    
    <item>
      <title>避开Tiller使用Helm部署K8S应用</title>
      <link>https://wubigo.com/post/helm-without-tiller/</link>
      <pubDate>Mon, 04 Jun 2018 21:02:56 +0800</pubDate>
      
      <guid>https://wubigo.com/post/helm-without-tiller/</guid>
      <description>避开Tiller使用Helm部署K8S应用
Tiller存在的问题  破坏RBAC访问机制  全局的Tiller拥有cluster-admin角色，所以在安装过程中，服务以cluster-admin 角色可以越权访问资源
 部署名字不能重复且唯一  部署名字唯一且很多chart中部署名字也添加到服务名中，导致服务名字混乱。
独立使用helm  获取模板 使用配置修改模板 生产yaml文件  git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 helm template install/kubernetes/helm/istio --name istio --namespace istio-system \ --set security.enabled=false \ --set ingress.enabled=false \ --set gateways.istio-ingressgateway.enabled=false \ --set gateways.istio-egressgateway.enabled=false \ --set galley.enabled=false \ --set sidecarInjectorWebhook.enabled=false \ --set mixer.enabled=false \ --set prometheus.enabled=false \ --set global.proxy.envoyStatsd.enabled=false \ --set pilot.sidecar=false &amp;gt; $HOME/istio-minimal.yaml kubectl create namespace istio-system kubectl apply -f $HOME/istio-minimal.</description>
    </item>
    
    <item>
      <title>K8S SDK Setup</title>
      <link>https://wubigo.com/post/k8s-sdk-setup/</link>
      <pubDate>Sat, 03 Mar 2018 20:45:50 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-sdk-setup/</guid>
      <description> 安装Golang Dep go get -v github.com/tools/godep  安装client-go go get k8s.io/client-go/kubernetes cd $GOPATH/src/k8s.io/client-go git checkout v10.0.0 godep restore ./...  集群外开发 集群内开发 </description>
    </item>
    
    <item>
      <title>K8s CNI之Kube Router实现</title>
      <link>https://wubigo.com/post/k8s_cni_kube-router/</link>
      <pubDate>Mon, 26 Feb 2018 11:11:08 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s_cni_kube-router/</guid>
      <description>准备  搭建测试环境  可以参考从源代码构件K8S开发环境</description>
    </item>
    
    <item>
      <title>K8S CSI</title>
      <link>https://wubigo.com/post/k8s-csi/</link>
      <pubDate>Sat, 24 Feb 2018 06:55:53 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-csi/</guid>
      <description>PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster’s default StorageClass is used instead
Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported yet</description>
    </item>
    
    <item>
      <title>kubectl cheat sheet</title>
      <link>https://wubigo.com/post/2018-01-11-kubectlcheatsheet/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wubigo.com/post/2018-01-11-kubectlcheatsheet/</guid>
      <description>Set namespace preference kubectl config set-context $(kubectl config current-context) --namespace=&amp;lt;bigo&amp;gt;  watch pod kubectl get pods pod1 --watch  Check Performance kubectl top node kubectl top pod  copy file between pod and local kubectl cp ~/f1 &amp;lt;namespace&amp;gt;/&amp;lt;pod-name&amp;gt;:/tmp/ kubectl cp &amp;lt;namespace&amp;gt;/&amp;lt;pod-name&amp;gt;:/tmp/ ~/  enable RBAC  kube-apiserver - --authorization-mode=RBAC  User CRUD openssl genrsa -out bigo.key 2048 openssl req -new -key bigo.key -out bigo.csr -subj &amp;quot;/CN=wubigo/O=bigo LLC&amp;quot; sudo openssl x509 -req -in bigo.</description>
    </item>
    
    <item>
      <title>K8S notes</title>
      <link>https://wubigo.com/post/2017-07-13-k8snotes/</link>
      <pubDate>Thu, 13 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://wubigo.com/post/2017-07-13-k8snotes/</guid>
      <description>无选择器服务 使用场景：
 通过SERVICE连接到外部服务 连接到另一个名字空间或集群 迁移过程中访问遗留系统  步骤
 创建服务  kind: Service apiVersion: v1 metadata: name: ext-db spec: ports: - protocol: TCP port: 80 targetPort: 3316   手动创建一个端点  kind: Endpoints apiVersion: v1 metadata: name: my-service subsets: - addresses: - ip: 10.8.0.2 ports: - port: 3316  kube-proxy mode kubectl get cm kube-proxy -n kube-system -o yaml &amp;gt; kube-proxy.yaml sed -i s/mode:&amp;quot;&amp;quot;/mode:&amp;quot;ipvs/ kube-proxy.yaml sec -i s/creationTimestamp:*// kube-proxy.yaml sed -i s/resourceVersion: &amp;quot;*&amp;quot;// kube-proxy.</description>
    </item>
    
    <item>
      <title>K8s Private Registry</title>
      <link>https://wubigo.com/post/k8s-private-registry/</link>
      <pubDate>Fri, 17 Mar 2017 06:44:46 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-private-registry/</guid>
      <description>Configuring Nodes to Authenticate to a Private Registry  Note: Kubernetes as of now only supports the auths and HttpHeaders section of docker config. This means credential helpers (credHelpers or credsStore) are not supported.
 Docker stores keys for private registries in the $HOME/.dockercfg or $HOME/.docker/config.json file. If there are files in the search paths list below, kubelet uses it as the credential provider when pulling images.
 {&amp;ndash;root-dir:-/var/lib/kubelet}/config.json {cwd of kubelet}/config.</description>
    </item>
    
    <item>
      <title>K8s HA Setup With Kubeadm</title>
      <link>https://wubigo.com/post/k8s-ha-setup-with-kubeadm/</link>
      <pubDate>Thu, 16 Mar 2017 13:23:32 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-ha-setup-with-kubeadm/</guid>
      <description>setup external ETCD  install docker, kubelet, and kubeadm Configure the kubelet to be a service manager for etcd Create configuration files for kubeadm  /tmp/${HOST0}/kubeadmcfg.yaml
apiVersion: &amp;quot;kubeadm.k8s.io/v1beta1&amp;quot; kind: ClusterConfiguration etcd: local: serverCertSANs: - &amp;quot;192.168.1.10&amp;quot; peerCertSANs: - &amp;quot;192.168.1.10&amp;quot; extraArgs: initial-cluster: infra0=https://192.168.1.10:2380 initial-cluster-state: new name: infra0 listen-peer-urls: https://192.168.1.10:2380 listen-client-urls: https://192.168.1.10:2379 advertise-client-urls: https://192.168.1.10:2379 initial-advertise-peer-urls: https://192.168.1.10:2380   Generate the certificate authority
sudo kubeadm init phase certs etcd-ca export HOST0=&amp;quot;192.168.1.10&amp;quot; sudo kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.</description>
    </item>
    
    <item>
      <title>Istio Notes</title>
      <link>https://wubigo.com/post/istio-notes/</link>
      <pubDate>Sat, 04 Mar 2017 16:38:38 +0800</pubDate>
      
      <guid>https://wubigo.com/post/istio-notes/</guid>
      <description>准备 docker pull istio/proxyv2:1.0.6 docker tag istio/proxyv2:1.0.6 gcr.io/istio-release/proxyv2:release-1.0-latest-daily docker push registry.cn-beijing.aliyuncs.com/co1/istio_proxyv2:1.0.6 docker pull istio/pilot:1.0.6 docker tag istio/pilot:1.0.6 gcr.io/istio-release/pilot:release-1.0-latest-daily docker pull istio/mixer:1.0.6 docker tag istio/mixer:1.0.6 gcr.io/istio-release/mixer:release-1.0-latest-daily docker pull istio/galley:1.0.6 docker tag istio/galley:1.0.6 gcr.io/istio-release/galley:release-1.0-latest-daily docker pull istio/citadel:1.0.6 docker tag istio/citadel:1.0.6 gcr.io/istio-release/citadel:release-1.0-latest-daily docker pull istio/sidecar_injector:1.0.6 docker tag istio/sidecar_injector:1.0.6 gcr.io/istio-release/sidecar_injector:release-1.0-latest-daily git clone https://github.com/istio/istio.git cd istio git checkout 1.0.6 -b 1.0.6 ```` # 安装 Istio by default uses LoadBalancer service object types. Some platforms do not support LoadBalancer service objects.</description>
    </item>
    
    <item>
      <title>K8S Monitor</title>
      <link>https://wubigo.com/post/k8s-monitor/</link>
      <pubDate>Thu, 23 Feb 2017 20:28:40 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-monitor/</guid>
      <description> setup prometheus  prepare pv for prometheus  https://wubigo.com/post/2018-01-11-kubectlcheatsheet/#pvc&amp;ndash;using-local-pv
 install  helm install --name prometheus1 stable/prometheus --set server.persistentVolume.storageClass=local-hdd,alertmanager.enabled=false  </description>
    </item>
    
    <item>
      <title>K8SCNI之L2 网络实现</title>
      <link>https://wubigo.com/post/k8s_cni_l2_network_on_bare_metal/</link>
      <pubDate>Thu, 26 Jan 2017 10:09:00 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s_cni_l2_network_on_bare_metal/</guid>
      <description> 准备  搭建测试环境
可以参考从源代码构件K8S开发环境
  </description>
    </item>
    
    <item>
      <title>K8S网络基础</title>
      <link>https://wubigo.com/post/k8s-network-basic/</link>
      <pubDate>Wed, 24 Feb 2016 19:39:03 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-network-basic/</guid>
      <description> K8S网络基础
K8S简介 K8S是自动化部署和监控容器的容器编排和管理工具。各大云厂商和应用开发平台都提供基于K8S的容器服务。 如果觉得K8S托管服务不容易上手或者和本公司的业务场景不很匹配，现在也有很多工具帮助在自己的数据 中心或私有云平台搭建K8S运行环境。
 Minikube kops kubeadm  如果你想搭建一个测试环境，请参考
 从K8S源代码构建容器集群(支持最新稳定版V1.13.3) 一个脚步部署K8S  Kubernetes主要构件:
 主节点： 主要的功能包括管理工作节点集群，服务部署，服务发现，工作调度，负载均衡等。 工作节点： 应用负载执行单元。 服务规范： 无状态服务，有状态服务，守护进程服务，定时任务等。   K8S网络基础 K8S网络模型
 每一个POD拥有独立的IP地址 任何两个POD之间都可以互相通信且不通过NAT 集群每个节点上的代理(KUBELET)可以和该节点上的所有POD通信  K8S网络模型从网络端口分配的角度为容器建立一个干净的，向后兼容的规范，极大的方便和简化应用从虚拟机往容器迁移的流程。
K8S解决的网络问题：
 容器间通信问题： 由POD和localhost通信解决
 POD间通信问题： 由CNI解决 POD和服务的通信问题： 由SERVICE解决 外部系统和SERVICE的通信问题： 由SERVICE解决  </description>
    </item>
    
    <item>
      <title>K8S local development setup from source code</title>
      <link>https://wubigo.com/post/k8s-local-development-setup/</link>
      <pubDate>Wed, 03 Feb 2016 11:38:27 +0800</pubDate>
      
      <guid>https://wubigo.com/post/k8s-local-development-setup/</guid>
      <description>更新到最新正式发布版V1.13.3
Main external dependencies  go docker cri cni  external-dependencies
KUBEADM IS CURRENTLY IN BETA
kubeadm maturity build k8s  docker v17.03  sudo apt-get install docker-ce=17.03.3~ce-0~ubuntu-xenial docker pull mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-apiserver-amd64:v1.11.7 k8s.gcr.io/kube-apiserver-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-controller-manager-amd64:v1.11.7 k8s.gcr.io/kube-controller-manager-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-scheduler-amd64:v1.11.7 k8s.gcr.io/kube-scheduler-amd64:v1.11.7 docker pull mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 docker tag mirrorgooglecontainers/kube-proxy-amd64:v1.11.7 k8s.gcr.io/kube-proxy-amd64:v1.11.7 docker pull mirrorgooglecontainers/pause:3.1 docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 docker pull mirrorgooglecontainers/etcd-amd64:3.2.18 docker tag mirrorgooglecontainers/etcd-amd64:3.2.18 k8s.</description>
    </item>
    
  </channel>
</rss>